{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/ayyoob/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "sys.path.insert(0, '../')\n",
    "from my_utils import gpu_utils\n",
    "import importlib, gc\n",
    "from my_utils.alignment_features import *\n",
    "import my_utils.alignment_features as afeatures\n",
    "importlib.reload(afeatures)\n",
    "import gnn_utils.graph_utils as gutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch-geometric\n",
    "# !pip install tensorboardX\n",
    "\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip ngrok-stable-linux-amd64.zip\n",
    "\n",
    "#  print(torch.version.cuda)\n",
    "#  print(torch.__version__)    \n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24159\n"
     ]
    }
   ],
   "source": [
    "from my_utils import align_utils as autils, utils\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "utils.setup(config_file)\n",
    "\n",
    "params = argparse.Namespace()\n",
    "\n",
    "\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses = list(pros.keys())\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses.extend(list(pros.keys()))\n",
    "all_verses = list(set(all_verses))\n",
    "print(len(all_verses))\n",
    "\n",
    "params.editions_file =  \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi_lang_list.txt\"\n",
    "editions, langs = autils.load_simalign_editions(params.editions_file)\n",
    "current_editions = [editions[lang] for lang in langs]\n",
    "\n",
    "def get_pruned_verse_alignments(args):\n",
    "    verse, current_editions = args\n",
    "    \n",
    "    verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "    gc.collect()\n",
    "    return verse_aligns_inter, verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "args = []\n",
    "for i,verse in enumerate(all_verses):\n",
    "    args.append((verse, current_editions[:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(afeatures)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features, n_head = 2, has_tagfreq_feature=False,):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels, heads= n_head)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= 1)\n",
    "        self.fin_lin = nn.Linear(out_channels, out_channels)\n",
    "        \n",
    "        if has_tagfreq_feature:\n",
    "            self.feature_encoder = afeatures.FeatureEncoding(features, [normalized_tag_frequencies, word_vectors])\n",
    "        else:\n",
    "            self.feature_encoder = afeatures.FeatureEncoding(features, [word_vectors])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.feature_encoder(x, dev)\n",
    "        x = F.elu(self.conv1(x, edge_index, ))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        return F.relu(self.fin_lin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_encoders_embedding(encoder):\n",
    "    for i,ft in enumerate(encoder.feature_types):\n",
    "        if ft.type == MAPPING:\n",
    "            print('doing it')\n",
    "            encoder.layers[i] = afeatures.MappingEncoding(encoder.layers[i].emb.weight, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class DataEncoder():\n",
    "\n",
    "    def __init__(self, data_loader, model, mask_language):\n",
    "        self.data_loader = data_loader\n",
    "        self.model = model\n",
    "        self.mask_language = mask_language\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i,batch in enumerate(tqdm(self.data_loader)):\n",
    "            \n",
    "            x = batch['x'][0].to(dev)\n",
    "            edge_index = batch['edge_index'][0].to(dev)\n",
    "            verse = batch['verse'][0]\n",
    "\n",
    "            if verse in masked_verses:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if self.mask_language:\n",
    "                    x[:, 0] = 0\n",
    "                z = self.model.encode(x, edge_index)\n",
    "                \n",
    "            except Exception as e:\n",
    "                global sag, khar, gav\n",
    "                sag, khar, gav =  (i, batch, verse)\n",
    "                print(e)\n",
    "                1/0\n",
    "            \n",
    "            yield z, verse, i, batch\n",
    "\n",
    "def train(epoch, data_loader, mask_language, max_batches=999999999):\n",
    "    global optimizer\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    loss_multi_round = 0\n",
    "\n",
    "    data_encoder = DataEncoder(data_loader, model, mask_language)\n",
    "\n",
    "    for z, verse, i, batch in data_encoder:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target = batch['pos_classes'][0].to(dev)\n",
    "        _, labels = torch.max(target, 1)\n",
    "        \n",
    "        index = batch['pos_index'][0].to(dev)\n",
    "        preds = model.decoder(z, index, batch)\n",
    "\n",
    "        # print(preds.shape, labels.shape)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss = loss * target.shape[0]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 5 == 0:\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        if i % 500 == 499:\n",
    "            print(f\"loss: {total_loss}\")\n",
    "            total_loss = 0\n",
    "            test(epoch, test_data_loader, mask_language)\n",
    "            model.train()\n",
    "            clean_memory()\n",
    "\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    \n",
    "    print(f\"total train loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class, drop_out=0):\n",
    "        super(POSDecoder, self).__init__()\n",
    "\n",
    "        self.transfer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                        nn.Linear(hidden_size, n_class))\n",
    "\n",
    "    def forward(self, z, index, batch=None):\n",
    "        h = z[index, :]\n",
    "\n",
    "        res = self.transfer(h)\n",
    "\n",
    "        return res\n",
    "\n",
    "class POSDecoderTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class, drop_out=0):\n",
    "        super(POSDecoderTransformer, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=8, dim_feedforward=hidden_size)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "\n",
    "        self.transfer = nn.Sequential( nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                        nn.Linear(hidden_size, n_class))\n",
    "\n",
    "    def forward(self, z, index, batch):\n",
    "        language_based_nodes = batch['lang_based_nodes']\n",
    "        transformer_indices = batch['transformer_indices']\n",
    "\n",
    "        batch = []\n",
    "        for lang_nodes in language_based_nodes:\n",
    "            tensor = z[lang_nodes, :]\n",
    "            tensor = F.pad(tensor, (0, 0, 0, 150 - tensor.size(0)))\n",
    "            batch.append(tensor)\n",
    "        \n",
    "        batch = torch.stack(batch)\n",
    "        batch = torch.transpose(batch, 0, 1)\n",
    "\n",
    "        h = self.transformer(batch)\n",
    "        h = torch.transpose(h, 0, 1)\n",
    "        h = h[transformer_indices[0], transformer_indices[1], :]\n",
    "\n",
    "        res = self.transfer(h)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, testloader, mask_language, filter_wordtypes=None):\n",
    "    print('testing',  epoch)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    data_encoder = DataEncoder(testloader, model, mask_language)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for z, verse, i, batch in data_encoder:\n",
    "            \n",
    "            target = batch['pos_classes'][0].to(dev)\n",
    "            index = batch['pos_index'][0].to(dev)\n",
    "            \n",
    "            if filter_wordtypes != None:\n",
    "                non_filtered_words = filter_wordtypes[batch['x'][0][:, 9].long()] == 1\n",
    "                non_filtered_words = non_filtered_words[index]\n",
    "                index = index[non_filtered_words]\n",
    "\n",
    "                target = target[non_filtered_words, :]\n",
    "\n",
    "            preds = model.decoder(z, index, batch)\n",
    "            \n",
    "            if preds.size(0) > 0:\n",
    "                _, predicted = torch.max(preds, 1)\n",
    "                _, labels = torch.max(target, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'test, epoch: {epoch}, total:{total} ACC: {correct/total}')\n",
    "    clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting_test(data_loader1, data_loader2):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i,(batch, batch2) in enumerate(tqdm(zip(data_loader1, data_loader2))) :\n",
    "            \n",
    "        x = batch['x'][0]\n",
    "        edge_index = batch['edge_index'][0]\n",
    "        verse = batch['verse'][0]\n",
    "\n",
    "        if verse in masked_verses:\n",
    "            continue\n",
    "\n",
    "        target = batch['pos_classes'][0]\n",
    "        index = batch['pos_index'][0]\n",
    "\n",
    "        index2 = batch2['pos_index'][0]\n",
    "        \n",
    "\n",
    "        for node, label in zip(index,target):\n",
    "            other_side = edge_index[1, edge_index[0, :] == node]\n",
    "            other_side_withpos = other_side[[True if i in index2 else False for i in other_side]]\n",
    "            other_side_target_indices = [(i == index2).nonzero(as_tuple=True)[0].item() for i in other_side_withpos]\n",
    "            #print(other_side_target_indices)\n",
    "            proj_tags = batch2['pos_classes'][0][other_side_target_indices]\n",
    "\n",
    "            if proj_tags.size(0) > 0:\n",
    "                _, proj_tags = torch.max(proj_tags, 1)\n",
    "                #print(target.shape, node, index.shape, proj_tags, other_side)\n",
    "                \n",
    "                if torch.argmax(label) == torch.mode(proj_tags)[0]:\n",
    "                    correct += 1\n",
    "                \n",
    "                total += 1\n",
    "\n",
    "    print(f'test, , total:{total} ACC: {correct/total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wordtype_frequencies[736])\n",
    "#print(wordtype_frequencies[1473])\n",
    "#print(wordtype_frequencies[3683])\n",
    "#print(wordtype_frequencies[7367])\n",
    "#print(wordtype_frequencies[14733])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequent_words = torch.zeros(word_frequencies.size(0))\n",
    "#frequent_words[word_frequencies > -1] = 1\n",
    "\n",
    "#test(1, test_data_loader, filter_wordtypes=frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2354770, 100)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"/mounts/work/ayyoob/models/w2v/word2vec_helfi_langs_15e.model\")\n",
    "\n",
    "print(w2v_model.wv.vectors.shape)\n",
    "word_vectors = torch.from_numpy(w2v_model.wv.vectors).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "test_verses = all_verses[:] \n",
    "editf1 = 'fin-x-bible-helfi'\n",
    "editf2 = \"heb-x-bible-helfi\"\n",
    "\n",
    "\n",
    "if 'jpn-x-bible-newworld' in  current_editions[:]:\n",
    "     current_editions.remove('jpn-x-bible-newworld')\n",
    "if 'grc-x-bible-unaccented' in  current_editions[:]:\n",
    "     current_editions.remove('grc-x-bible-unaccented')\n",
    "\n",
    "\n",
    "\n",
    "data_dir_train = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_train_community_word\"\n",
    "data_dir_blinker = \"/mounts/data/proj/ayyoob/align_induction/dataset/pruned_alignments_blinker_inter/\"\n",
    "data_dir_grc = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_grc_community_word/\"\n",
    "data_dir_heb = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_heb_community_word/\"\n",
    "\n",
    "train_dataset = torch.load(f\"{data_dir_train}/train_dataset_nox_noedge.torch.bin\")\n",
    "blinker_test_dataset = torch.load(f\"{data_dir_blinker}/train_dataset_nox_noedge.torch.bin\")\n",
    "grc_test_dataset = torch.load(f\"{data_dir_grc}/train_dataset_nox_noedge.torch.bin\")\n",
    "heb_test_dataset = torch.load(f\"{data_dir_heb}/train_dataset_nox_noedge.torch.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "\n",
    "postag_map = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "\n",
    "pos_lang_list = [\"eng-x-bible-mixed\", \"deu-x-bible-newworld\", \"ces-x-bible-newworld\", \n",
    "\t\t\"fra-x-bible-louissegond\",\"hin-x-bible-newworld\", \"ita-x-bible-2009\", \n",
    "\t\t\"prs-x-bible-goodnews\", \"ron-x-bible-2006\", \"spa-x-bible-newworld\"]\n",
    "\n",
    "def get_db_nodecount(dataset):\n",
    "\tres = 0\n",
    "\tfor lang in dataset.nodes_map.values():\n",
    "\t\tfor verse in lang.values():\n",
    "\t\t\tres += len(verse)\n",
    "\t\n",
    "\treturn res\n",
    "\n",
    "def get_language_nodes(dataset, lang_list, sentences):\n",
    "\tnode_count = get_db_nodecount(dataset)\n",
    "\tpos_labels = torch.zeros(node_count, len(postag_map))\n",
    "\n",
    "\tpos_node_cover = collections.defaultdict(list)\n",
    "\tfor lang in lang_list:\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\tif sentence in dataset.nodes_map[lang]:\n",
    "\t\t\t\tfor tok in dataset.nodes_map[lang][sentence]:\n",
    "\t\t\t\t\tpos_node_cover[sentence].append(dataset.nodes_map[lang][sentence][tok])\n",
    "\t\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\n",
    "\n",
    "def get_pos_tags(dataset, pos_lang_list):\n",
    "\tall_tags = {}\n",
    "\tfor lang in pos_lang_list:\n",
    "\t\tif lang not in dataset.nodes_map:\n",
    "\t\t\tcontinue\n",
    "\t\tall_tags[lang] = {}\n",
    "\t\twith codecs.open(F\"/mounts/work/mjalili/projects/gnn-align/data/pbc_pos_tags/{lang}.conllu\", \"r\", \"utf-8\") as lang_pos:\n",
    "\t\t\ttag_sent = []\n",
    "\t\t\tsent_id = \"\"\n",
    "\t\t\tfor sline in lang_pos:\n",
    "\t\t\t\tsline = sline.strip()\n",
    "\t\t\t\tif sline == \"\":\n",
    "\t\t\t\t\tif sent_id not in dataset.nodes_map[lang]:\n",
    "\t\t\t\t\t\ttag_sent = []\n",
    "\t\t\t\t\t\tsent_id = \"\"\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tall_tags[lang][sent_id] = [p[3] for p in tag_sent]\n",
    "\t\t\t\t\ttag_sent = []\n",
    "\t\t\t\t\tsent_id = \"\"\n",
    "\t\t\t\telif \"# verse_id\" in sline:\n",
    "\t\t\t\t\tsent_id = sline.split()[-1]\n",
    "\t\t\t\telif sline[0] == \"#\":\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttag_sent.append(sline.split(\"\\t\"))\n",
    "\n",
    "\tnode_count = get_db_nodecount(dataset)\n",
    "\tpos_labels = torch.zeros(node_count, len(postag_map))\n",
    "\tpos_node_cover = collections.defaultdict(list)\n",
    "\n",
    "\tfor lang in all_tags:\n",
    "\t\tfor sent_id in all_tags[lang]:\n",
    "\t\t\tsent_tags = all_tags[lang][sent_id]\n",
    "\t\t\tfor w_i in range(len(sent_tags)):\n",
    "\t\t\t\tif w_i not in dataset.nodes_map[lang][sent_id]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tpos_labels[dataset.nodes_map[lang][sent_id][w_i], postag_map[sent_tags[w_i]]] = 1\n",
    "\t\t\t\tpos_node_cover[sent_id].append(dataset.nodes_map[lang][sent_id][w_i])\n",
    "\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\t#pos_pickle = {\"pos_labels\": pos_labels, \"node_ids_train\": pos_ids_train, \"node_ids_dev\": pos_ids_dev}\n",
    "\t#torch.save(pos_pickle, '/mounts/work/ayyoob/models/gnn/postag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blinker_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_blinker_full_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf12 = \"eng-x-bible-mixed\"\n",
    "editf22 = 'fra-x-bible-louissegond'\n",
    "\n",
    "test_gold_eng_fra = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/eng_fra_pbc/eng-fra.gold\"\n",
    "\n",
    "pros_blinker, surs_blinker = autils.load_gold(test_gold_eng_fra)\n",
    "\n",
    "blinker_verse_alignments_inter = {}\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in blinker_test_dataset.nodes_map:\n",
    "    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "blinker_verses = [item[0] for item in sorted_verses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importlib.reload(afeatures)\n",
    "#grc_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_grc_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf_fin = \"fin-x-bible-helfi\"\n",
    "editf_grc = 'grc-x-bible-helfi'\n",
    "\n",
    "test_gold_grc = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_test.txt\"\n",
    "\n",
    "pros_grc, surs_grc = autils.load_gold(test_gold_grc)\n",
    "\n",
    "grc_test_verse_alignments_inter = {}\n",
    "grc_test_verse_alignments_gdfa = {}\n",
    "gc.collect()\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in grc_test_dataset.nodes_map:\n",
    "    for verse in grc_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in grc_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = grc_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "grc_test_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heb_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "\n",
    "test_gold_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_test.txt\"\n",
    "\n",
    "pros_heb, surs_heb = autils.load_gold(test_gold_heb)\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in heb_test_dataset.nodes_map:\n",
    "    for verse in heb_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in heb_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = heb_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "heb_test_verses = [item[0] for item in sorted_verses]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_map = {}\n",
    "\n",
    "for edit in train_dataset.nodes_map:\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in train_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = train_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "all_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "long_verses = set()\n",
    "\n",
    "for edit in train_dataset.nodes_map.keys():\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        to_print = False\n",
    "        for tok in train_dataset.nodes_map[edit][verse]:\n",
    "            if tok > 150:\n",
    "                to_print = True\n",
    "        if to_print == True:\n",
    "            long_verses.add(verse)\n",
    "\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "\n",
    "masked_verses = list(long_verses)\n",
    "#masked_verses.extend(blinker_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "def get_language_based_nodes(nodes_map, verse, train_nodes, padding):\n",
    "    res = []\n",
    "    transformer_indices = [[-1 for i in range(len(train_nodes))],[-1 for i in range(len(train_nodes))]]\n",
    "    \n",
    "    lang_ind = 0\n",
    "    for lang in nodes_map:\n",
    "        if verse in nodes_map[lang]:\n",
    "            items = nodes_map[lang][verse].items()\n",
    "            items = sorted(items, key=lambda i: i[0])\n",
    "\n",
    "            to_add = []\n",
    "            for i, it in enumerate(items):\n",
    "                to_add.append(it[1] - padding)\n",
    "                if it[1] in train_nodes:\n",
    "                    index = train_nodes.index(it[1])\n",
    "                    transformer_indices[0][index] = lang_ind\n",
    "                    transformer_indices[1][index] = i\n",
    "\n",
    "            res.append(to_add)\n",
    "            lang_ind += 1\n",
    "            \n",
    "    return res, transformer_indices\n",
    "\n",
    "class POSTAGGNNDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, verses, edit_files, alignments, node_cover, pos_labels, data_dir, create_data=False, group_size = 20):\n",
    "        self.node_cover = node_cover\n",
    "        self.pos_labels = pos_labels\n",
    "        self.data_dir = data_dir\n",
    "        self.items = self.calculate_size(verses, group_size, node_cover)\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if create_data:\n",
    "            self.calculate_verse_stats(verses, edit_files, alignments, dataset, data_dir)            \n",
    "        \n",
    "    def calculate_size(self, verses, group_size, node_cover):\n",
    "        res = []\n",
    "        for verse in verses:\n",
    "            covered_nodes = node_cover[verse]\n",
    "            random.shuffle(covered_nodes)\n",
    "            items = [covered_nodes[i:i + group_size] for i in range(0, len(covered_nodes), group_size)]\n",
    "            res.extend([(verse, i) for i in items])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calculate_verse_stats(self,verses, edition_files, alignments, dataset, data_dir):\n",
    "\n",
    "        min_edge = 0\n",
    "        for verse in tqdm(verses):\n",
    "            min_nodes = 99999999999999\n",
    "            max_nodes = 0\n",
    "            edges_tmp = [[],[]]\n",
    "            x_tmp = []\n",
    "            features = []\n",
    "            for i,editf1 in enumerate(edition_files):\n",
    "                for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "                    aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "                    if aligns != None:\n",
    "                        for align in aligns:\n",
    "                            try:\n",
    "                                n1,_ = gutils.node_nom(verse, editf1, align[0], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                n2,_ = gutils.node_nom(verse, editf2, align[1], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                edges_tmp[0].extend([n1, n2])\n",
    "\n",
    "                                max_nodes = max(n1, n2, max_nodes)\n",
    "                                min_nodes = min(n1, n2, min_nodes)\n",
    "                            except Exception as e:\n",
    "                                print(editf1, editf2, verse)\n",
    "                                raise(e)\n",
    "\n",
    "            self.verse_info = {}\n",
    "\n",
    "            self.verse_info['padding'] = min_nodes\n",
    "            \n",
    "            self.verse_info['x'] = torch.clone(dataset.x[min_nodes:max_nodes+1,:])\n",
    "            \n",
    "            self.verse_info['edge_index'] = torch.clone(dataset.edge_index[:, min_edge : min_edge + len(edges_tmp[0])] - min_nodes)\n",
    "\n",
    "            if torch.min(self.verse_info['edge_index']) != 0:\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info['edge_index']))\n",
    "            \n",
    "            if self.verse_info['x'].shape[0] != torch.max(self.verse_info['edge_index']) + 1 :\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info['edge_index']))\n",
    "            \n",
    "            min_edge = min_edge + len(edges_tmp[0])\n",
    "\n",
    "            torch.save(self.verse_info, f\"{data_dir}/verses/{verse}_info.torch.bin\")\n",
    "        \n",
    "        dataset.x = None\n",
    "        dataset.edge_index = None\n",
    "        torch.save(dataset, f\"{data_dir}/train_dataset_nox_noedge.torch.bin\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        verse, nodes = self.items[idx]\n",
    "        \n",
    "        self.verse_info = {verse: torch.load(f'{self.data_dir}/verses/{verse}_info.torch.bin')}\n",
    "\n",
    "\n",
    "        word_number = self.verse_info[verse]['x'][:, 9]\n",
    "        padding = self.verse_info[verse]['padding']\n",
    "        \n",
    "        language_based_nodes, transformer_indices = get_language_based_nodes(self.dataset.nodes_map, verse, nodes, padding)\n",
    "\n",
    "        # # Add POSTAG to set of features\n",
    "        # postags = self.pos_labels[padding: self.verse_info[verse]['x'].size(0) + padding, : ]\n",
    "        # postags = postags.detach().clone()\n",
    "        # postags[torch.LongTensor(nodes) - padding, :] = 0\n",
    "        # self.verse_info[verse]['x'] = torch.cat((self.verse_info[verse]['x'], postags), dim=1)\n",
    "\n",
    "        # Add token id as a feature, used to extract token information (like token's tag distribution)\n",
    "        word_number = torch.unsqueeze(word_number, 1)\n",
    "        self.verse_info[verse]['x'] = torch.cat((self.verse_info[verse]['x'], word_number), dim=1)\n",
    "\n",
    "        return {'verse':verse, 'x':self.verse_info[verse]['x'], 'edge_index':self.verse_info[verse]['edge_index'], \n",
    "                'pos_classes': self.pos_labels[nodes, :], 'pos_index': torch.LongTensor(nodes) - padding, \n",
    "                'padding': padding, 'lang_based_nodes': language_based_nodes, 'transformer_indices': transformer_indices}\n",
    "\n",
    "\n",
    "# # train_pos_labels, train_pos_node_cover = get_pos_tags(train_dataset, pos_lang_list)\n",
    "# # torch.save({'pos_labels':train_pos_labels, 'pos_node_cover':train_pos_node_cover}, f'{data_dir_train}/pos_data.torch.bin')\n",
    "pos_data = torch.load(f'{data_dir_train}/pos_data.torch.bin')\n",
    "train_pos_labels, train_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "gnn_dataset_train_pos = POSTAGGNNDataset(train_dataset, train_verses, current_editions, {},\n",
    "                    train_pos_node_cover, train_pos_labels, data_dir_train, group_size = 100)\n",
    "\n",
    "## blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, pos_lang_list)\n",
    "## torch.save({'pos_labels':blinker_pos_labels, 'pos_node_cover': blinker_pos_node_cover}, f'{data_dir_blinker}/pos_data.torch.bin')\n",
    "pos_data = torch.load(f'{data_dir_blinker}/pos_data.torch.bin')\n",
    "blinker_pos_labels, blinker_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "gnn_dataset_blinker_pos = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "                            blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 10000)\n",
    "\n",
    "##grc_pos_labels, grc_pos_node_cover = get_pos_tags(grc_test_dataset)\n",
    "##torch.save({'pos_labels':grc_pos_labels, 'pos_node_cover': grc_pos_node_cover}, f'{data_dir_grc}/pos_data.torch.bin')\n",
    "pos_data = torch.load(f'{data_dir_grc}/pos_data.torch.bin')\n",
    "grc_pos_labels, grc_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "gnn_dataset_grc_pos = POSTAGGNNDataset(grc_test_dataset, grc_test_verses, current_editions, grc_test_verse_alignments_inter, grc_pos_node_cover, grc_pos_labels, data_dir_grc, group_size = 100)\n",
    "\n",
    "##heb_pos_labels, heb_pos_node_cover = get_pos_tags(heb_test_dataset)\n",
    "##torch.save({'pos_labels':heb_pos_labels, 'pos_node_cover': heb_pos_node_cover}, f'{data_dir_heb}/pos_data.torch.bin')\n",
    "pos_data = torch.load(f'{data_dir_heb}/pos_data.torch.bin')\n",
    "heb_pos_labels, heb_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "gnn_dataset_heb_pos = POSTAGGNNDataset(heb_test_dataset, heb_test_verses, current_editions, {}, heb_pos_node_cover, heb_pos_labels, data_dir_heb, group_size = 100)\n",
    "\n",
    "\n",
    "gnn_dataset_train_pos_bigbatch = POSTAGGNNDataset(train_dataset, train_verses, current_editions, {},\n",
    "                   train_pos_node_cover, train_pos_labels, data_dir_train, group_size = 10000)\n",
    "train_data_loader_bigbatch = DataLoader(gnn_dataset_train_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "\n",
    "gnn_dataset_grc_pos_bigbatch = POSTAGGNNDataset(grc_test_dataset, grc_test_verses, current_editions, {}, grc_pos_node_cover, grc_pos_labels, data_dir_grc, group_size = 10000)\n",
    "grc_data_loader_bigbatch = DataLoader(gnn_dataset_grc_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "\n",
    "gnn_dataset_heb_pos_bigbatch = POSTAGGNNDataset(heb_test_dataset, heb_test_verses, current_editions, {}, heb_pos_node_cover, heb_pos_labels, data_dir_heb, group_size = 10000)\n",
    "heb_data_loader_bigbatch = DataLoader(gnn_dataset_heb_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "blinker_data_loader_bigbatch = DataLoader(gnn_dataset_blinker_pos, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_eng_langs = pos_lang_list[:]\n",
    "no_eng_langs.remove('eng-x-bible-mixed')\n",
    "\n",
    "# # train_pos_labels, train_pos_node_cover = get_pos_tags(train_dataset, no_eng_langs)\n",
    "# # gnn_dataset_train_pos = POSTAGGNNDataset(train_dataset, train_verses, current_editions, verse_alignments_inter,\n",
    "# #                        train_pos_node_cover, train_pos_labels, data_dir_train, group_size = 10)\n",
    "\n",
    "\n",
    "# _, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, ['eng-x-bible-mixed'])\n",
    "# blinker_pos_labels, _ = get_pos_tags(blinker_test_dataset, pos_lang_list)\n",
    "# gnn_dataset_blinker_pos = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                             blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    #model.encoder.feature_encoder.feature_types[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "    #model.encoder.feature_encoder.feature_types[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "    #model.encoder.feature_encoder.feature_types[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "    #model.encoder.feature_encoder.feature_types[8] = afeatures.OneHotFeature(32, 250, 'community_2')\n",
    "    #model.encoder.feature_encoder.feature_types[9] = afeatures.MappingFeature(100, 'word')\n",
    "    #model.encoder.feature_encoder.feature_types[10] = afeatures.MappingFeature(len(postag_map), 'tag_priors', freeze=True)\n",
    "    torch.save(model, f'/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_{name}_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test(1, test_data_loader) \n",
    "\n",
    "# #finetune_pos_labels, finetune_pos_node_cover = get_pos_tags(train_dataset, ['eng-x-bible-mixed'])\n",
    "# #gnn_dataset_finetune_pos = POSTAGGNNDataset(train_dataset, train_verses, current_editions, verse_alignments_inter,\n",
    "# #                       finetune_pos_node_cover, finetune_pos_labels, data_dir_train, group_size = 100)\n",
    "# #finetune_data_loader = DataLoader(gnn_dataset_finetune_pos, batch_size=1, shuffle=False)\n",
    "\n",
    "# # train(1, finetune_data_loader, max_batches=1000)\n",
    "# # test(1, test_data_loader) \n",
    "\n",
    "# blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, ['eng-x-bible-mixed'])\n",
    "# gnn_dataset_blinker_pos = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                              blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 10000)\n",
    "\n",
    "# blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, no_eng_langs)\n",
    "# gnn_dataset_blinker_pos_majvoting_test = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                              blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 10000)\n",
    "\n",
    "# test_data_loader = DataLoader(gnn_dataset_blinker_pos, batch_size=1, shuffle=False)\n",
    "# test_data_loader_majvoting = DataLoader(gnn_dataset_blinker_pos_majvoting_test, batch_size=1, shuffle=False)\n",
    "# majority_voting_test(test_data_loader, test_data_loader_majvoting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(7)\n",
    "features = train_dataset.features\n",
    "\n",
    "# features.append(afeatures.PassFeature(name='posTAG', dim=len(postag_map)))\n",
    "# features.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_model(tag_frequencies=False, use_transformers=False, train_word_embedding=False, mask_language=True):\n",
    "    global model, criterion, optimizer, test_data_loader\n",
    "\n",
    "    features = train_dataset.features\n",
    "    features[9].freeze = not train_word_embedding\n",
    "    \n",
    "    train_data_loader = DataLoader(gnn_dataset_train_pos, batch_size=1, shuffle=True)\n",
    "    grc_data_loader = DataLoader(gnn_dataset_grc_pos, batch_size=1, shuffle=True)\n",
    "    heb_data_loader = DataLoader(gnn_dataset_heb_pos, batch_size=1, shuffle=True)\n",
    "    test_data_loader = DataLoader(gnn_dataset_blinker_pos, batch_size=1, shuffle=True)\n",
    "\n",
    "    clean_memory()\n",
    "    drop_out = 0\n",
    "    n_head = 1\n",
    "    in_dim = sum(t.out_dim for t in features)\n",
    "\n",
    "\n",
    "    channels = 512\n",
    "\n",
    "    decoder_in_dim = n_head * channels \n",
    "\n",
    "    # model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer20210910-235352-.pickle')\n",
    "    if use_transformers:\n",
    "        decoder = POSDecoderTransformer(decoder_in_dim, decoder_in_dim*2, len(postag_map))\n",
    "    else:\n",
    "        decoder = POSDecoder(decoder_in_dim, decoder_in_dim*2, len(postag_map))\n",
    "        \n",
    "    model = pyg_nn.GAE(Encoder(in_dim, channels, features, n_head, has_tagfreq_feature=tag_frequencies), decoder).to(dev)\n",
    "    freeze_encoders_embedding(model.encoder.feature_encoder)\n",
    "\n",
    "    model.to(dev)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "    torch.set_printoptions(edgeitems=5)\n",
    "    print(\"model params - decoder params - conv1\", sum(p.numel() for p in model.parameters()), sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "    for epoch in range(1, 3):\n",
    "        print(f\"\\n----------------epoch {epoch} ---------------\")\n",
    "        \n",
    "        train(epoch, train_data_loader, mask_language)\n",
    "        train(epoch, grc_data_loader, mask_language)\n",
    "        train(epoch, heb_data_loader, mask_language)\n",
    "        save_model(model, f'posfeat{tag_frequencies}_transformer{use_transformers}_trainWE{train_word_embedding}_maskLang{mask_language}')\n",
    "        test(epoch, test_data_loader, mask_language) \n",
    "        clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = sag\n",
    "#batch = khar\n",
    "#verse = gav\n",
    "#print(i, verse)\n",
    "\n",
    "#keys = list(gnn_dataset.verse_info.keys())\n",
    "\n",
    "#gnn_dataset.verse_info[verse]\n",
    "#save_model(model, 'freeze-embedding_noLang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): GATConv(236, 1024, heads=1)\n",
       "    (conv2): GATConv(1024, 512, heads=1)\n",
       "    (fin_lin): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (feature_encoder): FeatureEncoding(\n",
       "      (layers): ModuleList(\n",
       "        (0): OneHotEncoding(\n",
       "          (linear): Linear(in_features=83, out_features=20, bias=True)\n",
       "        )\n",
       "        (1): OneHotEncoding(\n",
       "          (linear): Linear(in_features=150, out_features=32, bias=True)\n",
       "        )\n",
       "        (2): FloatEncoding(\n",
       "          (linear): Linear(in_features=1, out_features=4, bias=True)\n",
       "        )\n",
       "        (3): FloatEncoding(\n",
       "          (linear): Linear(in_features=1, out_features=4, bias=True)\n",
       "        )\n",
       "        (4): FloatEncoding(\n",
       "          (linear): Linear(in_features=1, out_features=4, bias=True)\n",
       "        )\n",
       "        (5): FloatEncoding(\n",
       "          (linear): Linear(in_features=1, out_features=4, bias=True)\n",
       "        )\n",
       "        (6): FloatEncoding(\n",
       "          (linear): Linear(in_features=1, out_features=4, bias=True)\n",
       "        )\n",
       "        (7): OneHotEncoding(\n",
       "          (linear): Linear(in_features=250, out_features=32, bias=True)\n",
       "        )\n",
       "        (8): OneHotEncoding(\n",
       "          (linear): Linear(in_features=250, out_features=32, bias=True)\n",
       "        )\n",
       "        (9): MappingEncoding(\n",
       "          (emb): Embedding(2354770, 100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): POSDecoder(\n",
       "    (transfer): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0, inplace=False)\n",
       "      (3): Linear(in_features=1024, out_features=17, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_freeze-embedding_noLang_20211022-125404.pickle')\n",
    "torch.cuda.set_device(0)\n",
    "model.to(dev)\n",
    "#epoch = 0\n",
    "#test_data_loader = DataLoader(gnn_dataset_blinker_pos, batch_size=1, shuffle=False)\n",
    "#test(epoch, test_data_loader, mask_language=True)\n",
    "#yoruba_postags = generate_target_lang_tags('yor-x-bible-2010', f\"posfeatFalse_transformerFalse_trainWEFalse\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "# These are functions to calculate the frequency vectors and also update training data with predictions for target language tags\n",
    "\n",
    "# This funcion update existing structures that are used for training with new predictions\n",
    "def update_trainset_with_predictions(node_cover, pos_labels, padding, index, max_values, pos_tags, word_added_to_train_freq, word_pos):\n",
    "    accepted_values = max_values > 0.9\n",
    "    index = index[accepted_values]\n",
    "    pos_tags = pos_tags[accepted_values]\n",
    "    word_pos = word_pos[accepted_values]\n",
    "\n",
    "    # I should filter high repetition words here!\n",
    "    word_added_to_train_freq[word_pos.long()] += 1\n",
    "    accepted_by_frequency = word_added_to_train_freq[word_pos.long()] < 10\n",
    "    index = index[accepted_by_frequency]\n",
    "    pos_tags = pos_tags[accepted_by_frequency]\n",
    "\n",
    "    index_global = index + padding\n",
    "    node_cover.extend(index_global.tolist())\n",
    "    pos_labels[index_global, pos_tags] = 1\n",
    "\n",
    "# This function iterates a dataset to creates the tag vectors and update training structures\n",
    "def get_words_tag_frequence(model, word_count, class_count, data_loader, tag_frequencies=None, from_gold_data=False, node_cover=None, pos_labels=None, mask_language=True):\n",
    "    \n",
    "    res = tag_frequencies\n",
    "    if res == None:\n",
    "        res = torch.ones(word_count, class_count)\n",
    "        res[:, :] = 0.0000001\n",
    "    \n",
    "    data_encoder = DataEncoder(data_loader, model, mask_language)\n",
    "    word_added_to_train_freq = torch.zeros(word_count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for z, verse, i, batch in data_encoder:\n",
    "            index = batch['pos_index'][0].to(dev)\n",
    "            \n",
    "            if from_gold_data:\n",
    "                tags_onehot = batch['pos_classes'][0]\n",
    "            else:\n",
    "                tags_onehot = model.decoder(z, index, batch)\n",
    "\n",
    "            max_values, pos_tags = torch.max(torch.softmax(tags_onehot, dim=1), 1)\n",
    "            word_pos = batch['x'][0][index, 9]\n",
    "\n",
    "            if not from_gold_data:\n",
    "                if node_cover != None:\n",
    "                    update_trainset_with_predictions(node_cover[verse], pos_labels, batch['padding'][0], index, max_values, pos_tags, word_added_to_train_freq, word_pos)\n",
    "                accepted_values = max_values > 0.5\n",
    "                word_pos = word_pos[accepted_values]\n",
    "                pos_tags = pos_tags[accepted_values]\n",
    "\n",
    "            res[word_pos.long(), pos_tags.long()] += 1\n",
    "\n",
    "\n",
    "\n",
    "    sm = torch.sum(res, dim=1)\n",
    "    res_normalized = (res.transpose(1,0) / sm).transpose(1,0)\n",
    "    \n",
    "    return res_normalized, res\n",
    "\n",
    "def get_data_loadrs_for_target_editions(target_editions, dataset, pos_node_cover, verses, data_dir):\n",
    "    target_pos_labels, target_pos_node_cover = get_language_nodes(dataset, target_editions, pos_node_cover.keys())\n",
    "    gnn_dataset_target_pos = POSTAGGNNDataset(dataset, verses, None, {}, target_pos_node_cover, target_pos_labels, data_dir, group_size = 500)\n",
    "    target_data_loader = DataLoader(gnn_dataset_target_pos, batch_size=1, shuffle=False)\n",
    "    \n",
    "    return target_data_loader\n",
    "\n",
    "# Calls the above functions over any of the datasets (train, grc, heb, blinker) to get pos_tag vectors for each word. (once for source languages and once for target languages)\n",
    "def get_tag_frequencies_node_tags(editions):\n",
    "    train_pos_node_cover_copy, train_pos_labels_copy = deepcopy(train_pos_node_cover), deepcopy(train_pos_labels)\n",
    "    grc_pos_node_cover_copy, grc_pos_labels_copy = deepcopy(grc_pos_node_cover), deepcopy(grc_pos_labels)\n",
    "    heb_pos_node_cover_copy, heb_pos_labels_copy = deepcopy(heb_pos_node_cover), deepcopy(heb_pos_labels)\n",
    "    blinker_pos_node_cover_copy, blinker_pos_labels_copy = deepcopy(blinker_pos_node_cover), deepcopy(blinker_pos_labels)\n",
    "\n",
    "    _, tag_frequencies = get_words_tag_frequence(model, 2354770, len(postag_map), train_data_loader_bigbatch, from_gold_data=True)\n",
    "    _, tag_frequencies = get_words_tag_frequence(model, 2354770, len(postag_map), grc_data_loader_bigbatch, from_gold_data=True, tag_frequencies=tag_frequencies)\n",
    "    _, tag_frequencies = get_words_tag_frequence(model, 2354770, len(postag_map), heb_data_loader_bigbatch, from_gold_data=True, tag_frequencies=tag_frequencies)\n",
    "    _, tag_frequencies = get_words_tag_frequence(model, 2354770, len(postag_map), blinker_data_loader_bigbatch, from_gold_data=True, tag_frequencies=tag_frequencies)\n",
    "\n",
    "    \n",
    "    target_data_loader = get_data_loadrs_for_target_editions(editions, train_dataset, train_pos_node_cover, train_verses, data_dir_train)\n",
    "    _, tag_frequencies_target = get_words_tag_frequence(model, 2354770, len(postag_map), target_data_loader, node_cover=train_pos_node_cover_copy, pos_labels=train_pos_labels_copy)\n",
    "    target_data_loader = get_data_loadrs_for_target_editions(editions, grc_test_dataset, grc_pos_node_cover, grc_test_verses, data_dir_grc)\n",
    "    _, tag_frequencies_target = get_words_tag_frequence(model, 2354770, len(postag_map), target_data_loader, node_cover=grc_pos_node_cover_copy, pos_labels=grc_pos_labels_copy, tag_frequencies=tag_frequencies_target)\n",
    "    target_data_loader = get_data_loadrs_for_target_editions(editions, heb_test_dataset, heb_pos_node_cover, heb_test_verses, data_dir_heb)\n",
    "    _, tag_frequencies_target = get_words_tag_frequence(model, 2354770, len(postag_map), target_data_loader, node_cover=heb_pos_node_cover_copy, pos_labels=heb_pos_labels_copy, tag_frequencies=tag_frequencies_target)\n",
    "    target_data_loader = get_data_loadrs_for_target_editions(editions, blinker_test_dataset, blinker_pos_node_cover, blinker_verses, data_dir_blinker)\n",
    "    _, tag_frequencies_target = get_words_tag_frequence(model, 2354770, len(postag_map), target_data_loader, node_cover=blinker_pos_node_cover_copy, pos_labels=blinker_pos_labels_copy, tag_frequencies=tag_frequencies_target)\n",
    "\n",
    "    tag_frequencies += tag_frequencies_target\n",
    "    return tag_frequencies, tag_frequencies_target, train_pos_node_cover_copy, train_pos_labels_copy, grc_pos_node_cover_copy, grc_pos_labels_copy, heb_pos_node_cover_copy, heb_pos_labels_copy, blinker_pos_node_cover_copy, blinker_pos_labels_copy\n",
    "\n",
    "res_ = get_tag_frequencies_node_tags(['yor-x-bible-2010'])\n",
    "tag_frequencies, tag_frequencies_target, train_pos_node_cover_copy, train_pos_labels_copy, grc_pos_node_cover_copy, grc_pos_labels_copy, heb_pos_node_cover_copy, heb_pos_labels_copy, blinker_pos_node_cover_copy, blinker_pos_labels_copy = res_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies_target = torch.sum(tag_frequencies_target, dim=1)\n",
    "tag_frequencies_copy = tag_frequencies.detach().clone()\n",
    "\n",
    "tag_frequencies_copy[torch.logical_and(word_frequencies_target>0.1, word_frequencies_target<3), :] = 0.0000001\n",
    "\n",
    "# We have to give uniform noise to some training examples to prevent the model from returning one of the most frequent tags always!!\n",
    "uniform_noise = torch.BoolTensor(tag_frequencies.size(0))\n",
    "uniform_noise[:] = True\n",
    "shuffle_tensor = torch.randperm(tag_frequencies.size(0))[:int(tag_frequencies.size(0)*0.7)]\n",
    "uniform_noise[shuffle_tensor] = False\n",
    "tag_frequencies_copy[torch.logical_and(uniform_noise, word_frequencies_target < 0.1), :] = 0.0000001\n",
    "\n",
    "sm = torch.sum(tag_frequencies_copy, dim=1)\n",
    "normalized_tag_frequencies = (tag_frequencies_copy.transpose(1,0) / sm).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24078/24078 [09:09<00:00, 43.85it/s]\n",
      "100%|| 2225/2225 [00:43<00:00, 51.45it/s]\n",
      "100%|| 783/783 [00:27<00:00, 28.74it/s]\n",
      "100%|| 250/250 [00:06<00:00, 39.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#importlib.reload(afeatures)\n",
    "#initial_model = model\n",
    "#features.append(afeatures.MappingFeature(len(postag_map), 'tag_priors', freeze=True))\n",
    "torch.cuda.set_device(1)\n",
    "#create_model(tag_frequencies=True)\n",
    "yoruba_postags = generate_target_lang_tags('yor-x-bible-2010', f\"posfeatTrue_transformerFalse_trainWEFalse\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#normalized_gold_frequencies, gold_frequencies_all = get_words_tag_frequence(model, 2354770, len(postag_map), english_data_loader, from_gold_data=True)\n",
    "\n",
    "#gold_frequencies_all = gold_frequencies\n",
    "word_frequencies = torch.sum(gold_frequencies_all, dim=1)\n",
    "\n",
    "subjectword_indices =  word_frequencies > 0.1\n",
    "print(word_frequencies.shape)\n",
    "gold_frequencies = gold_frequencies_all[subjectword_indices, :]\n",
    "predicted_frequencies = tag_frequencies_english[subjectword_indices, :]\n",
    "wordtype_frequencies = word_frequencies[subjectword_indices]\n",
    "print(gold_frequencies.shape)\n",
    "\n",
    "_, gold_tags = torch.max(gold_frequencies, dim=1)\n",
    "_, predicted_tags = torch.max(predicted_frequencies, dim=1)\n",
    "\n",
    "sorted_wordtype_frequencies, sort_pattern = torch.sort(wordtype_frequencies, descending=True)\n",
    "\n",
    "sorted_gold_tags = gold_tags[sort_pattern]\n",
    "sorted_predicted_tags = predicted_tags[sort_pattern]\n",
    "quarter_size = int(sorted_gold_tags.size(0)/2.0)\n",
    "\n",
    "print('quarter size', quarter_size)\n",
    "print(\"general accuracy\", torch.sum(gold_tags == predicted_tags)/predicted_tags.size(0))\n",
    "print('first quarter accuracy', torch.sum(sorted_gold_tags[:quarter_size] == sorted_predicted_tags[:quarter_size])/quarter_size)\n",
    "print('last part accuracy', torch.sum(sorted_gold_tags[1*quarter_size:] == sorted_predicted_tags[1*quarter_size:])/sorted_predicted_tags[1*quarter_size:].size(0))\n",
    "\n",
    "print('total token count', torch.sum(wordtype_frequencies))\n",
    "print('first quarter words token count', torch.sum(word_frequencies[:quarter_size]))\n",
    "\n",
    "print('1st frequency', sorted_wordtype_frequencies[0])\n",
    "print('10st frequency', sorted_wordtype_frequencies[10])\n",
    "print('100st frequency', sorted_wordtype_frequencies[100])\n",
    "print('100st frequency', sorted_wordtype_frequencies[736])\n",
    "print('1000st frequency', sorted_wordtype_frequencies[1000])\n",
    "print('10000st frequency', sorted_wordtype_frequencies[10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.pop()\n",
    "#features.append(afeatures.MappingFeature(len(postag_map), 'tag_priors'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized_tag_frequencies = torch.softmax(tag_frequencies_copy, dim=1)\n",
    "\n",
    "sm = torch.sum(tag_frequencies_copy, dim=1)\n",
    "normalized_tag_frequencies = (tag_frequencies_copy.transpose(1,0) / sm).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_lang_postags(dataset, data_loader, edit, mask_language):\n",
    "    model.eval()\n",
    "    res = {}\n",
    "    data_endoer = DataEncoder(data_loader, model, mask_language)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for z, verse, _, batch in data_endoer:\n",
    "            if verse in dataset.nodes_map[edit]:\n",
    "\n",
    "                index = []\n",
    "                toks = list(dataset.nodes_map[edit][verse].keys())\n",
    "                for i in toks:\n",
    "                    index.append(dataset.nodes_map[edit][verse][i])\n",
    "                index = torch.LongTensor(index).to(dev) - batch['padding'][0]\n",
    "\n",
    "                preds = model.decoder(z, index, batch)\n",
    "\n",
    "                _, predicted = torch.max(preds, 1)\n",
    "\n",
    "                res[verse] = {toks[i]:predicted[i].item() for i in range(len(toks))}\n",
    "\n",
    "    return res\n",
    "\n",
    "def generate_target_lang_tags(target_lang, params, mask_language):\n",
    "    target_pos_tags = {}\n",
    "     \n",
    "    res_ = get_target_lang_postags(train_dataset, train_data_loader_bigbatch, target_lang, mask_language)\n",
    "    target_pos_tags.update(res_)\n",
    "\n",
    "    res_ = get_target_lang_postags(heb_test_dataset, heb_data_loader_bigbatch, target_lang, mask_language)\n",
    "    target_pos_tags.update(res_)\n",
    "\n",
    "    res_ = get_target_lang_postags(grc_test_dataset, grc_data_loader_bigbatch, target_lang, mask_language)\n",
    "    target_pos_tags.update(res_)\n",
    "\n",
    "    res_ = get_target_lang_postags(blinker_test_dataset, blinker_data_loader_bigbatch, target_lang, mask_language)\n",
    "    target_pos_tags.update(res_)\n",
    "\n",
    "    torch.save(target_pos_tags, f'/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_{params}_maskLang{mask_language}.pickle')\n",
    "    return target_pos_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global model, decoder\n",
    "#1/0\n",
    "\n",
    "decoder = None\n",
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 1, 'out_dim': 20, 'global_normalize': False, 'name': 'editf', 'Active': True, 'n_classes': 83}\n",
      "{'type': 1, 'out_dim': 32, 'global_normalize': False, 'name': 'position', 'Active': True, 'n_classes': 150}\n",
      "{'type': 3, 'out_dim': 4, 'global_normalize': False, 'name': 'degree_centrality', 'Active': True}\n",
      "{'type': 3, 'out_dim': 4, 'global_normalize': False, 'name': 'closeness_centrality', 'Active': True}\n",
      "{'type': 3, 'out_dim': 4, 'global_normalize': False, 'name': 'betweenness_centrality', 'Active': True}\n",
      "{'type': 3, 'out_dim': 4, 'global_normalize': False, 'name': 'load_centrality', 'Active': True}\n",
      "{'type': 3, 'out_dim': 4, 'global_normalize': False, 'name': 'harmonic_centrality', 'Active': True}\n",
      "{'type': 1, 'out_dim': 32, 'global_normalize': False, 'name': 'greedy_modularity_community', 'Active': True, 'n_classes': 250}\n",
      "{'type': 1, 'out_dim': 32, 'global_normalize': False, 'name': 'label_propagation_community', 'Active': True, 'n_classes': 250}\n",
      "{'type': 6, 'out_dim': 100, 'global_normalize': False, 'name': 'word', 'Active': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#features = blinker_test_dataset.features[:]\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "from pprint import pprint\n",
    "#print('indim',in_dim)\n",
    "#features[-1].out_dim = 50\n",
    "for i in features:\n",
    "    #if i.type==3:\n",
    "    #    i.out_dim=4\n",
    "    print(vars(i))\n",
    "\n",
    "#sum(p.out_dim for p in features)\n",
    "#train_dataset.features.pop()\n",
    "#train_dataset.features[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "#train_dataset.features[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "#train_dataset.features[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "#train_dataset.features[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "#train_dataset.features[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "#train_dataset.features[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "#train_dataset.features[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "#train_dataset.features[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "##train_dataset.features.append(afeatures.MappingFeature(100, 'word'))\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\")\n",
    "#torch.save(train_dataset.features[-3], \"./features.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_map = train_dataset.nodes_map\n",
    "bad_edition_files = []\n",
    "for edit in nodes_map:\n",
    "    bad_count = 0\n",
    "    for verse in nodes_map[edit]:\n",
    "        if len(nodes_map[edit][verse].keys()) < 2:\n",
    "            bad_count += 1\n",
    "        if bad_count > 1:\n",
    "            bad_edition_files.append(edit)\n",
    "            break\n",
    "print(bad_edition_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_japanese_nodes = set()\n",
    "nodes_map = train_dataset.nodes_map\n",
    "\n",
    "for bad_editionf in bad_edition_files:\n",
    "    for verse in nodes_map[bad_editionf]:\n",
    "        for item in nodes_map[bad_editionf][verse].items():\n",
    "            all_japanese_nodes.add(item[1])\n",
    "\n",
    "print(\" all japansese nodes: \", len(all_japanese_nodes))\n",
    "edge_index = train_dataset.edge_index.to('cpu')\n",
    "remaining_edges_index = []\n",
    "for i in tqdm(range(0, edge_index.shape[1], 2)):\n",
    "    if edge_index[0, i].item() not in all_japanese_nodes and edge_index[0, i+1].item() not in all_japanese_nodes:\n",
    "        remaining_edges_index.extend([i, i+1])\n",
    "\n",
    "print('original total edges count', edge_index.shape)\n",
    "print('remaining edge count', len(remaining_edges_index))\n",
    "train_dataset.edge_index = edge_index[:, remaining_edges_index]\n",
    "train_dataset.edge_index.shape\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab640873abb67ad450730b814c8d7de015aa287a6fc3bae4b7154b533e57676"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('multalign_graph': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
