{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "my_baseline_dir = '/mounts/data/proj/ayyoob/POS_tagging/baselines/'\n",
    "def generate_text_from_ramy_annotation(f):\n",
    "    f_path = '/mounts/work/silvia/POS/data/baseline_data/2021/'+f\n",
    "    o_path =  my_baseline_dir + f\n",
    "\n",
    "    with codecs.open(f_path, 'r', 'utf-8') as fi, codecs.open(o_path, 'w',  'utf-8') as fo:\n",
    "        for i, line in enumerate(fi):\n",
    "            items = line.strip().split()\n",
    "            text = [item.split('_')[0]  for item in items]\n",
    "            text = ' '.join(text)\n",
    "            fo.write(str(i) + '\\t' + text + '\\n')\n",
    "\n",
    "ramy_file= 'POR-SPA-bible-POSUD-TRAIN.txt'\n",
    "ramy_file_hi = 'HIN-ENG-bible-POSUD-TRAIN.txt'\n",
    "ramy_file_fa = 'PES-SPA-bible-POSUD-TRAIN.txt'\n",
    "generate_text_from_ramy_annotation(ramy_file_fa)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1168961c15a64798b24cd6e62e17d88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 10:10:59 INFO: Downloading default packages for language: fa (Persian)...\n",
      "2022-03-30 10:11:00 INFO: File exists: /mounts/Users/student/ayyoob/stanza_resources/fa/default.zip.\n",
      "2022-03-30 10:11:02 INFO: Finished downloading models and saved to /mounts/Users/student/ayyoob/stanza_resources.\n",
      "2022-03-30 10:11:02 WARNING: Language fa package default expects mwt, which has been added\n",
      "2022-03-30 10:11:02 INFO: Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "=======================\n",
      "\n",
      "2022-03-30 10:11:02 INFO: Use device: gpu\n",
      "2022-03-30 10:11:02 INFO: Loading: tokenize\n",
      "2022-03-30 10:11:02 INFO: Loading: mwt\n",
      "2022-03-30 10:11:02 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len is 335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 10:11:02 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 37 tokens =======\n",
      "====== Sentence 43 tokens =======\n",
      "====== Sentence 77 tokens =======\n",
      "====== Sentence 410 tokens =======\n",
      "====== Sentence 452 tokens =======\n",
      "====== Sentence 460 tokens =======\n",
      "====== Sentence 522 tokens =======\n",
      "====== Sentence 769 tokens =======\n",
      "====== Sentence 1062 tokens =======\n",
      "====== Sentence 1114 tokens =======\n",
      "====== Sentence 1177 tokens =======\n",
      "====== Sentence 1188 tokens =======\n",
      "====== Sentence 1214 tokens =======\n",
      "====== Sentence 1300 tokens =======\n",
      "====== Sentence 1335 tokens =======\n",
      "====== Sentence 1398 tokens =======\n",
      "====== Sentence 1605 tokens =======\n",
      "====== Sentence 1948 tokens =======\n",
      "====== Sentence 1969 tokens =======\n",
      "====== Sentence 2065 tokens =======\n",
      "====== Sentence 2209 tokens =======\n",
      "====== Sentence 2266 tokens =======\n",
      "====== Sentence 2278 tokens =======\n",
      "====== Sentence 2766 tokens =======\n",
      "====== Sentence 2854 tokens =======\n",
      "====== Sentence 3026 tokens =======\n",
      "====== Sentence 3255 tokens =======\n",
      "====== Sentence 3259 tokens =======\n",
      "====== Sentence 3284 tokens =======\n",
      "====== Sentence 3324 tokens =======\n",
      "====== Sentence 3371 tokens =======\n",
      "====== Sentence 3431 tokens =======\n",
      "====== Sentence 3483 tokens =======\n",
      "====== Sentence 3516 tokens =======\n",
      "====== Sentence 3800 tokens =======\n",
      "====== Sentence 3858 tokens =======\n",
      "====== Sentence 3914 tokens =======\n",
      "====== Sentence 4004 tokens =======\n",
      "====== Sentence 4129 tokens =======\n",
      "====== Sentence 4135 tokens =======\n",
      "====== Sentence 4513 tokens =======\n",
      "====== Sentence 4538 tokens =======\n",
      "====== Sentence 4673 tokens =======\n",
      "====== Sentence 5039 tokens =======\n",
      "====== Sentence 5169 tokens =======\n",
      "====== Sentence 5232 tokens =======\n",
      "====== Sentence 5289 tokens =======\n",
      "====== Sentence 5522 tokens =======\n",
      "====== Sentence 5686 tokens =======\n",
      "====== Sentence 5786 tokens =======\n",
      "====== Sentence 5999 tokens =======\n",
      "====== Sentence 6054 tokens =======\n",
      "====== Sentence 6103 tokens =======\n",
      "====== Sentence 6297 tokens =======\n",
      "====== Sentence 6419 tokens =======\n",
      "====== Sentence 6466 tokens =======\n",
      "====== Sentence 6651 tokens =======\n",
      "====== Sentence 6762 tokens =======\n",
      "====== Sentence 6881 tokens =======\n",
      "====== Sentence 7043 tokens =======\n",
      "====== Sentence 7402 tokens =======\n",
      "====== Sentence 7486 tokens =======\n",
      "====== Sentence 7562 tokens =======\n",
      "====== Sentence 7623 tokens =======\n",
      "====== Sentence 7686 tokens =======\n",
      "====== Sentence 7705 tokens =======\n",
      "====== Sentence 7713 tokens =======\n",
      "====== Sentence 8007 tokens =======\n",
      "====== Sentence 8371 tokens =======\n",
      "====== Sentence 8406 tokens =======\n",
      "====== Sentence 8438 tokens =======\n",
      "====== Sentence 8626 tokens =======\n",
      "====== Sentence 8915 tokens =======\n",
      "====== Sentence 8948 tokens =======\n",
      "====== Sentence 9129 tokens =======\n",
      "====== Sentence 9155 tokens =======\n",
      "====== Sentence 9298 tokens =======\n",
      "====== Sentence 9596 tokens =======\n",
      "====== Sentence 9673 tokens =======\n",
      "====== Sentence 9922 tokens =======\n",
      "====== Sentence 9946 tokens =======\n",
      "====== Sentence 9993 tokens =======\n",
      "====== Sentence 10042 tokens =======\n",
      "====== Sentence 10148 tokens =======\n",
      "====== Sentence 10435 tokens =======\n",
      "====== Sentence 10600 tokens =======\n",
      "====== Sentence 10740 tokens =======\n",
      "====== Sentence 10743 tokens =======\n",
      "====== Sentence 10845 tokens =======\n",
      "====== Sentence 10916 tokens =======\n",
      "====== Sentence 11133 tokens =======\n",
      "====== Sentence 11391 tokens =======\n",
      "====== Sentence 11573 tokens =======\n",
      "====== Sentence 11725 tokens =======\n",
      "====== Sentence 11873 tokens =======\n",
      "====== Sentence 11887 tokens =======\n",
      "====== Sentence 11935 tokens =======\n",
      "====== Sentence 11950 tokens =======\n",
      "====== Sentence 11953 tokens =======\n",
      "====== Sentence 12002 tokens =======\n",
      "====== Sentence 12107 tokens =======\n",
      "====== Sentence 12162 tokens =======\n",
      "====== Sentence 12255 tokens =======\n",
      "====== Sentence 12307 tokens =======\n",
      "====== Sentence 12428 tokens =======\n",
      "====== Sentence 12809 tokens =======\n",
      "====== Sentence 12979 tokens =======\n",
      "====== Sentence 13087 tokens =======\n",
      "====== Sentence 13253 tokens =======\n",
      "====== Sentence 13397 tokens =======\n",
      "====== Sentence 13420 tokens =======\n",
      "====== Sentence 13449 tokens =======\n",
      "====== Sentence 13529 tokens =======\n",
      "====== Sentence 13541 tokens =======\n",
      "====== Sentence 13803 tokens =======\n",
      "====== Sentence 13956 tokens =======\n",
      "====== Sentence 13966 tokens =======\n",
      "====== Sentence 14035 tokens =======\n",
      "====== Sentence 14135 tokens =======\n",
      "====== Sentence 14306 tokens =======\n",
      "====== Sentence 14449 tokens =======\n",
      "====== Sentence 14693 tokens =======\n",
      "====== Sentence 14787 tokens =======\n",
      "====== Sentence 15078 tokens =======\n",
      "====== Sentence 15174 tokens =======\n",
      "====== Sentence 15268 tokens =======\n",
      "====== Sentence 15271 tokens =======\n",
      "====== Sentence 15331 tokens =======\n",
      "====== Sentence 15441 tokens =======\n",
      "====== Sentence 15664 tokens =======\n",
      "====== Sentence 15686 tokens =======\n",
      "====== Sentence 15801 tokens =======\n",
      "====== Sentence 15849 tokens =======\n",
      "====== Sentence 15967 tokens =======\n",
      "====== Sentence 16214 tokens =======\n",
      "====== Sentence 16227 tokens =======\n",
      "====== Sentence 16337 tokens =======\n",
      "====== Sentence 16400 tokens =======\n",
      "====== Sentence 16498 tokens =======\n",
      "====== Sentence 16639 tokens =======\n",
      "====== Sentence 16665 tokens =======\n",
      "====== Sentence 16677 tokens =======\n",
      "====== Sentence 16678 tokens =======\n",
      "====== Sentence 16750 tokens =======\n",
      "====== Sentence 16763 tokens =======\n",
      "====== Sentence 16811 tokens =======\n",
      "====== Sentence 16868 tokens =======\n",
      "====== Sentence 16943 tokens =======\n",
      "====== Sentence 16945 tokens =======\n",
      "====== Sentence 16961 tokens =======\n",
      "====== Sentence 16991 tokens =======\n",
      "====== Sentence 16997 tokens =======\n",
      "====== Sentence 17010 tokens =======\n",
      "====== Sentence 17110 tokens =======\n",
      "====== Sentence 17138 tokens =======\n",
      "====== Sentence 17341 tokens =======\n",
      "====== Sentence 17460 tokens =======\n",
      "====== Sentence 17512 tokens =======\n",
      "====== Sentence 17580 tokens =======\n",
      "====== Sentence 17596 tokens =======\n",
      "====== Sentence 17648 tokens =======\n",
      "====== Sentence 18082 tokens =======\n",
      "====== Sentence 18125 tokens =======\n",
      "====== Sentence 18563 tokens =======\n",
      "====== Sentence 18640 tokens =======\n",
      "====== Sentence 18716 tokens =======\n",
      "====== Sentence 18912 tokens =======\n",
      "====== Sentence 18954 tokens =======\n",
      "====== Sentence 18956 tokens =======\n",
      "====== Sentence 18960 tokens =======\n",
      "====== Sentence 19039 tokens =======\n",
      "====== Sentence 19345 tokens =======\n",
      "====== Sentence 19503 tokens =======\n",
      "====== Sentence 19527 tokens =======\n",
      "====== Sentence 19627 tokens =======\n",
      "====== Sentence 19685 tokens =======\n",
      "====== Sentence 19709 tokens =======\n",
      "====== Sentence 19715 tokens =======\n",
      "====== Sentence 19739 tokens =======\n",
      "====== Sentence 19814 tokens =======\n",
      "====== Sentence 19962 tokens =======\n",
      "====== Sentence 20025 tokens =======\n",
      "====== Sentence 20357 tokens =======\n",
      "====== Sentence 20374 tokens =======\n",
      "====== Sentence 20494 tokens =======\n",
      "====== Sentence 20637 tokens =======\n",
      "====== Sentence 20707 tokens =======\n",
      "len pos_tags 20722\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use STANZA POS tagger to tag high-resource languages.\n",
    "To run change:\n",
    "- code\n",
    "- out\n",
    "- bible\n",
    "\"\"\"\n",
    "\n",
    "import stanza\n",
    "import logging\n",
    "import random\n",
    "\n",
    "# code = 'it'\n",
    "# code = 'nl'\n",
    "# code = 'es'\n",
    "# code = 'da'\n",
    "# code = 'cs'\n",
    "# code = 'de'\n",
    "# code = 'fr'\n",
    "# code = 'sv'\n",
    "# code = 'pl'\n",
    "# code = 'en'\n",
    "#code = 'pt'\n",
    "# code = 'ar' \n",
    "#code = 'hi'\n",
    "code = 'fa'\n",
    "# code = 'ga' # irish\n",
    "# code = 'ru' \n",
    "# code = 'zh' \n",
    "# code = 'hu' \n",
    "# code = 'ur' \n",
    "# code = 'el' \n",
    "# code = 'he' \n",
    "# code = 'cs' # check\n",
    "\n",
    "stanza.download(code)\n",
    "def tag_sentence(sentences):\n",
    "    logging.getLogger().setLevel(logging.CRITICAL)\n",
    "    # nlp = stanza.Pipeline(lang='fa', processors='tokenize,mwt,pos', tokenize_pretokenized=True)\n",
    "    # nlp = stanza.Pipeline(lang=code, processors='tokenize,mwt,pos', tokenize_pretokenized=True, pos_batch_size=300)\n",
    "    nlp = stanza.Pipeline(lang=code, processors='tokenize,pos', tokenize_pretokenized=True, pos_batch_size=300)\n",
    "    # print(sentences)\n",
    "    doc = nlp(sentences)\n",
    "    list_tags = []\n",
    "    for i, sent in enumerate(doc.sentences):\n",
    "        if random.randint(0,100) == 0:\n",
    "            print(f'====== Sentence {i+1} tokens =======')\n",
    "        app = []\n",
    "        for word in sent.words:\n",
    "            # print(word.text, word.upos)\n",
    "            app.append([word.text,word.upos])\n",
    "        list_tags.append(app)\n",
    "    return list_tags\n",
    "    # print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "#bible_file_name = 'por-x-bible-versaointernacional.txt'\n",
    "bible_file_name = 'pes-x-bible-newmillennium2011.txt'\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/tam-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/fin-x-bible-helfi.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/prs-x-bible-goodnews.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/ita-x-bible-2009.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/nld-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/nld-x-bible-2007.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/spa-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/spa-x-bible-hablahoi-latina.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/dan-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/ces-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/deu-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/fra-x-bible-louissegond.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/swe-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/pol-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/eng-x-bible-mixed.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/por-x-bible-versaointernacional.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/arb-x-bible.conllu\", \"w+\")\n",
    "out = open(my_baseline_dir + ramy_file_fa + 'conllu', \"w\")\n",
    "#out = open(my_baseline_dir + bible_file_name + 'conllu', \"w\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/gle-x-bible.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/deu-x-bible-bolsinger.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/rus-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/zho-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/hun-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/urd-x-bible-2007.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/ell-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/heb-x-bible-helfi.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/ces-x-bible-newworld.conllu\", \"w+\")\n",
    "\n",
    "\n",
    "# bible_file = \"/nfs/datc/pbc/prs-x-bible-goodnews.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/tam-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/ita-x-bible-2009.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/nld-x-bible-2007.txt\" #############\n",
    "# bible_file = \"/nfs/datc/pbc/nld-x-bible-newworld.txt\" #############\n",
    "# bible_file = \"/nfs/datc/pbc/spa-x-bible-hablahoi-latina.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/spa-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/dan-x-bible-newworld.txt\" #############\n",
    "# bible_file = \"/nfs/datc/pbc/ces-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/deu-x-bible-newworld.txt\" # double check\n",
    "# bible_file = \"/nfs/datc/pbc/fra-x-bible-louissegond.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/swe-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/pol-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/por-x-bible-versaointernacional.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/arb-x-bible.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/hin-x-bible-newworld.txt\"\n",
    "#bible_file = \"/nfs/datc/pbc/hin-x-bible-bsi.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/gle-x-bible.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/deu-x-bible-bolsinger.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/rus-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/zho-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/hun-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/urd-x-bible-2007.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/ell-x-bible-newworld.txt\"\n",
    "#bible_file = \"/nfs/datc/pbc/\" + bible_file_name\n",
    "\n",
    "bible_file = my_baseline_dir + ramy_file_fa\n",
    "\n",
    "# bible_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/eng-x-bible-mixed.txt\"\n",
    "# bible_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/heb-x-bible-helfi.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bible_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/fin-x-bible-helfi.txt\"\n",
    "sentences = \"\"\n",
    "count = 0\n",
    "MAX_LEN = 0\n",
    "with open(bible_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        if len(l)<2:\n",
    "            continue\n",
    "        if len(l[1])>MAX_LEN:\n",
    "            MAX_LEN = len(l[1])\n",
    "        sentences+=l[1]+\"\\n\"\n",
    "        count+=1\n",
    "        # if count==3:\n",
    "        #     break\n",
    "\n",
    "print('max len is', MAX_LEN)\n",
    "upos_tags = tag_sentence(sentences)\n",
    "print('len pos_tags', len(upos_tags))\n",
    "\n",
    "sent_num = 0\n",
    "with open(bible_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        l = line.strip().rstrip().split(\"\\t\")\n",
    "        if len(l)<2:\n",
    "            continue\n",
    "        out.write(F\"# sent_id = {l[0]}\\n\")\n",
    "        out.write(F\"# text = {l[1]}\\n\")\n",
    "        # upos_tags = tag_sentence(l[1])\n",
    "        # l[1] = l[1].replace(\"  \", \" \")\n",
    "        for i,w in enumerate(l[1].split(\" \")):\n",
    "\n",
    "            if not w.lower()==upos_tags[sent_num][i][0].lower():\n",
    "                print(\"Error!\")\n",
    "                print(w)\n",
    "                print(l[1])\n",
    "                print(l[1].split(\" \"))\n",
    "                print(upos_tags[sent_num][i][0].lower())\n",
    "                break\n",
    "            out.write(F\"{i+1}\\t{w}\\t{w.lower()}\\t{upos_tags[sent_num][i][1]}\\t-\\t-\\t-\\t-\\t-\\t-\\n\")\n",
    "\n",
    "        sent_num+=1\n",
    "        out.write(\"\\n\")\n",
    "        # if sent_num==3:\n",
    "        #     break\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_file_name = 'hin-x-bible-bsi.txt'\n",
    "postag_map = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "\n",
    "def read_conllu_file(f_path):\n",
    "\ttags = {}\n",
    "\twith codecs.open(f_path, \"r\", \"utf-8\") as lang_pos:\n",
    "\t\ttag_sent = []\n",
    "\t\tsent_id = \"\"\n",
    "\t\tfor sline in lang_pos:\n",
    "\t\t\tsline = sline.strip()\n",
    "\t\t\tif sline == \"\":\n",
    "\t\t\t\ttags[sent_id] = [p[3].upper() for p in tag_sent]\n",
    "\t\t\t\ttag_sent = []\n",
    "\t\t\t\tsent_id = \"\"\n",
    "\t\t\telif \"# verse_id\" in sline or '# sent_id' in sline:\n",
    "\t\t\t\tsent_id = sline.split()[-1]\n",
    "\t\t\telif sline[0] == \"#\":\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\ttag_sent.append(sline.split(\"\\t\"))\n",
    "\t\n",
    "\treturn tags\n",
    "\n",
    "silver_ramy_pt = read_conllu_file(my_baseline_dir + ramy_file + 'conllu')\n",
    "silver_ours_pt2 = read_conllu_file(my_baseline_dir + 'por-x-bible-newworld1996.txt' + 'conllu')\n",
    "#silver_ours_hi = read_conllu_file(my_baseline_dir + 'hin-x-bible-newworld.txt' + 'conllu')\n",
    "silver_ours_hi2 = read_conllu_file(my_baseline_dir + bible_file_name + 'conllu')\n",
    "#silver_ours_fa = read_conllu_file(my_baseline_dir + 'prs-x-bible-goodnews.txt' + 'conllu')\n",
    "silver_ours_fa2 = read_conllu_file(my_baseline_dir + 'pes-x-bible-newmillennium2011.txt' + 'conllu')\n",
    "silver_ramy_hi = read_conllu_file(my_baseline_dir + ramy_file_hi + 'conllu')\n",
    "silver_ramy_fa = read_conllu_file(my_baseline_dir + ramy_file_fa + 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\",  \n",
    "              \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "def read_ramy_annotation(f):\n",
    "    f_path = '/mounts/work/silvia/POS/data/baseline_data/2021/'+f\n",
    "    res = {}\n",
    "\n",
    "    with codecs.open(f_path, 'r', 'utf-8') as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            res[str(i)] = {}\n",
    "            items = line.strip().split()\n",
    "            for j,item in enumerate(items):\n",
    "                tag = item.split('_')[1]\n",
    "                if tag in postag:\n",
    "                    res[str(i)][j] = tag.upper()\n",
    "    \n",
    "    return res\n",
    "\n",
    "bronze_ramy_pt = read_ramy_annotation(ramy_file)\n",
    "bronze_ramy_hi = read_ramy_annotation(ramy_file_hi)\n",
    "bronze_ramy_fa = read_ramy_annotation(ramy_file_fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted percentage 0.7445468367754701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.6485    0.7008    0.6736     12149\n",
      "         ADP     0.7377    0.8861    0.8051     59615\n",
      "         ADV     0.9252    0.7915    0.8531     20528\n",
      "         AUX     0.7521    0.8574    0.8013     12165\n",
      "       CCONJ     0.9907    0.9980    0.9944     29458\n",
      "         DET     0.8315    0.9106    0.8693     62954\n",
      "        INTJ     0.2404    0.5605    0.3365       669\n",
      "        NOUN     0.9548    0.9353    0.9450    110082\n",
      "         NUM     0.9580    0.9253    0.9414      2663\n",
      "        PART     0.0000    0.0000    0.0000         1\n",
      "        PRON     0.8707    0.7745    0.8198     41915\n",
      "       PROPN     0.9502    0.9260    0.9380     42716\n",
      "       PUNCT     0.9999    0.9975    0.9987    128992\n",
      "       SCONJ     0.6583    0.5931    0.6240     15988\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.9195    0.8054    0.8587     71213\n",
      "           X     0.0000    0.0000    0.0000        22\n",
      "\n",
      "   micro avg     0.8968    0.8968    0.8968    611130\n",
      "   macro avg     0.6728    0.6860    0.6740    611130\n",
      "weighted avg     0.9023    0.8968    0.8977    611130\n",
      "\n",
      "accepted percentage 0.5556941136890339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.5469    0.4551    0.4968     10665\n",
      "         ADP     0.9289    0.9256    0.9272     43835\n",
      "         ADV     0.1125    0.6900    0.1935      1545\n",
      "         AUX     0.6611    0.8729    0.7524     10254\n",
      "       CCONJ     0.8914    0.9944    0.9401     18502\n",
      "         DET     0.3253    0.4361    0.3727      5019\n",
      "        INTJ     0.0123    0.0395    0.0187       253\n",
      "        NOUN     0.8735    0.7341    0.7978     83649\n",
      "         NUM     0.9462    0.7481    0.8356      5947\n",
      "        PART     0.9943    0.7561    0.8590      5555\n",
      "        PRON     0.9197    0.8779    0.8983     58177\n",
      "       PROPN     0.7462    0.7770    0.7613     36698\n",
      "       PUNCT     1.0000    0.9998    0.9999     84989\n",
      "       SCONJ     0.8975    0.6088    0.7255      8200\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.6349    0.7803    0.7001     26418\n",
      "           X     0.0000    0.0000    0.0000        12\n",
      "\n",
      "   micro avg     0.8412    0.8412    0.8412    399718\n",
      "   macro avg     0.6171    0.6292    0.6046    399718\n",
      "weighted avg     0.8653    0.8412    0.8489    399718\n",
      "\n",
      "accepted percentage 0.534575533746792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.6678    0.3483    0.4579     10036\n",
      "         ADP     0.9235    0.9155    0.9195     24946\n",
      "         ADV     0.2629    0.5977    0.3652      2722\n",
      "         AUX     0.8740    0.9039    0.8887      4982\n",
      "       CCONJ     0.9665    0.9899    0.9781     15906\n",
      "         DET     0.2150    0.7397    0.3331      3092\n",
      "        INTJ     0.6121    0.7929    0.6909       840\n",
      "        NOUN     0.8822    0.6436    0.7442     79624\n",
      "         NUM     0.9025    0.8326    0.8662      1912\n",
      "        PART     0.0000    0.0000    0.0000        82\n",
      "        PRON     0.5639    0.6650    0.6103     16403\n",
      "       PROPN     0.5468    0.7844    0.6444     21470\n",
      "       PUNCT     1.0000    0.9998    0.9999     66141\n",
      "       SCONJ     0.7466    0.3836    0.5068      9660\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.5851    0.8617    0.6970     15683\n",
      "           X     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.7864    0.7864    0.7864    273499\n",
      "   macro avg     0.5735    0.6152    0.5707    273499\n",
      "weighted avg     0.8294    0.7864    0.7928    273499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "postag = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\",  \n",
    "              \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "\n",
    "def calculate_accuracy(gold, predict):\n",
    "    total = 0\n",
    "    y_true, y_pred = [], []\n",
    "    for s in predict:\n",
    "        total += len(gold[s])\n",
    "        for i in predict[s]:\n",
    "            if i >= len(gold[s]):\n",
    "                if random.randint(0,100)==0:\n",
    "                    print(s,i)\n",
    "                continue\n",
    "            y_true.append(gold[s][i])\n",
    "            y_pred.append(predict[s][i])\n",
    "    print('accepted percentage', len(y_pred)/ total)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred, digits=4, labels=postag, zero_division=0))\n",
    "\n",
    "calculate_accuracy(silver_ramy_pt, bronze_ramy_pt)\n",
    "calculate_accuracy(silver_ramy_hi, bronze_ramy_hi)\n",
    "calculate_accuracy(silver_ramy_fa, bronze_ramy_fa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "from math import log10\n",
    "\n",
    "postag_map = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "postag_reverse_map = {item[1]:item[0] for item in postag_map.items()}\n",
    "new_testament_verses = torch.load('/mounts/Users/student/ayyoob/Dokumente/code/POS-TAGGING/pos_evaluation/new_testament_verses.torch.bin')\n",
    "\n",
    "\n",
    "type_counts = collections.defaultdict(lambda:0)  # number of times a token happened\n",
    "type_tag_counts = collections.defaultdict(lambda: collections.defaultdict(lambda:0))\n",
    "type_tag_probabilities = collections.defaultdict(lambda: collections.defaultdict(lambda:0.0))\n",
    "sentences = {}\n",
    "\n",
    "def read_bible_file(f_name):\n",
    "    with codecs.open('/nfs/datc/pbc/'+f_name, 'r', 'utf-8') as fi:\n",
    "        for l in fi:\n",
    "            l = l.strip()\n",
    "            lparts = l.split('\\t')\n",
    "            if l.startswith('#') or l == '' or len(lparts)<2:\n",
    "                continue\n",
    "            \n",
    "            toks = lparts[1].split()\n",
    "            sentences[lparts[0]] = toks\n",
    "            for tok in toks:\n",
    "                type_counts[tok] +=1\n",
    "\n",
    "def read_our_bronze_file(f_path, bible_file_name):\n",
    "    read_bible_file(bible_file_name)\n",
    "    data = torch.load(f_path)\n",
    "    res = {}\n",
    "\n",
    "    for sent in data:\n",
    "        for item in data[sent]:\n",
    "            type = sentences[sent][item[0]]\n",
    "            type_tag_counts[type][item[1]] += 1\n",
    "            type_tag_probabilities[type][item[1]] += 1\n",
    "\n",
    "    for type in type_tag_probabilities:\n",
    "        for tag in type_tag_probabilities[type]:\n",
    "            type_tag_probabilities[type][tag] /= type_counts[type]\n",
    "\n",
    "    for sent in data:\n",
    "        res[sent] = {}\n",
    "        for item in data[sent]:\n",
    "            type = sentences[sent][item[0]]\n",
    "            p_type_tag = type_tag_probabilities[type][item[1]]\n",
    "            p_mul = item[2] * p_type_tag\n",
    "            p_gross = item[2] + p_type_tag\n",
    "            size_factor = log10(type_counts[type])  \n",
    "            if size_factor * p_mul > 1:# and sent in new_testament_verses: \n",
    "            #if type_counts[type] <= 50 and type_counts[type]>10 and ( p_gross>1.7  ): \n",
    "            #if type_counts[type] > 100 and ((p_type_tag >0.5 and p_mul > 0.50 ) or (p_type_tag <= 0.5 and p_type_tag >0.4 and p_mul > 0.4) ): \n",
    "            #if (type_tag_probabilities[type][item[1]]>0.5 and type_counts[type] < 10 and type_counts[type] >2 and item[2] > 0.9):\n",
    "            #if (type_tag_probabilities[type][item[1]]>0.4 and type_counts[type] >= 2  and item[2] > 0.8):\n",
    "            #if (type_tag_probabilities[type][item[1]]>0.9 and type_counts[type] == 1 and item[2] > 0.95):\n",
    "            #if sent not in new_testament_verses:\n",
    "                res[sent][item[0]] = postag_reverse_map[item[1]]\n",
    "\n",
    "    return res\n",
    "\n",
    "bronze_file_pt2 = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_por-x-bible-newworld1996_15langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_maskLangTrue_epoch1_traintarget0.95_typecheckFalse_20220321-144100_earlystoppingdelta0-GA-channels512_maskLangTrue.pickle'\n",
    "#bronze_file = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_por-x-bible-versaointernacional_11langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_traintarget0.95_typecheckFalse_20220306-232534_earlystopping-GA_maskLangTrue.pickle'\n",
    "bronze_file_hi = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_hin-x-bible-newworld_11langs-nopersian_posfeatTrueealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_traintarget0.95_typecheckFalse_20220306-232534_earlystopping-GA__maskLangTrue.pickle'\n",
    "bronze_file_hi2 = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_hin-x-bible-bsi_15langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_maskLangTrue_epoch1_traintarget0.95_typecheckFalse_20220321-144100_earlystoppingdelta0-GA-channels512_maskLangTrue.pickle'\n",
    "bronze_file_fa = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_prs-x-bible-goodnews_11langs-nopersian_posfeatTrueealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_traintarget0.95_typecheckFalse_20220306-232534_earlystopping-GA__maskLangTrue.pickle'\n",
    "bronze_file_fa2 = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_pes-x-bible-newmillennium2011_15langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_maskLangTrue_epoch1_traintarget0.95_typecheckFalse_20220321-144100_earlystoppingdelta0-GA-channels512_maskLangTrue.pickle'\n",
    "\n",
    "bronze_ours_pt2 = read_our_bronze_file(bronze_file_pt2, 'por-x-bible-newworld1996.txt')\n",
    "#bronze_ours_hi = read_our_bronze_file(bronze_file_hi, 'hin-x-bible-newworld.txt')\n",
    "bronze_ours_hi2 = read_our_bronze_file(bronze_file_hi2, 'hin-x-bible-bsi.txt')\n",
    "#bronze_ours_fa = read_our_bronze_file(bronze_file_fa, 'prs-x-bible-goodnews.txt')\n",
    "bronze_ours_fa2 = read_our_bronze_file(bronze_file_fa2, 'pes-x-bible-newmillennium2011.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43019038 49\n",
      "60003021 38\n",
      "60003021 44\n",
      "42012010 25\n",
      "42012010 29\n",
      "43002009 47\n",
      "30006008 19\n",
      "38008010 38\n",
      "42011042 15\n",
      "38008023 26\n",
      "accepted percentage 0.7246687065493734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.7202    0.4336    0.5413     12718\n",
      "         ADP     0.9574    0.8026    0.8732     97783\n",
      "         ADV     0.1574    0.6820    0.2558      3456\n",
      "         AUX     0.8066    0.5852    0.6783     43054\n",
      "       CCONJ     0.8885    0.9890    0.9361     33791\n",
      "         DET     0.0775    0.2152    0.1140      9573\n",
      "        INTJ     0.0000    0.0000    0.0000       287\n",
      "        NOUN     0.9045    0.8340    0.8679    115149\n",
      "         NUM     0.9783    0.5447    0.6997      8196\n",
      "        PART     0.9540    0.7447    0.8365     14550\n",
      "        PRON     0.8920    0.8049    0.8462     97917\n",
      "       PROPN     0.8079    0.7767    0.7920     33428\n",
      "       PUNCT     0.9974    0.9991    0.9982     94760\n",
      "       SCONJ     0.7606    0.7515    0.7560     12427\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.6227    0.8761    0.7280     67055\n",
      "           X     0.0000    0.0000    0.0000        43\n",
      "\n",
      "   micro avg     0.8164    0.8164    0.8164    644187\n",
      "   macro avg     0.6191    0.5906    0.5837    644187\n",
      "weighted avg     0.8614    0.8164    0.8306    644187\n",
      "\n",
      "accepted percentage 0.5338529214879985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.7261    0.3974    0.5137      2335\n",
      "         ADP     0.9541    0.9846    0.9691     16161\n",
      "         ADV     0.4830    0.5977    0.5343      2048\n",
      "         AUX     0.9585    0.8568    0.9048      3562\n",
      "       CCONJ     0.9319    1.0000    0.9648      7818\n",
      "         DET     0.6575    0.7627    0.7062      1905\n",
      "        INTJ     0.0000    0.0000    0.0000        78\n",
      "        NOUN     0.9526    0.6450    0.7692     21699\n",
      "         NUM     0.9446    0.8736    0.9077       625\n",
      "        PART     0.2195    0.9934    0.3595       152\n",
      "        PRON     0.7072    0.9789    0.8211     10477\n",
      "       PROPN     0.5604    0.8235    0.6669      3184\n",
      "       PUNCT     0.9978    0.9998    0.9988     15641\n",
      "       SCONJ     0.9030    0.5354    0.6722      4970\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.7730    0.9793    0.8640      9830\n",
      "           X     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.8547    0.8547    0.8547    100485\n",
      "   macro avg     0.6335    0.6722    0.6266    100485\n",
      "weighted avg     0.8781    0.8547    0.8515    100485\n",
      "\n",
      "accepted percentage 0.7526229068802651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.6636    0.5281    0.5881     12769\n",
      "         ADP     0.7187    0.9098    0.8031     82278\n",
      "         ADV     0.8000    0.5066    0.6203     28518\n",
      "         AUX     0.7749    0.9229    0.8425     15446\n",
      "       CCONJ     0.9951    0.9940    0.9945     49781\n",
      "         DET     0.8506    0.8561    0.8533     80673\n",
      "        INTJ     0.0000    0.0000    0.0000       466\n",
      "        NOUN     0.9529    0.9313    0.9420    118675\n",
      "         NUM     0.9886    0.9045    0.9447      5446\n",
      "        PART     0.0000    0.0000    0.0000         2\n",
      "        PRON     0.7577    0.8045    0.7804     53802\n",
      "       PROPN     0.9886    0.9197    0.9529     36257\n",
      "       PUNCT     1.0000    0.9961    0.9980    145182\n",
      "       SCONJ     0.6977    0.3339    0.4517     13501\n",
      "         SYM     0.0000    0.0000    0.0000         1\n",
      "        VERB     0.9220    0.7705    0.8395     61979\n",
      "           X     0.0000    0.0000    0.0000        26\n",
      "\n",
      "    accuracy                         0.8766    704802\n",
      "   macro avg     0.6536    0.6105    0.6242    704802\n",
      "weighted avg     0.8901    0.8766    0.8784    704802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(silver_ours_hi2, bronze_ours_hi2)\n",
    "calculate_accuracy(silver_ours_fa2, bronze_ours_fa2)\n",
    "calculate_accuracy(silver_ours_pt2, bronze_ours_pt2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
