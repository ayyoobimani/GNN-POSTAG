{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "my_baseline_dir = '/mounts/data/proj/ayyoob/POS_tagging/baselines/'\n",
    "def generate_text_from_ramy_annotation(f):\n",
    "    f_path = '/mounts/work/silvia/POS/data/baseline_data/2021/'+f\n",
    "    o_path =  my_baseline_dir + f\n",
    "\n",
    "    with codecs.open(f_path, 'r', 'utf-8') as fi, codecs.open(o_path, 'w',  'utf-8') as fo:\n",
    "        for i, line in enumerate(fi):\n",
    "            items = line.strip().split()\n",
    "            text = [item.split('_')[0]  for item in items]\n",
    "            text = ' '.join(text)\n",
    "            fo.write(str(i) + '\\t' + text + '\\n')\n",
    "\n",
    "ramy_file= 'POR-SPA-bible-POSUD-TRAIN.txt'\n",
    "ramy_file_hi = 'HIN-ENG-bible-POSUD-TRAIN.txt'\n",
    "ramy_file_fa = 'PES-SPA-bible-POSUD-TRAIN.txt'\n",
    "ramy_file_id = 'IND-ENG-bible-POSUD-TRAIN.txt'\n",
    "ramy_file_tr = 'TUR-FRA-bible-POSUD-TRAIN.txt'\n",
    "generate_text_from_ramy_annotation(ramy_file_id)\n",
    "generate_text_from_ramy_annotation(ramy_file_tr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4931640567b04a9abdc109a0d28dfd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 15:51:56 INFO: Downloading default packages for language: hi (Hindi)...\n",
      "2022-04-04 15:51:57 INFO: File exists: /mounts/Users/student/ayyoob/stanza_resources/hi/default.zip.\n",
      "2022-04-04 15:51:59 INFO: Finished downloading models and saved to /mounts/Users/student/ayyoob/stanza_resources.\n",
      "2022-04-04 15:51:59 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-04-04 15:51:59 INFO: Use device: gpu\n",
      "2022-04-04 15:51:59 INFO: Loading: tokenize\n",
      "2022-04-04 15:51:59 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len is 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 15:51:59 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 13 tokens =======\n",
      "====== Sentence 227 tokens =======\n",
      "====== Sentence 311 tokens =======\n",
      "====== Sentence 313 tokens =======\n",
      "====== Sentence 375 tokens =======\n",
      "====== Sentence 457 tokens =======\n",
      "====== Sentence 467 tokens =======\n",
      "====== Sentence 935 tokens =======\n",
      "====== Sentence 1017 tokens =======\n",
      "====== Sentence 1052 tokens =======\n",
      "====== Sentence 1072 tokens =======\n",
      "====== Sentence 1103 tokens =======\n",
      "====== Sentence 1725 tokens =======\n",
      "====== Sentence 1901 tokens =======\n",
      "====== Sentence 1906 tokens =======\n",
      "====== Sentence 1908 tokens =======\n",
      "====== Sentence 1978 tokens =======\n",
      "====== Sentence 2036 tokens =======\n",
      "====== Sentence 2628 tokens =======\n",
      "====== Sentence 2801 tokens =======\n",
      "====== Sentence 2870 tokens =======\n",
      "====== Sentence 3387 tokens =======\n",
      "====== Sentence 3533 tokens =======\n",
      "====== Sentence 3599 tokens =======\n",
      "====== Sentence 3777 tokens =======\n",
      "====== Sentence 3788 tokens =======\n",
      "====== Sentence 3830 tokens =======\n",
      "====== Sentence 4198 tokens =======\n",
      "====== Sentence 4219 tokens =======\n",
      "====== Sentence 4222 tokens =======\n",
      "====== Sentence 4268 tokens =======\n",
      "====== Sentence 4364 tokens =======\n",
      "====== Sentence 4371 tokens =======\n",
      "====== Sentence 4382 tokens =======\n",
      "====== Sentence 4557 tokens =======\n",
      "====== Sentence 4622 tokens =======\n",
      "====== Sentence 4630 tokens =======\n",
      "====== Sentence 4631 tokens =======\n",
      "====== Sentence 4656 tokens =======\n",
      "====== Sentence 4676 tokens =======\n",
      "====== Sentence 4841 tokens =======\n",
      "====== Sentence 4846 tokens =======\n",
      "====== Sentence 4976 tokens =======\n",
      "====== Sentence 5075 tokens =======\n",
      "====== Sentence 5420 tokens =======\n",
      "====== Sentence 5867 tokens =======\n",
      "====== Sentence 5878 tokens =======\n",
      "====== Sentence 5882 tokens =======\n",
      "====== Sentence 5904 tokens =======\n",
      "====== Sentence 5941 tokens =======\n",
      "====== Sentence 6158 tokens =======\n",
      "====== Sentence 6189 tokens =======\n",
      "====== Sentence 6282 tokens =======\n",
      "====== Sentence 6498 tokens =======\n",
      "====== Sentence 6530 tokens =======\n",
      "====== Sentence 6546 tokens =======\n",
      "====== Sentence 6643 tokens =======\n",
      "====== Sentence 6947 tokens =======\n",
      "====== Sentence 7286 tokens =======\n",
      "====== Sentence 7290 tokens =======\n",
      "====== Sentence 7328 tokens =======\n",
      "====== Sentence 7424 tokens =======\n",
      "====== Sentence 7478 tokens =======\n",
      "====== Sentence 7480 tokens =======\n",
      "====== Sentence 7652 tokens =======\n",
      "====== Sentence 7674 tokens =======\n",
      "====== Sentence 7757 tokens =======\n",
      "====== Sentence 7922 tokens =======\n",
      "====== Sentence 8085 tokens =======\n",
      "====== Sentence 8099 tokens =======\n",
      "====== Sentence 8116 tokens =======\n",
      "====== Sentence 8328 tokens =======\n",
      "====== Sentence 8441 tokens =======\n",
      "====== Sentence 8457 tokens =======\n",
      "====== Sentence 8682 tokens =======\n",
      "====== Sentence 8709 tokens =======\n",
      "====== Sentence 8717 tokens =======\n",
      "====== Sentence 8727 tokens =======\n",
      "====== Sentence 8841 tokens =======\n",
      "====== Sentence 8867 tokens =======\n",
      "====== Sentence 9045 tokens =======\n",
      "====== Sentence 9122 tokens =======\n",
      "====== Sentence 9130 tokens =======\n",
      "====== Sentence 9196 tokens =======\n",
      "====== Sentence 9212 tokens =======\n",
      "====== Sentence 9292 tokens =======\n",
      "====== Sentence 9305 tokens =======\n",
      "====== Sentence 9481 tokens =======\n",
      "====== Sentence 9530 tokens =======\n",
      "====== Sentence 9560 tokens =======\n",
      "====== Sentence 9957 tokens =======\n",
      "====== Sentence 10151 tokens =======\n",
      "====== Sentence 10255 tokens =======\n",
      "====== Sentence 10333 tokens =======\n",
      "====== Sentence 10650 tokens =======\n",
      "====== Sentence 10710 tokens =======\n",
      "====== Sentence 10794 tokens =======\n",
      "====== Sentence 10833 tokens =======\n",
      "====== Sentence 10929 tokens =======\n",
      "====== Sentence 10958 tokens =======\n",
      "====== Sentence 11252 tokens =======\n",
      "====== Sentence 11263 tokens =======\n",
      "====== Sentence 11274 tokens =======\n",
      "====== Sentence 11349 tokens =======\n",
      "====== Sentence 11380 tokens =======\n",
      "====== Sentence 11511 tokens =======\n",
      "====== Sentence 11515 tokens =======\n",
      "====== Sentence 11544 tokens =======\n",
      "====== Sentence 11579 tokens =======\n",
      "====== Sentence 11668 tokens =======\n",
      "====== Sentence 11709 tokens =======\n",
      "====== Sentence 11931 tokens =======\n",
      "====== Sentence 11969 tokens =======\n",
      "====== Sentence 12206 tokens =======\n",
      "====== Sentence 12210 tokens =======\n",
      "====== Sentence 12309 tokens =======\n",
      "====== Sentence 12369 tokens =======\n",
      "====== Sentence 12372 tokens =======\n",
      "====== Sentence 12433 tokens =======\n",
      "====== Sentence 12707 tokens =======\n",
      "====== Sentence 12793 tokens =======\n",
      "====== Sentence 12816 tokens =======\n",
      "====== Sentence 12906 tokens =======\n",
      "====== Sentence 13083 tokens =======\n",
      "====== Sentence 13197 tokens =======\n",
      "====== Sentence 13219 tokens =======\n",
      "====== Sentence 13312 tokens =======\n",
      "====== Sentence 13413 tokens =======\n",
      "====== Sentence 13641 tokens =======\n",
      "====== Sentence 13806 tokens =======\n",
      "====== Sentence 13893 tokens =======\n",
      "====== Sentence 13963 tokens =======\n",
      "====== Sentence 14048 tokens =======\n",
      "====== Sentence 14091 tokens =======\n",
      "====== Sentence 14122 tokens =======\n",
      "====== Sentence 14141 tokens =======\n",
      "====== Sentence 14242 tokens =======\n",
      "====== Sentence 14354 tokens =======\n",
      "====== Sentence 14463 tokens =======\n",
      "====== Sentence 14607 tokens =======\n",
      "====== Sentence 15041 tokens =======\n",
      "====== Sentence 15044 tokens =======\n",
      "====== Sentence 15055 tokens =======\n",
      "====== Sentence 15056 tokens =======\n",
      "====== Sentence 15242 tokens =======\n",
      "====== Sentence 15328 tokens =======\n",
      "====== Sentence 15339 tokens =======\n",
      "====== Sentence 15617 tokens =======\n",
      "====== Sentence 15767 tokens =======\n",
      "====== Sentence 15873 tokens =======\n",
      "====== Sentence 15910 tokens =======\n",
      "====== Sentence 16052 tokens =======\n",
      "====== Sentence 16088 tokens =======\n",
      "====== Sentence 16553 tokens =======\n",
      "====== Sentence 16612 tokens =======\n",
      "====== Sentence 16764 tokens =======\n",
      "====== Sentence 16884 tokens =======\n",
      "====== Sentence 16892 tokens =======\n",
      "====== Sentence 16966 tokens =======\n",
      "====== Sentence 17312 tokens =======\n",
      "====== Sentence 17383 tokens =======\n",
      "====== Sentence 17417 tokens =======\n",
      "====== Sentence 17549 tokens =======\n",
      "====== Sentence 17659 tokens =======\n",
      "====== Sentence 17801 tokens =======\n",
      "====== Sentence 17809 tokens =======\n",
      "====== Sentence 18184 tokens =======\n",
      "====== Sentence 18248 tokens =======\n",
      "====== Sentence 18386 tokens =======\n",
      "====== Sentence 18636 tokens =======\n",
      "====== Sentence 18763 tokens =======\n",
      "====== Sentence 18770 tokens =======\n",
      "====== Sentence 19068 tokens =======\n",
      "====== Sentence 19089 tokens =======\n",
      "====== Sentence 19144 tokens =======\n",
      "====== Sentence 19220 tokens =======\n",
      "====== Sentence 19298 tokens =======\n",
      "====== Sentence 19328 tokens =======\n",
      "====== Sentence 19345 tokens =======\n",
      "====== Sentence 19352 tokens =======\n",
      "====== Sentence 19474 tokens =======\n",
      "====== Sentence 19548 tokens =======\n",
      "====== Sentence 19579 tokens =======\n",
      "====== Sentence 19589 tokens =======\n",
      "====== Sentence 19638 tokens =======\n",
      "====== Sentence 19654 tokens =======\n",
      "====== Sentence 19708 tokens =======\n",
      "====== Sentence 19852 tokens =======\n",
      "====== Sentence 19880 tokens =======\n",
      "====== Sentence 19905 tokens =======\n",
      "====== Sentence 20015 tokens =======\n",
      "====== Sentence 20048 tokens =======\n",
      "====== Sentence 20060 tokens =======\n",
      "====== Sentence 20281 tokens =======\n",
      "====== Sentence 20442 tokens =======\n",
      "====== Sentence 20548 tokens =======\n",
      "====== Sentence 20572 tokens =======\n",
      "====== Sentence 20638 tokens =======\n",
      "====== Sentence 20798 tokens =======\n",
      "====== Sentence 20867 tokens =======\n",
      "====== Sentence 20872 tokens =======\n",
      "====== Sentence 21072 tokens =======\n",
      "====== Sentence 21129 tokens =======\n",
      "====== Sentence 21193 tokens =======\n",
      "====== Sentence 21292 tokens =======\n",
      "====== Sentence 21598 tokens =======\n",
      "====== Sentence 21661 tokens =======\n",
      "====== Sentence 21666 tokens =======\n",
      "====== Sentence 21743 tokens =======\n",
      "====== Sentence 21774 tokens =======\n",
      "====== Sentence 21886 tokens =======\n",
      "====== Sentence 22124 tokens =======\n",
      "====== Sentence 22240 tokens =======\n",
      "====== Sentence 22377 tokens =======\n",
      "====== Sentence 22518 tokens =======\n",
      "====== Sentence 22542 tokens =======\n",
      "====== Sentence 22765 tokens =======\n",
      "====== Sentence 22795 tokens =======\n",
      "====== Sentence 22850 tokens =======\n",
      "====== Sentence 22868 tokens =======\n",
      "====== Sentence 22938 tokens =======\n",
      "====== Sentence 23147 tokens =======\n",
      "====== Sentence 23265 tokens =======\n",
      "====== Sentence 23492 tokens =======\n",
      "====== Sentence 23499 tokens =======\n",
      "====== Sentence 23572 tokens =======\n",
      "====== Sentence 23780 tokens =======\n",
      "====== Sentence 23890 tokens =======\n",
      "====== Sentence 23938 tokens =======\n",
      "====== Sentence 23943 tokens =======\n",
      "====== Sentence 24109 tokens =======\n",
      "====== Sentence 24278 tokens =======\n",
      "====== Sentence 24640 tokens =======\n",
      "====== Sentence 24641 tokens =======\n",
      "====== Sentence 25141 tokens =======\n",
      "====== Sentence 25201 tokens =======\n",
      "====== Sentence 25253 tokens =======\n",
      "====== Sentence 25267 tokens =======\n",
      "====== Sentence 25327 tokens =======\n",
      "====== Sentence 25417 tokens =======\n",
      "====== Sentence 25883 tokens =======\n",
      "====== Sentence 25889 tokens =======\n",
      "====== Sentence 26004 tokens =======\n",
      "====== Sentence 26113 tokens =======\n",
      "====== Sentence 26147 tokens =======\n",
      "====== Sentence 26148 tokens =======\n",
      "====== Sentence 26345 tokens =======\n",
      "====== Sentence 26358 tokens =======\n",
      "====== Sentence 26495 tokens =======\n",
      "====== Sentence 26498 tokens =======\n",
      "====== Sentence 27080 tokens =======\n",
      "====== Sentence 27123 tokens =======\n",
      "====== Sentence 27204 tokens =======\n",
      "====== Sentence 27318 tokens =======\n",
      "====== Sentence 27322 tokens =======\n",
      "====== Sentence 27601 tokens =======\n",
      "====== Sentence 27610 tokens =======\n",
      "====== Sentence 27882 tokens =======\n",
      "====== Sentence 27901 tokens =======\n",
      "====== Sentence 27946 tokens =======\n",
      "====== Sentence 28114 tokens =======\n",
      "====== Sentence 28261 tokens =======\n",
      "====== Sentence 28315 tokens =======\n",
      "====== Sentence 28401 tokens =======\n",
      "====== Sentence 28676 tokens =======\n",
      "====== Sentence 28703 tokens =======\n",
      "====== Sentence 28800 tokens =======\n",
      "====== Sentence 29005 tokens =======\n",
      "====== Sentence 29220 tokens =======\n",
      "====== Sentence 29391 tokens =======\n",
      "====== Sentence 29727 tokens =======\n",
      "====== Sentence 29736 tokens =======\n",
      "====== Sentence 29785 tokens =======\n",
      "====== Sentence 29827 tokens =======\n",
      "====== Sentence 29929 tokens =======\n",
      "====== Sentence 29973 tokens =======\n",
      "====== Sentence 30404 tokens =======\n",
      "====== Sentence 30432 tokens =======\n",
      "====== Sentence 30701 tokens =======\n",
      "====== Sentence 30762 tokens =======\n",
      "len pos_tags 31063\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use STANZA POS tagger to tag high-resource languages.\n",
    "To run change:\n",
    "- code\n",
    "- out\n",
    "- bible\n",
    "\"\"\"\n",
    "\n",
    "import stanza\n",
    "import logging\n",
    "import random\n",
    "\n",
    "#code = 'id' #indonesian\n",
    "#code = 'tr'\n",
    "# code = 'it'\n",
    "# code = 'nl'\n",
    "# code = 'es'\n",
    "# code = 'da'\n",
    "# code = 'cs'\n",
    "# code = 'de'\n",
    "# code = 'fr'\n",
    "# code = 'sv'\n",
    "# code = 'pl'\n",
    "# code = 'en'\n",
    "#code = 'pt'\n",
    "# code = 'ar' \n",
    "code = 'hi'\n",
    "#code = 'fa'\n",
    "# code = 'ga' # irish\n",
    "# code = 'ru' \n",
    "# code = 'zh' \n",
    "# code = 'hu' \n",
    "# code = 'ur' \n",
    "# code = 'el' \n",
    "# code = 'he' \n",
    "# code = 'cs' # check\n",
    "\n",
    "stanza.download(code)\n",
    "def tag_sentence(sentences):\n",
    "    logging.getLogger().setLevel(logging.CRITICAL)\n",
    "    # nlp = stanza.Pipeline(lang='fa', processors='tokenize,mwt,pos', tokenize_pretokenized=True)\n",
    "    # nlp = stanza.Pipeline(lang=code, processors='tokenize,mwt,pos', tokenize_pretokenized=True, pos_batch_size=300)\n",
    "    nlp = stanza.Pipeline(lang=code, processors='tokenize,pos', tokenize_pretokenized=True, pos_batch_size=300)\n",
    "    # print(sentences)\n",
    "    doc = nlp(sentences)\n",
    "    list_tags = []\n",
    "    for i, sent in enumerate(doc.sentences):\n",
    "        if random.randint(0,100) == 0:\n",
    "            print(f'====== Sentence {i+1} tokens =======')\n",
    "        app = []\n",
    "        for word in sent.words:\n",
    "            # print(word.text, word.upos)\n",
    "            app.append([word.text,word.upos])\n",
    "        list_tags.append(app)\n",
    "    return list_tags\n",
    "    # print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "#bible_file_name = 'por-x-bible-versaointernacional.txt'\n",
    "bible_file_name = 'hin-x-bible-bsi.txt'\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/tam-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/fin-x-bible-helfi.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/prs-x-bible-goodnews.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/ita-x-bible-2009.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/nld-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/nld-x-bible-2007.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/spa-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/spa-x-bible-hablahoi-latina.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/dan-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/ces-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/deu-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/fra-x-bible-louissegond.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/swe-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/pol-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/eng-x-bible-mixed.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/por-x-bible-versaointernacional.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/arb-x-bible.conllu\", \"w+\")\n",
    "#out = open(my_baseline_dir + ramy_file_tr + 'conllu', \"w\")\n",
    "out = open(my_baseline_dir + bible_file_name + 'conllu', \"w\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/gle-x-bible.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/deu-x-bible-bolsinger.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/rus-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/zho-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/hun-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/urd-x-bible-2007.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/ell-x-bible-newworld.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/heb-x-bible-helfi.conllu\", \"w+\")\n",
    "# out = open(\"/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/ces-x-bible-newworld.conllu\", \"w+\")\n",
    "\n",
    "\n",
    "# bible_file = \"/nfs/datc/pbc/prs-x-bible-goodnews.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/tam-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/ita-x-bible-2009.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/nld-x-bible-2007.txt\" #############\n",
    "# bible_file = \"/nfs/datc/pbc/nld-x-bible-newworld.txt\" #############\n",
    "# bible_file = \"/nfs/datc/pbc/spa-x-bible-hablahoi-latina.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/spa-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/dan-x-bible-newworld.txt\" #############\n",
    "# bible_file = \"/nfs/datc/pbc/ces-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/deu-x-bible-newworld.txt\" # double check\n",
    "# bible_file = \"/nfs/datc/pbc/fra-x-bible-louissegond.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/swe-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/pol-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/por-x-bible-versaointernacional.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/arb-x-bible.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/hin-x-bible-newworld.txt\"\n",
    "#bible_file = \"/nfs/datc/pbc/hin-x-bible-bsi.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/gle-x-bible.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/deu-x-bible-bolsinger.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/rus-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/zho-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/hun-x-bible-newworld.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/urd-x-bible-2007.txt\"\n",
    "# bible_file = \"/nfs/datc/pbc/ell-x-bible-newworld.txt\"\n",
    "bible_file = \"/nfs/datc/pbc/\" + bible_file_name\n",
    "\n",
    "#bible_file = my_baseline_dir + ramy_file_tr\n",
    "\n",
    "# bible_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/eng-x-bible-mixed.txt\"\n",
    "# bible_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/heb-x-bible-helfi.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# bible_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/fin-x-bible-helfi.txt\"\n",
    "sentences = \"\"\n",
    "count = 0\n",
    "MAX_LEN = 0\n",
    "with open(bible_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        if len(l)<2:\n",
    "            continue\n",
    "        if len(l[1])>MAX_LEN:\n",
    "            MAX_LEN = len(l[1])\n",
    "        sentences+=l[1]+\"\\n\"\n",
    "        count+=1\n",
    "        # if count==3:\n",
    "        #     break\n",
    "\n",
    "print('max len is', MAX_LEN)\n",
    "upos_tags = tag_sentence(sentences)\n",
    "print('len pos_tags', len(upos_tags))\n",
    "\n",
    "sent_num = 0\n",
    "with open(bible_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        l = line.strip().rstrip().split(\"\\t\")\n",
    "        if len(l)<2:\n",
    "            continue\n",
    "        out.write(F\"# sent_id = {l[0]}\\n\")\n",
    "        out.write(F\"# text = {l[1]}\\n\")\n",
    "        # upos_tags = tag_sentence(l[1])\n",
    "        l[1] = l[1].replace(\"  \", \" \")\n",
    "        for i,w in enumerate(l[1].split()):\n",
    "\n",
    "            if not w.lower()==upos_tags[sent_num][i][0].lower():\n",
    "                print(\"Error!\")\n",
    "                print(w)\n",
    "                print(l[1])\n",
    "                print(l[1].split(\" \"))\n",
    "                print(upos_tags[sent_num][i][0].lower())\n",
    "                break\n",
    "            out.write(F\"{i+1}\\t{w}\\t{w.lower()}\\t{upos_tags[sent_num][i][1]}\\t-\\t-\\t-\\t-\\t-\\t-\\n\")\n",
    "\n",
    "        sent_num+=1\n",
    "        out.write(\"\\n\")\n",
    "        # if sent_num==3:\n",
    "        #     break\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "postag_map = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "\n",
    "def read_conllu_file(f_path):\n",
    "\ttags = {}\n",
    "\twith codecs.open(f_path, \"r\", \"utf-8\") as lang_pos:\n",
    "\t\ttag_sent = []\n",
    "\t\tsent_id = \"\"\n",
    "\t\tfor sline in lang_pos:\n",
    "\t\t\tsline = sline.strip()\n",
    "\t\t\tif sline == \"\":\n",
    "\t\t\t\ttags[sent_id] = [p[3].upper() for p in tag_sent]\n",
    "\t\t\t\ttag_sent = []\n",
    "\t\t\t\tsent_id = \"\"\n",
    "\t\t\telif \"# verse_id\" in sline or '# sent_id' in sline:\n",
    "\t\t\t\tsent_id = sline.split()[-1]\n",
    "\t\t\telif sline[0] == \"#\":\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\ttag_sent.append(sline.split(\"\\t\"))\n",
    "\t\n",
    "\treturn tags\n",
    "\n",
    "#silver_ramy_pt = read_conllu_file(my_baseline_dir + ramy_file + 'conllu')\n",
    "#silver_ours_pt2 = read_conllu_file(my_baseline_dir + 'por-x-bible-newworld1996.txt' + 'conllu')\n",
    "##silver_ours_hi = read_conllu_file(my_baseline_dir + 'hin-x-bible-newworld.txt' + 'conllu')\n",
    "silver_ours_hi2 = read_conllu_file(my_baseline_dir + 'hin-x-bible-bsi.txt' + 'conllu')\n",
    "##silver_ours_fa = read_conllu_file(my_baseline_dir + 'prs-x-bible-goodnews.txt' + 'conllu')\n",
    "#silver_ours_fa2 = read_conllu_file(my_baseline_dir + 'pes-x-bible-newmillennium2011.txt' + 'conllu')\n",
    "#silver_ours_id = read_conllu_file(my_baseline_dir + 'ind-x-bible-newworld.txt' + 'conllu')\n",
    "#silver_ours_tr = read_conllu_file(my_baseline_dir + 'tur-x-bible-newworld.txt' + 'conllu')\n",
    "\n",
    "#silver_ramy_hi = read_conllu_file(my_baseline_dir + ramy_file_hi + 'conllu')\n",
    "#silver_ramy_fa = read_conllu_file(my_baseline_dir + ramy_file_fa + 'conllu')\n",
    "#silver_ramy_tr = read_conllu_file(my_baseline_dir + ramy_file_tr + 'conllu')\n",
    "#silver_ramy_id = read_conllu_file(my_baseline_dir + ramy_file_id + 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words for  POR-SPA-bible-POSUD-TRAIN.txt 27799\n",
      "unique words for  HIN-ENG-bible-POSUD-TRAIN.txt 11018\n",
      "unique words for  PES-SPA-bible-POSUD-TRAIN.txt 15274\n",
      "unique words for  IND-ENG-bible-POSUD-TRAIN.txt 6738\n",
      "unique words for  TUR-FRA-bible-POSUD-TRAIN.txt 15923\n"
     ]
    }
   ],
   "source": [
    "postag = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\",  \n",
    "              \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "def read_ramy_annotation(f):\n",
    "    f_path = '/mounts/work/silvia/POS/data/baseline_data/2021/'+f\n",
    "    res = {}\n",
    "    unique_words = set()\n",
    "\n",
    "    with codecs.open(f_path, 'r', 'utf-8') as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            res[str(i)] = {}\n",
    "            items = line.strip().split()\n",
    "            for j,item in enumerate(items):\n",
    "                tag = item.split('_')[1]\n",
    "                word = item.split('_')[0]\n",
    "                if tag in postag:\n",
    "                    res[str(i)][j] = tag.upper()\n",
    "                    unique_words.add(word)\n",
    "    \n",
    "    print('unique words for ', f, len(unique_words))\n",
    "    \n",
    "    return res\n",
    "\n",
    "bronze_ramy_pt = read_ramy_annotation(ramy_file)\n",
    "bronze_ramy_hi = read_ramy_annotation(ramy_file_hi)\n",
    "bronze_ramy_fa = read_ramy_annotation(ramy_file_fa)\n",
    "bronze_ramy_id = read_ramy_annotation(ramy_file_id)\n",
    "bronze_ramy_tr = read_ramy_annotation(ramy_file_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "\n",
    "postag = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\",  \n",
    "              \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
    "\n",
    "def calculate_accuracy(gold, predict):\n",
    "    total = 0\n",
    "    y_true, y_pred = [], []\n",
    "    for s in predict:\n",
    "        total += len(gold[s])\n",
    "        for i in predict[s]:\n",
    "            if i >= len(gold[s]):\n",
    "                #if random.randint(0,100)==0:\n",
    "                print(s,i)\n",
    "                continue\n",
    "            y_true.append(gold[s][i])\n",
    "            y_pred.append(predict[s][i])\n",
    "    print('accepted percentage', len(y_pred)/ total)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred, digits=4, labels=postag, zero_division=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accepted percentage 0.7445468367754701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.6485    0.7008    0.6736     12149\n",
      "         ADP     0.7377    0.8861    0.8051     59615\n",
      "         ADV     0.9252    0.7915    0.8531     20528\n",
      "         AUX     0.7521    0.8574    0.8013     12165\n",
      "       CCONJ     0.9907    0.9980    0.9944     29458\n",
      "         DET     0.8315    0.9106    0.8693     62954\n",
      "        INTJ     0.2404    0.5605    0.3365       669\n",
      "        NOUN     0.9548    0.9353    0.9450    110082\n",
      "         NUM     0.9580    0.9253    0.9414      2663\n",
      "        PART     0.0000    0.0000    0.0000         1\n",
      "        PRON     0.8707    0.7745    0.8198     41915\n",
      "       PROPN     0.9502    0.9260    0.9380     42716\n",
      "       PUNCT     0.9999    0.9975    0.9987    128992\n",
      "       SCONJ     0.6583    0.5931    0.6240     15988\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.9195    0.8054    0.8587     71213\n",
      "           X     0.0000    0.0000    0.0000        22\n",
      "\n",
      "   micro avg     0.8968    0.8968    0.8968    611130\n",
      "   macro avg     0.6728    0.6860    0.6740    611130\n",
      "weighted avg     0.9023    0.8968    0.8977    611130\n",
      "\n",
      "accepted percentage 0.5556941136890339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.5469    0.4551    0.4968     10665\n",
      "         ADP     0.9289    0.9256    0.9272     43835\n",
      "         ADV     0.1125    0.6900    0.1935      1545\n",
      "         AUX     0.6611    0.8729    0.7524     10254\n",
      "       CCONJ     0.8914    0.9944    0.9401     18502\n",
      "         DET     0.3253    0.4361    0.3727      5019\n",
      "        INTJ     0.0123    0.0395    0.0187       253\n",
      "        NOUN     0.8735    0.7341    0.7978     83649\n",
      "         NUM     0.9462    0.7481    0.8356      5947\n",
      "        PART     0.9943    0.7561    0.8590      5555\n",
      "        PRON     0.9197    0.8779    0.8983     58177\n",
      "       PROPN     0.7462    0.7770    0.7613     36698\n",
      "       PUNCT     1.0000    0.9998    0.9999     84989\n",
      "       SCONJ     0.8975    0.6088    0.7255      8200\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.6349    0.7803    0.7001     26418\n",
      "           X     0.0000    0.0000    0.0000        12\n",
      "\n",
      "   micro avg     0.8412    0.8412    0.8412    399718\n",
      "   macro avg     0.6171    0.6292    0.6046    399718\n",
      "weighted avg     0.8653    0.8412    0.8489    399718\n",
      "\n",
      "accepted percentage 0.534575533746792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.6678    0.3483    0.4579     10036\n",
      "         ADP     0.9235    0.9155    0.9195     24946\n",
      "         ADV     0.2629    0.5977    0.3652      2722\n",
      "         AUX     0.8740    0.9039    0.8887      4982\n",
      "       CCONJ     0.9665    0.9899    0.9781     15906\n",
      "         DET     0.2150    0.7397    0.3331      3092\n",
      "        INTJ     0.6121    0.7929    0.6909       840\n",
      "        NOUN     0.8822    0.6436    0.7442     79624\n",
      "         NUM     0.9025    0.8326    0.8662      1912\n",
      "        PART     0.0000    0.0000    0.0000        82\n",
      "        PRON     0.5639    0.6650    0.6103     16403\n",
      "       PROPN     0.5468    0.7844    0.6444     21470\n",
      "       PUNCT     1.0000    0.9998    0.9999     66141\n",
      "       SCONJ     0.7466    0.3836    0.5068      9660\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.5851    0.8617    0.6970     15683\n",
      "           X     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.7864    0.7864    0.7864    273499\n",
      "   macro avg     0.5735    0.6152    0.5707    273499\n",
      "weighted avg     0.8294    0.7864    0.7928    273499\n",
      "\n",
      "accepted percentage 0.5280576548363632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.5042    0.5238    0.5138      3131\n",
      "         ADP     0.8765    0.8745    0.8755     11917\n",
      "         ADV     0.3409    0.1599    0.2177      7206\n",
      "         AUX     0.0287    0.9835    0.0558       182\n",
      "       CCONJ     0.8237    0.9972    0.9022      8793\n",
      "         DET     0.8970    0.8185    0.8559      3118\n",
      "        INTJ     0.0000    0.0000    0.0000         0\n",
      "        NOUN     0.8417    0.8309    0.8363     24308\n",
      "         NUM     0.9310    0.8616    0.8950      1958\n",
      "        PART     0.7817    0.8106    0.7959      2001\n",
      "        PRON     0.8551    0.9883    0.9169     17175\n",
      "       PROPN     0.9683    0.8679    0.9154     26188\n",
      "       PUNCT     1.0000    0.9999    1.0000     50730\n",
      "       SCONJ     0.8875    0.3951    0.5468      5113\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.8963    0.8579    0.8767     18941\n",
      "           X     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.8681    0.8681    0.8681    180761\n",
      "   macro avg     0.6254    0.6453    0.6002    180761\n",
      "weighted avg     0.8888    0.8681    0.8727    180761\n",
      "\n",
      "accepted percentage 0.49688204622322435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.5510    0.3023    0.3904      6488\n",
      "         ADP     0.4231    0.7646    0.5448      2638\n",
      "         ADV     0.2910    0.4137    0.3417      1936\n",
      "         AUX     0.0141    0.0387    0.0207       181\n",
      "       CCONJ     0.9547    0.7474    0.8384      6492\n",
      "         DET     0.1451    0.5689    0.2312       733\n",
      "        INTJ     0.1667    0.3810    0.2319       105\n",
      "        NOUN     0.7841    0.5511    0.6473     35038\n",
      "         NUM     0.9102    0.5342    0.6733      4839\n",
      "        PART     0.0000    0.0000    0.0000         0\n",
      "        PRON     0.8237    0.8152    0.8194      4849\n",
      "       PROPN     0.3812    0.8721    0.5306      8938\n",
      "       PUNCT     1.0000    0.9984    0.9992     82034\n",
      "       SCONJ     0.0000    0.0000    0.0000         0\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.7564    0.7299    0.7429     14970\n",
      "           X     0.0000    0.0000    0.0000         1\n",
      "\n",
      "   micro avg     0.8069    0.8069    0.8069    169242\n",
      "   macro avg     0.4236    0.4540    0.4125    169242\n",
      "weighted avg     0.8521    0.8069    0.8155    169242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(silver_ramy_pt, bronze_ramy_pt)\n",
    "calculate_accuracy(silver_ramy_hi, bronze_ramy_hi)\n",
    "calculate_accuracy(silver_ramy_fa, bronze_ramy_fa)\n",
    "calculate_accuracy(silver_ramy_id, bronze_ramy_id)\n",
    "calculate_accuracy(silver_ramy_tr, bronze_ramy_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words for ind-x-bible-newworld.txt 14205\n",
      "accepted percentage 0.7242875140210954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.7151    0.6768    0.6954     10811\n",
      "         ADP     0.9540    0.9180    0.9357     60857\n",
      "         ADV     0.5623    0.2235    0.3199     25594\n",
      "         AUX     0.0834    1.0000    0.1540      1805\n",
      "       CCONJ     0.8434    0.9996    0.9149     36876\n",
      "         DET     0.9928    0.9160    0.9529      5836\n",
      "        INTJ     0.0000    0.0000    0.0000         0\n",
      "        NOUN     0.9143    0.9002    0.9072    127890\n",
      "         NUM     0.9525    0.9045    0.9279      6764\n",
      "        PART     0.9577    0.9700    0.9638      8696\n",
      "        PRON     0.8535    0.9959    0.9192     57375\n",
      "       PROPN     0.9720    0.8517    0.9079     48326\n",
      "       PUNCT     1.0000    1.0000    1.0000    130487\n",
      "       SCONJ     0.8822    0.4939    0.6333     13907\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.9574    0.9157    0.9361     73037\n",
      "           X     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.8962    0.8962    0.8962    608261\n",
      "   macro avg     0.6847    0.6921    0.6569    608261\n",
      "weighted avg     0.9166    0.8962    0.8996    608261\n",
      "\n",
      "unique words for tur-x-bible-newworld.txt 34073\n",
      "accepted percentage 0.6893264917206173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.6766    0.3408    0.4533     21025\n",
      "         ADP     0.4464    0.6603    0.5327      8249\n",
      "         ADV     0.3218    0.4916    0.3890      5923\n",
      "         AUX     0.0005    0.0010    0.0006       975\n",
      "       CCONJ     0.9440    0.6951    0.8007     25409\n",
      "         DET     0.1934    0.5459    0.2856      3640\n",
      "        INTJ     0.0000    0.0000    0.0000       539\n",
      "        NOUN     0.8687    0.7543    0.8074    119974\n",
      "         NUM     0.9741    0.4077    0.5748     13571\n",
      "        PART     0.0000    0.0000    0.0000         0\n",
      "        PRON     0.7598    0.9511    0.8448     28267\n",
      "       PROPN     0.6206    0.9544    0.7522     27714\n",
      "       PUNCT     0.9621    0.9988    0.9801    109652\n",
      "       SCONJ     0.0000    0.0000    0.0000         0\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.8529    0.7694    0.8090     53922\n",
      "           X     0.0000    0.0000    0.0000        12\n",
      "\n",
      "   micro avg     0.8010    0.8010    0.8010    418872\n",
      "   macro avg     0.4483    0.4453    0.4253    418872\n",
      "weighted avg     0.8406    0.8010    0.8072    418872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "from math import log\n",
    "\n",
    "postag_map = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "postag_reverse_map = {item[1]:item[0] for item in postag_map.items()}\n",
    "new_testament_verses = torch.load('/mounts/Users/student/ayyoob/Dokumente/code/POS-TAGGING/pos_evaluation/new_testament_verses.torch.bin')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_bible_file(f_name, sentences, type_counts):\n",
    "    with codecs.open('/nfs/datc/pbc/'+f_name, 'r', 'utf-8') as fi:\n",
    "        for l in fi:\n",
    "            l = l.strip()\n",
    "            lparts = l.split('\\t')\n",
    "            if l.startswith('#') or l == '' or len(lparts)<2:\n",
    "                continue\n",
    "            \n",
    "            toks = lparts[1].split()\n",
    "            sentences[lparts[0]] = toks\n",
    "            for tok in toks:\n",
    "                type_counts[tok] +=1\n",
    "\n",
    "\n",
    "def read_our_bronze_file(f_path, bible_file_name):\n",
    "    type_counts = collections.defaultdict(lambda:0)  # number of times a token happened\n",
    "    type_tag_counts = collections.defaultdict(lambda: collections.defaultdict(lambda:0))\n",
    "    type_tag_probabilities = collections.defaultdict(lambda: collections.defaultdict(lambda:0.0))\n",
    "    unique_words = set()\n",
    "    sentences = {}\n",
    "\n",
    "    read_bible_file(bible_file_name, sentences, type_counts)\n",
    "    data = torch.load(f_path)\n",
    "    max_count = 0\n",
    "    total_count = 0\n",
    "    res = {}\n",
    "\n",
    "    for sent in data:\n",
    "        for item in data[sent]:\n",
    "            type = sentences[sent][item[0]]\n",
    "            type_tag_counts[type][item[1]] += 1\n",
    "            type_tag_probabilities[type][item[1]] += 1\n",
    "            total_count += 1\n",
    "\n",
    "    for type in type_tag_probabilities:\n",
    "        max_count = max(max_count, type_counts[type])\n",
    "        for tag in type_tag_probabilities[type]:\n",
    "            type_tag_probabilities[type][tag] /= type_counts[type]\n",
    "\n",
    "    for sent in data:\n",
    "        res[sent] = {}\n",
    "        for item in data[sent]:\n",
    "            type = sentences[sent][item[0]]\n",
    "            p_type_tag = type_tag_probabilities[type][item[1]]\n",
    "            p_mul = item[2] * p_type_tag\n",
    "            p_gross = item[2] + p_type_tag\n",
    "            size_factor = 1+(log(type_counts[type]+1, 10)/log(total_count+1, 10))\n",
    "            if size_factor * p_mul > 0.8:# and sent in new_testament_verses: \n",
    "            #if type_counts[type] <= 50 and type_counts[type]>10 and ( p_gross>1.7  ): \n",
    "            #if type_counts[type] > 100 and ((p_type_tag >0.5 and p_mul > 0.50 ) or (p_type_tag <= 0.5 and p_type_tag >0.4 and p_mul > 0.4) ): \n",
    "            #if (type_tag_probabilities[type][item[1]]>0.5 and type_counts[type] < 10 and type_counts[type] >2 and item[2] > 0.9):\n",
    "            #if (type_tag_probabilities[type][item[1]]>0.4 and type_counts[type] >= 2  and item[2] > 0.8):\n",
    "            #if (type_tag_probabilities[type][item[1]]>0.9 and type_counts[type] == 1 and item[2] > 0.95):\n",
    "            #if sent not in new_testament_verses:\n",
    "                unique_words.add(type)\n",
    "                res[sent][item[0]] = postag_reverse_map[item[1]]\n",
    "    print('unique words for', bible_file_name, len(unique_words))\n",
    "    return res\n",
    "\n",
    "bronze_file_pt3 = '/mounts/work/ayyoob/results/gnn_align/yoruba/POSTgs_por-x-bible-newworld1996_15lngs-POSFeatTruealltgts_trnsfrmrTrue6LResFalse_trainWEFalse_mskLngTrue_E1_traintgt0.9_TypchckFalse_TgBsdSlctFalse1_20220401-150804_ElyStpDlta0-GA-chnls512_small.pickle'\n",
    "bronze_file_pt2 = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_por-x-bible-newworld1996_15langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_maskLangTrue_epoch1_traintarget0.95_typecheckFalse_20220321-144100_earlystoppingdelta0-GA-channels512_maskLangTrue.pickle'\n",
    "##bronze_file = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_por-x-bible-versaointernacional_11langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_traintarget0.95_typecheckFalse_20220306-232534_earlystopping-GA_maskLangTrue.pickle'\n",
    "#bronze_file_hi = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_hin-x-bible-newworld_11langs-nopersian_posfeatTrueealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_traintarget0.95_typecheckFalse_20220306-232534_earlystopping-GA__maskLangTrue.pickle'\n",
    "bronze_file_hi2 = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_hin-x-bible-bsi_15langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_maskLangTrue_epoch1_traintarget0.95_typecheckFalse_20220321-144100_earlystoppingdelta0-GA-channels512_maskLangTrue.pickle'\n",
    "bronze_file_hi3 = '/mounts/work/ayyoob/results/gnn_align/yoruba/POSTgs_hin-x-bible-bsi_15lngs-POSFeatTruealltgts_trnsfrmrTrue6LResFalse_trainWEFalse_mskLngTrue_E1_traintgt0.9_TypchckFalse_TgBsdSlctFalse1_20220401-150804_ElyStpDlta0-GA-chnls512_small.pickle'\n",
    "#bronze_file_fa = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_prs-x-bible-goodnews_11langs-nopersian_posfeatTrueealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_traintarget0.95_typecheckFalse_20220306-232534_earlystopping-GA__maskLangTrue.pickle'\n",
    "bronze_file_fa2 = '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_pes-x-bible-newmillennium2011_15langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_maskLangTrue_epoch1_traintarget0.95_typecheckFalse_20220321-144100_earlystoppingdelta0-GA-channels512_maskLangTrue.pickle'\n",
    "bronze_file_fa3 = '/mounts/work/ayyoob/results/gnn_align/yoruba/POSTgs_pes-x-bible-newmillennium2011_15lngs-POSFeatTruealltgts_trnsfrmrTrue6LResFalse_trainWEFalse_mskLngTrue_E1_traintgt0.9_TypchckFalse_TgBsdSlctFalse1_20220401-150804_ElyStpDlta0-GA-chnls512_small.pickle'\n",
    "bronze_file_id = '/mounts/work/ayyoob/results/gnn_align/yoruba/POSTgs_ind-x-bible-newworld_15lngs-POSFeatTruealltgts_trnsfrmrTrue6LResFalse_trainWEFalse_mskLngTrue_E1_traintgt0.9_TypchckFalse_TgBsdSlctFalse1_20220401-171947_ElyStpDlta0-GA-chnls512_small.pickle'\n",
    "bronze_file_tr = '/mounts/work/ayyoob/results/gnn_align/yoruba/POSTgs_tur-x-bible-newworld_15lngs-POSFeatTruealltgts_trnsfrmrTrue6LResFalse_trainWEFalse_mskLngTrue_E1_traintgt0.9_TypchckFalse_TgBsdSlctFalse1_20220401-171947_ElyStpDlta0-GA-chnls512_small.pickle'\n",
    "\n",
    "bronze_ours_pt2 = read_our_bronze_file(bronze_file_pt2, 'por-x-bible-newworld1996.txt')\n",
    "calculate_accuracy(silver_ours_pt2, bronze_ours_pt2)\n",
    "#bronze_ours_pt3 = read_our_bronze_file(bronze_file_pt3, 'por-x-bible-newworld1996.txt')\n",
    "#calculate_accuracy(silver_ours_pt2, bronze_ours_pt3)\n",
    "\n",
    "##bronze_ours_hi = read_our_bronze_file(bronze_file_hi, 'hin-x-bible-newworld.txt')\n",
    "bronze_ours_hi2 = read_our_bronze_file(bronze_file_hi2, 'hin-x-bible-bsi.txt')\n",
    "calculate_accuracy(silver_ours_hi2, bronze_ours_hi2)\n",
    "bronze_ours_hi3 = read_our_bronze_file(bronze_file_hi3, 'hin-x-bible-bsi.txt')\n",
    "calculate_accuracy(silver_ours_hi2, bronze_ours_hi3)\n",
    "##bronze_ours_fa = read_our_bronze_file(bronze_file_fa, 'prs-x-bible-goodnews.txt')\n",
    "bronze_ours_fa2 = read_our_bronze_file(bronze_file_fa2, 'pes-x-bible-newmillennium2011.txt')\n",
    "calculate_accuracy(silver_ours_fa2, bronze_ours_fa2)\n",
    "#bronze_ours_fa3 = read_our_bronze_file(bronze_file_fa3, 'pes-x-bible-newmillennium2011.txt')\n",
    "#calculate_accuracy(silver_ours_fa2, bronze_ours_fa3)\n",
    "bronze_ours_id = read_our_bronze_file(bronze_file_id, 'ind-x-bible-newworld.txt')\n",
    "calculate_accuracy(silver_ours_id, bronze_ours_id)\n",
    "bronze_ours_tr = read_our_bronze_file(bronze_file_tr, 'tur-x-bible-newworld.txt')\n",
    "calculate_accuracy(silver_ours_tr, bronze_ours_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24015002 65\n",
      "accepted percentage 0.6892231196706189\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.7204    0.4296    0.5382     14246\n",
      "         ADP     0.9571    0.8999    0.9276     70651\n",
      "         ADV     0.1714    0.7453    0.2787      2638\n",
      "         AUX     0.8144    0.5807    0.6780     37756\n",
      "       CCONJ     0.9080    0.9928    0.9485     33475\n",
      "         DET     0.0791    0.1151    0.0938      7065\n",
      "        INTJ     0.0000    0.0000    0.0000       256\n",
      "        NOUN     0.8933    0.8418    0.8668    122537\n",
      "         NUM     0.9548    0.5640    0.7091      7895\n",
      "        PART     0.9919    0.8512    0.9162     11965\n",
      "        PRON     0.9058    0.8498    0.8769     86024\n",
      "       PROPN     0.8264    0.7874    0.8064     39367\n",
      "       PUNCT     0.9995    0.9991    0.9993     96823\n",
      "       SCONJ     0.7268    0.7251    0.7260     10116\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.6551    0.8847    0.7528     71826\n",
      "           X     0.0000    0.0000    0.0000        38\n",
      "\n",
      "   micro avg     0.8441    0.8441    0.8441    612678\n",
      "   macro avg     0.6238    0.6039    0.5952    612678\n",
      "weighted avg     0.8659    0.8441    0.8488    612678\n",
      "\n",
      "24051027 49\n",
      "42015025 17\n",
      "38008023 32\n",
      "38008023 42\n",
      "61002008 11\n",
      "accepted percentage 0.6664434190484724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.7190    0.4782    0.5744     13845\n",
      "         ADP     0.9568    0.9045    0.9299     70312\n",
      "         ADV     0.0863    0.2889    0.1329      2108\n",
      "         AUX     0.7663    0.7745    0.7704     30971\n",
      "       CCONJ     0.8453    0.9977    0.9152     33917\n",
      "         DET     0.0654    0.0504    0.0569      6709\n",
      "        INTJ     0.0000    0.0000    0.0000       871\n",
      "        NOUN     0.9032    0.8128    0.8556    119366\n",
      "         NUM     0.9514    0.6360    0.7623      7137\n",
      "        PART     0.9150    0.7765    0.8401     11087\n",
      "        PRON     0.9099    0.8906    0.9001     88060\n",
      "       PROPN     0.7846    0.8005    0.7925     41150\n",
      "       PUNCT     0.9870    0.9991    0.9930     97647\n",
      "       SCONJ     0.6673    0.6783    0.6728      8966\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.6673    0.8289    0.7394     62863\n",
      "           X     0.0000    0.0000    0.0000        45\n",
      "\n",
      "   micro avg     0.8508    0.8508    0.8508    595054\n",
      "   macro avg     0.6015    0.5833    0.5844    595054\n",
      "weighted avg     0.8600    0.8508    0.8523    595054\n",
      "\n",
      "accepted percentage 0.6499049015545143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.7007    0.3618    0.4772      4820\n",
      "         ADP     0.9432    0.9843    0.9633     14439\n",
      "         ADV     0.4169    0.5942    0.4900      1688\n",
      "         AUX     0.9613    0.8392    0.8961      3047\n",
      "       CCONJ     0.9435    1.0000    0.9709      7732\n",
      "         DET     0.5841    0.7885    0.6711      1026\n",
      "        INTJ     0.0000    0.0000    0.0000        90\n",
      "        NOUN     0.9162    0.6936    0.7895     30782\n",
      "         NUM     0.9295    0.9056    0.9174       699\n",
      "        PART     0.2180    1.0000    0.3580       133\n",
      "        PRON     0.7736    0.9872    0.8674      9841\n",
      "       PROPN     0.6027    0.7885    0.6832      4052\n",
      "       PUNCT     0.9984    0.9991    0.9988     26553\n",
      "       SCONJ     0.8768    0.6281    0.7319      3049\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.7298    0.9602    0.8293     14378\n",
      "           X     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.8611    0.8611    0.8611    122329\n",
      "   macro avg     0.6232    0.6782    0.6261    122329\n",
      "weighted avg     0.8758    0.8611    0.8578    122329\n",
      "\n",
      "accepted percentage 0.6503473508948657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.7030    0.4249    0.5297      5196\n",
      "         ADP     0.9393    0.9837    0.9610     13476\n",
      "         ADV     0.5150    0.5872    0.5487      2258\n",
      "         AUX     0.8898    0.8565    0.8728      3149\n",
      "       CCONJ     0.9056    0.9981    0.9496      8031\n",
      "         DET     0.5622    0.4883    0.5226      1065\n",
      "        INTJ     0.0000    0.0000    0.0000       123\n",
      "        NOUN     0.9249    0.6177    0.7407     31031\n",
      "         NUM     0.8171    1.0000    0.8993       661\n",
      "        PART     0.1635    0.9897    0.2807        97\n",
      "        PRON     0.7057    0.9858    0.8226     10250\n",
      "       PROPN     0.5292    0.8023    0.6377      4228\n",
      "       PUNCT     0.9983    0.9996    0.9989     27566\n",
      "       SCONJ     0.7672    0.4554    0.5715      3496\n",
      "         SYM     0.0000    0.0000    0.0000         0\n",
      "        VERB     0.6675    0.9286    0.7767     13972\n",
      "           X     0.0000    0.0000    0.0000         3\n",
      "\n",
      "   micro avg     0.8312    0.8312    0.8312    124602\n",
      "   macro avg     0.5934    0.6540    0.5949    124602\n",
      "weighted avg     0.8539    0.8312    0.8266    124602\n",
      "\n",
      "accepted percentage 0.7518305620842726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.6962    0.5439    0.6107     16272\n",
      "         ADP     0.7398    0.9422    0.8288     69146\n",
      "         ADV     0.8397    0.5028    0.6290     26501\n",
      "         AUX     0.7724    0.9325    0.8449     12800\n",
      "       CCONJ     0.9977    0.9965    0.9971     49260\n",
      "         DET     0.9020    0.9252    0.9135     61825\n",
      "        INTJ     0.0000    0.0000    0.0000       537\n",
      "        NOUN     0.9373    0.9204    0.9288    129822\n",
      "         NUM     0.9809    0.9186    0.9487      5465\n",
      "        PART     0.0000    0.0000    0.0000         1\n",
      "        PRON     0.8114    0.7976    0.8045     47373\n",
      "       PROPN     0.9710    0.8944    0.9311     43711\n",
      "       PUNCT     1.0000    0.9968    0.9984    144112\n",
      "       SCONJ     0.6829    0.3441    0.4576      9056\n",
      "         SYM     0.0000    0.0000    0.0000         1\n",
      "        VERB     0.9031    0.8574    0.8796     88144\n",
      "           X     0.0000    0.0000    0.0000        34\n",
      "\n",
      "    accuracy                         0.8938    704060\n",
      "   macro avg     0.6609    0.6219    0.6337    704060\n",
      "weighted avg     0.9053    0.8938    0.8954    704060\n",
      "\n",
      "accepted percentage 0.7483854369871271\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.7163    0.6099    0.6588     16462\n",
      "         ADP     0.7280    0.9385    0.8199     68519\n",
      "         ADV     0.8843    0.4689    0.6128     24653\n",
      "         AUX     0.7779    0.9301    0.8472     14428\n",
      "       CCONJ     0.9708    0.9971    0.9838     50727\n",
      "         DET     0.9041    0.7406    0.8142     62572\n",
      "        INTJ     0.0000    0.0000    0.0000       431\n",
      "        NOUN     0.9496    0.9254    0.9373    127170\n",
      "         NUM     0.9784    0.9441    0.9609      5418\n",
      "        PART     0.0000    0.0000    0.0000         0\n",
      "        PRON     0.6518    0.8060    0.7207     49022\n",
      "       PROPN     0.9682    0.9083    0.9373     44936\n",
      "       PUNCT     0.9993    0.9969    0.9981    144731\n",
      "       SCONJ     0.4529    0.1777    0.2552      8634\n",
      "         SYM     0.0000    0.0000    0.0000         1\n",
      "        VERB     0.9040    0.8505    0.8764     86233\n",
      "           X     0.0000    0.0000    0.0000        37\n",
      "\n",
      "    accuracy                         0.8786    703974\n",
      "   macro avg     0.6403    0.6055    0.6131    703974\n",
      "weighted avg     0.8928    0.8786    0.8797    703974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "calculate_accuracy(silver_ours_hi2, bronze_ours_hi2)\n",
    "calculate_accuracy(silver_ours_hi2, bronze_ours_hi3)\n",
    "calculate_accuracy(silver_ours_fa2, bronze_ours_fa2)\n",
    "calculate_accuracy(silver_ours_fa2, bronze_ours_fa3)\n",
    "calculate_accuracy(silver_ours_pt2, bronze_ours_pt2)\n",
    "calculate_accuracy(silver_ours_pt2, bronze_ours_pt3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
