{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/ayyoob/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "sys.path.insert(0, '../')\n",
    "from my_utils import gpu_utils\n",
    "import importlib, gc\n",
    "from my_utils.alignment_features import *\n",
    "import my_utils.alignment_features as afeatures\n",
    "importlib.reload(afeatures)\n",
    "import gnn_utils.graph_utils as gutils\n",
    "import postag_utils as posutil\n",
    "from my_utils.pytorch_utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dev2 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24159\n"
     ]
    }
   ],
   "source": [
    "from my_utils import align_utils as autils, utils\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "utils.setup(config_file)\n",
    "\n",
    "params = argparse.Namespace()\n",
    "\n",
    "\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses = list(pros.keys())\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses.extend(list(pros.keys()))\n",
    "all_verses = list(set(all_verses))\n",
    "print(len(all_verses))\n",
    "\n",
    "params.editions_file =  \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi_lang_list.txt\"\n",
    "editions, langs = autils.load_simalign_editions(params.editions_file)\n",
    "current_editions = [editions[lang] for lang in langs]\n",
    "\n",
    "def get_pruned_verse_alignments(args):\n",
    "    verse, current_editions = args\n",
    "    \n",
    "    verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "    gc.collect()\n",
    "    return verse_aligns_inter, verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "args = []\n",
    "for i,verse in enumerate(all_verses):\n",
    "    args.append((verse, current_editions[:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(afeatures)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features, n_head = 2, has_tagfreq_feature=False,):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels, heads= n_head)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= 1)\n",
    "        #self.fin_lin = nn.Linear(out_channels, out_channels)\n",
    "        \n",
    "        if has_tagfreq_feature:\n",
    "            self.feature_encoder = afeatures.FeatureEncoding(features, [normalized_tag_frequencies, word_vectors])\n",
    "            #self.feature_encoder = afeatures.FeatureEncoding(features, [normalized_tag_frequencies,train_pos_labels, word_vectors])\n",
    "        else:\n",
    "            self.feature_encoder = afeatures.FeatureEncoding(features, [word_vectors])\n",
    "            #self.feature_encoder = afeatures.FeatureEncoding(features, [train_pos_labels, word_vectors])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        encoded = self.feature_encoder(x, dev)\n",
    "        x = F.elu(self.conv1(encoded, edge_index, ))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        #return F.relu(self.fin_lin(x)), encoded\n",
    "        return x, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_encoders_embedding(encoder):\n",
    "    for i,ft in enumerate(encoder.feature_types):\n",
    "        if ft.type == MAPPING:\n",
    "            print('doing it')\n",
    "            encoder.layers[i] = afeatures.MappingEncoding(encoder.layers[i].emb.weight, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class DataEncoder():\n",
    "\n",
    "    def __init__(self, data_loader, model, mask_language):\n",
    "        self.data_loader = data_loader\n",
    "        self.model = model\n",
    "        self.mask_language = mask_language\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i,batch in enumerate(tqdm(self.data_loader)):\n",
    "\n",
    "            x = batch['x'][0].to(dev)  # initial features (not encoded)\n",
    "            edge_index = batch['edge_index'][0].to(dev) \n",
    "            verse = batch['verse'][0]\n",
    "\n",
    "            if verse in masked_verses:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if self.mask_language:\n",
    "                    x[:, 0] = 0\n",
    "                z, encoded = self.model.encode(x, edge_index) # Z will be the output of the GNN\n",
    "                batch['encoded'] = encoded\n",
    "            except Exception as e:\n",
    "                global sag, khar, gav\n",
    "                sag, khar, gav =  (i, batch, verse)\n",
    "                print(e)\n",
    "                1/0\n",
    "            \n",
    "            yield z, verse, i, batch\n",
    "\n",
    "def train(epoch, data_loader, mask_language, test_data_loader, max_batches=999999999):\n",
    "    global optimizer\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    loss_multi_round = 0\n",
    "\n",
    "    data_encoder = DataEncoder(data_loader, model, mask_language)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for z, verse, i, batch in data_encoder:\n",
    "        \n",
    "        target = batch['pos_classes'][0].to(dev)\n",
    "        _, labels = torch.max(target, 1)\n",
    "        \n",
    "        index = batch['pos_index'][0].to(dev)\n",
    "\n",
    "        preds = model.decoder(z, index, batch)\n",
    "\n",
    "        # print(preds.shape, labels.shape)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss = loss * target.shape[0] # TODO check if this is necessary\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 5 == 0: # Gradient accumulation\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            # print(f\"loss: {total_loss}\")\n",
    "            total_loss = 0\n",
    "            val_loss = test(epoch, test_data_loader, mask_language)\n",
    "            test_mostfreq(yor_data_loader_heb, True, yor_gold_mostfreq_tag, yor_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "            test_mostfreq(tam_data_loader_grc, True, tam_gold_mostfreq_tag, tam_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "            test_mostfreq(arb_data_loader_grc, True, arb_gold_mostfreq_tag, arb_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "            test_mostfreq(por_data_loader_grc, True, por_gold_mostfreq_tag, por_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "            print('----------------------------------------------------------------------------------------------------------')\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "            clean_memory()\n",
    "            \n",
    "\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    \n",
    "    print(f\"total train loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class, drop_out=0):\n",
    "        super(POSDecoder, self).__init__()\n",
    "\n",
    "        self.transfer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                        nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                        nn.Linear(hidden_size, n_class))\n",
    "\n",
    "    def forward(self, z, index, batch=None):\n",
    "        h = z[index, :]\n",
    "\n",
    "        res = self.transfer(h)\n",
    "\n",
    "        return res\n",
    "\n",
    "class POSDecoderTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class, residual_connection, drop_out=0):\n",
    "        super(POSDecoderTransformer, self).__init__()\n",
    "\n",
    "        self.residual_connection = residual_connection\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=8, dim_feedforward=hidden_size)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "\n",
    "        self.transfer = nn.Sequential( nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out), # TODO check what happens if I remove this.\n",
    "                        nn.Linear(hidden_size, n_class))\n",
    "\n",
    "    def forward(self, z_, index, batch_):\n",
    "        z = z_.to(dev2)\n",
    "\n",
    "        x = F.pad(batch_['encoded'], (0, z.size(1) - batch_['encoded'].size(1))).to(dev2)\n",
    "\n",
    "\n",
    "        language_based_nodes = batch_['lang_based_nodes'] # determines which node belongs to which language\n",
    "        transformer_indices = batch_['transformer_indices'] # the reverse of the prev structure\n",
    "\n",
    "        sentences = []\n",
    "        for lang_nodes in language_based_nodes: # we rearrange the nodes into sentences of each language\n",
    "            if self.residual_connection:\n",
    "                tensor = z[lang_nodes, :] + x[lang_nodes, :]\n",
    "            else:\n",
    "                tensor = z[lang_nodes, :]\n",
    "            tensor = F.pad(tensor, (0, 0, 0, 150 - tensor.size(0)))\n",
    "            sentences.append(tensor)\n",
    "        \n",
    "        batch = torch.stack(sentences) # A batch contains all translations of one sentence in all training languages.\n",
    "        batch = torch.transpose(batch, 0, 1)\n",
    "\n",
    "        h = self.transformer(batch)\n",
    "        h = torch.transpose(h, 0, 1)\n",
    "        h = h[transformer_indices[0], transformer_indices[1], :] # rearrange the nodes back to the order in which we recieved (the order that represents the graph)\n",
    "\n",
    "        res = self.transfer(h)\n",
    "\n",
    "        return res.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, testloader, mask_language, filter_wordtypes=None):\n",
    "    print('testing',  epoch)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    probability_sum = 0 \n",
    "    probability_count = 0\n",
    "\n",
    "    data_encoder = DataEncoder(testloader, model, mask_language)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for z, verse, i, batch in data_encoder:\n",
    "            \n",
    "            target = batch['pos_classes'][0].to(dev)\n",
    "            index = batch['pos_index'][0].to(dev)\n",
    "            \n",
    "            if filter_wordtypes != None:\n",
    "                non_filtered_words = filter_wordtypes[batch['x'][0][:, 9].long()] == 1\n",
    "                non_filtered_words = non_filtered_words[index]\n",
    "                index = index[non_filtered_words]\n",
    "\n",
    "                target = target[non_filtered_words, :]\n",
    "\n",
    "            preds = model.decoder(z, index, batch)\n",
    "            \n",
    "\n",
    "            if preds.size(0) > 0:\n",
    "                max_probs, predicted = torch.max(torch.softmax(preds, dim=1), 1)\n",
    "                _, labels = torch.max(target, 1)\n",
    "\n",
    "                loss = criterion(preds, labels)\n",
    "                probability_sum += torch.sum(max_probs)\n",
    "                probability_count += max_probs.size(0)\n",
    "                total_loss += loss\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'test, epoch: {epoch}, total:{total} ACC: {correct/total}, loss: {total_loss}, confidence: {probability_sum/probability_count}')\n",
    "    clean_memory()\n",
    "    return total_loss\n",
    "\n",
    "def test_mostfreq(testloader, mask_language, target_mostfreq_tags, target_mostfreq_index, word_types_shape, from_target=False):\n",
    "    \n",
    "    res = torch.zeros(word_types_shape[0], word_types_shape[1])\n",
    "    model.eval()\n",
    "\n",
    "    data_encoder = DataEncoder(testloader, model, mask_language)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for z, verse, i, batch in data_encoder:\n",
    "            \n",
    "            index = batch['pos_index'][0].to(dev)\n",
    "            x = batch['x'][0][index, :]\n",
    "            target = batch['pos_classes'][0]\n",
    "\n",
    "            preds = model.decoder(z, index, batch)\n",
    "\n",
    "            _, pred_max = torch.max(preds, dim=1)\n",
    "            _, targe_max = torch.max(target, dim=1)\n",
    "\n",
    "            if from_target:\n",
    "                res[x[:, 9].long(), targe_max.long()] += 1\n",
    "            else:\n",
    "                res[x[:, 9].long(), pred_max.long()] += 1\n",
    "    \n",
    "    max_vals, res_tags = torch.max(res, dim=1)\n",
    "    res_tags = res_tags[target_mostfreq_index]\n",
    "    max_vals = max_vals[target_mostfreq_index]\n",
    "    \n",
    "\n",
    "    \n",
    "    #print(f'correct = {torch.sum(target_mostfreq_tags == res_tags)}')\n",
    "    #print(f'most frequency test, total:{target_mostfreq_tags.shape[0]}, accuracy:{torch.sum(target_mostfreq_tags == res_tags)/target_mostfreq_tags.shape[0]}, ')\n",
    "    #print('target mostfreq tags', target_mostfreq_tags.shape)\n",
    "    #print('res_tags', res_tags.shape)\n",
    "\n",
    "    res_tags = res_tags[max_vals>0.1]\n",
    "    target_mostfreq_tags_cp = target_mostfreq_tags[max_vals>0.1]\n",
    "    #print(f'correct = {torch.sum(target_mostfreq_tags_cp == res_tags)}')\n",
    "    print(f'most frequency test, total:{target_mostfreq_tags_cp.shape[0]}, accuracy:{torch.sum(target_mostfreq_tags_cp == res_tags)/target_mostfreq_tags_cp.shape[0]}, ')\n",
    "    #print('target mostfreq tags', target_mostfreq_tags_cp.shape)\n",
    "    #print('res_tags', res_tags.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting_test(data_loader1, data_loader2):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i,(batch, batch2) in enumerate(tqdm(zip(data_loader1, data_loader2))) :\n",
    "            \n",
    "        x = batch['x'][0]\n",
    "        edge_index = batch['edge_index'][0]\n",
    "        verse = batch['verse'][0]\n",
    "\n",
    "        if verse in masked_verses:\n",
    "            continue\n",
    "\n",
    "        target = batch['pos_classes'][0]\n",
    "        index = batch['pos_index'][0]\n",
    "\n",
    "        index2 = batch2['pos_index'][0]\n",
    "        \n",
    "\n",
    "        for node, label in zip(index,target):\n",
    "            other_side = edge_index[1, edge_index[0, :] == node]\n",
    "            other_side_withpos = other_side[[True if i in index2 else False for i in other_side]]\n",
    "            other_side_target_indices = [(i == index2).nonzero(as_tuple=True)[0].item() for i in other_side_withpos]\n",
    "            #print(other_side_target_indices)\n",
    "            proj_tags = batch2['pos_classes'][0][other_side_target_indices]\n",
    "\n",
    "            if proj_tags.size(0) > 0:\n",
    "                _, proj_tags = torch.max(proj_tags, 1)\n",
    "                #print(target.shape, node, index.shape, proj_tags, other_side)\n",
    "                \n",
    "                if torch.argmax(label) == torch.mode(proj_tags)[0]:\n",
    "                    correct += 1\n",
    "                \n",
    "                total += 1\n",
    "\n",
    "    print(f'test, , total:{total} ACC: {correct/total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wordtype_frequencies[736])\n",
    "#print(wordtype_frequencies[1473])\n",
    "#print(wordtype_frequencies[3683])\n",
    "#print(wordtype_frequencies[7367])\n",
    "#print(wordtype_frequencies[14733])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequent_words = torch.zeros(word_frequencies.size(0))\n",
    "#frequent_words[word_frequencies > -1] = 1\n",
    "\n",
    "#test(1, test_data_loader, filter_wordtypes=frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2354770, 100)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"/mounts/work/ayyoob/models/w2v/word2vec_helfi_langs_15e.model\")\n",
    "\n",
    "print(w2v_model.wv.vectors.shape)\n",
    "word_vectors = torch.from_numpy(w2v_model.wv.vectors).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "test_verses = all_verses[:] \n",
    "editf1 = 'fin-x-bible-helfi'\n",
    "editf2 = \"heb-x-bible-helfi\"\n",
    "\n",
    "\n",
    "if 'jpn-x-bible-newworld' in  current_editions[:]:\n",
    "     current_editions.remove('jpn-x-bible-newworld')\n",
    "if 'grc-x-bible-unaccented' in  current_editions[:]:\n",
    "     current_editions.remove('grc-x-bible-unaccented')\n",
    "\n",
    "\n",
    "\n",
    "data_dir_train = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_train_community_word\"\n",
    "data_dir_blinker = \"/mounts/data/proj/ayyoob/align_induction/dataset/pruned_alignments_blinker_inter/\"\n",
    "data_dir_grc = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_grc_community_word/\"\n",
    "data_dir_heb = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_heb_community_word/\"\n",
    "\n",
    "train_dataset = torch.load(f\"{data_dir_train}/train_dataset_nox_noedge.torch.bin\")\n",
    "blinker_test_dataset = torch.load(f\"{data_dir_blinker}/train_dataset_nox_noedge.torch.bin\")\n",
    "grc_test_dataset = torch.load(f\"{data_dir_grc}/train_dataset_nox_noedge.torch.bin\")\n",
    "heb_test_dataset = torch.load(f\"{data_dir_heb}/train_dataset_nox_noedge.torch.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "\n",
    "postag_map = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "postag_reverse_map = {item[1]:item[0] for item in postag_map.items()}\n",
    "\n",
    "pos_lang_list = [\"eng-x-bible-mixed\", \"deu-x-bible-newworld\", \"ces-x-bible-newworld\", \n",
    "\t\t#\"prs-x-bible-goodnews\", \"hin-x-bible-newworld\", \"ron-x-bible-2006\",\n",
    "\t\t'dan-x-bible-newworld', 'fin-x-bible-helfi', 'nld-x-bible-newworld', 'pol-x-bible-newworld', 'swe-x-bible-newworld',\n",
    "\t\t\"ita-x-bible-2009\", \"fra-x-bible-louissegond\", \"spa-x-bible-newworld\"\n",
    "\t\t#,'prs-x-bible-goodnews'\n",
    "\t\t]\n",
    "\n",
    "def get_db_nodecount(dataset):\n",
    "\tres = 0\n",
    "\tfor lang in dataset.nodes_map.values():\n",
    "\t\tfor verse in lang.values():\n",
    "\t\t\tres += len(verse)\n",
    "\t\n",
    "\treturn res\n",
    "\n",
    "def get_language_nodes(dataset, lang_list, sentences):\n",
    "\tnode_count = get_db_nodecount(dataset)\n",
    "\tpos_labels = torch.zeros(node_count, len(postag_map))\n",
    "\n",
    "\tpos_node_cover = collections.defaultdict(list)\n",
    "\tfor lang in lang_list:\n",
    "\t\tif lang in dataset.nodes_map:\n",
    "\t\t\tfor sentence in sentences:\n",
    "\t\t\t\tif sentence in dataset.nodes_map[lang]:\n",
    "\t\t\t\t\tfor tok in dataset.nodes_map[lang][sentence]:\n",
    "\t\t\t\t\t\tpos_node_cover[sentence].append(dataset.nodes_map[lang][sentence][tok])\n",
    "\t\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\n",
    "def create_structures(dataset, all_tags):\n",
    "\tnode_count = get_db_nodecount(dataset)\n",
    "\tpos_labels = torch.zeros(node_count, len(postag_map))\n",
    "\tpos_node_cover = collections.defaultdict(list)\n",
    "\n",
    "\tfor lang in all_tags:\n",
    "\t\tfor sent_id in all_tags[lang]:\n",
    "\t\t\tsent_tags = all_tags[lang][sent_id]\n",
    "\t\t\tfor w_i in range(len(sent_tags)):\n",
    "\t\t\t\tif w_i not in dataset.nodes_map[lang][sent_id]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tpos_labels[dataset.nodes_map[lang][sent_id][w_i], sent_tags[w_i]] = 1\n",
    "\t\t\t\tpos_node_cover[sent_id].append(dataset.nodes_map[lang][sent_id][w_i])\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\n",
    "def get_pos_tags(dataset, pos_lang_list):\n",
    "\tall_tags = {}\n",
    "\tfor lang in pos_lang_list:\n",
    "\t\tif lang not in dataset.nodes_map:\n",
    "\t\t\tcontinue\n",
    "\t\tall_tags[lang] = {}\n",
    "\n",
    "\t\t#if os.path.exists(f'/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/{lang}.conllu'):\n",
    "\t\t#\tbase_path = '/mounts/work/silvia/POS/TAGGED_LANGS/STANZA/'\n",
    "\t\tif os.path.exists(F\"/mounts/work/silvia/POS/TAGGED_LANGS/{lang}.conllu\"):\t\n",
    "\t\t\tbase_path = F\"/mounts/work/silvia/POS/TAGGED_LANGS/\"\n",
    "\t\telse:\n",
    "\t\t\tbase_path = F\"/mounts/work/mjalili/projects/gnn-align/data/pbc_pos_tags/\"\n",
    "\t\t\n",
    "\t\twith codecs.open(F\"{base_path}{lang}.conllu\", \"r\", \"utf-8\") as lang_pos:\n",
    "\t\t\ttag_sent = []\n",
    "\t\t\tsent_id = \"\"\n",
    "\t\t\tfor sline in lang_pos:\n",
    "\t\t\t\tsline = sline.strip()\n",
    "\t\t\t\tif sline == \"\":\n",
    "\t\t\t\t\tif sent_id not in dataset.nodes_map[lang]:\n",
    "\t\t\t\t\t\ttag_sent = []\n",
    "\t\t\t\t\t\tsent_id = \"\"\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tall_tags[lang][sent_id] = [postag_map[p[3]] for p in tag_sent]\n",
    "\t\t\t\t\ttag_sent = []\n",
    "\t\t\t\t\tsent_id = \"\"\n",
    "\t\t\t\telif \"# verse_id\" in sline:\n",
    "\t\t\t\t\tsent_id = sline.split()[-1]\n",
    "\t\t\t\telif sline[0] == \"#\":\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttag_sent.append(sline.split(\"\\t\"))\n",
    "\n",
    "\tpos_labels, pos_node_cover = create_structures(dataset, all_tags)\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\t\n",
    "\n",
    "def get_pos_tags_from_bronze_data(dataset, file_path, language):\n",
    "\tfile_content = torch.load(file_path)\n",
    "\tall_tags = {language:{}}\n",
    "\n",
    "\tnode_count = get_db_nodecount(dataset)\n",
    "\tpos_labels = torch.zeros(node_count, len(postag_map))\n",
    "\tpos_node_cover = collections.defaultdict(list)\n",
    "\n",
    "\tfor sent_id in file_content:\n",
    "\t\tif sent_id in dataset.nodes_map[language]:\n",
    "\t\t\tm_list = file_content[sent_id].items() if isinstance(file_content[sent_id], dict) else file_content[sent_id]\n",
    "\t\t\tfor item in m_list:\n",
    "\t\t\t\tpos_labels[ dataset.nodes_map[language][sent_id][item[0]], item[1]] = 1\n",
    "\t\t\t\tpos_node_cover[sent_id].append(dataset.nodes_map[language][sent_id][item[0]])\n",
    "\t\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\n",
    "def read_ud_gold_file(f_path, w2v_model, lang):\n",
    "\tpos_labels = torch.zeros(w2v_model.wv.vectors.shape[0], len(postag_map))\n",
    "\twith codecs.open(f_path, \"r\", \"utf-8\") as lang_pos:\n",
    "\t\t\tfor sline in lang_pos:\n",
    "\t\t\t\tsline = sline.strip()\n",
    "\t\t\t\tif sline == \"\":\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telif \"# verse_id\" in sline:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telif sline[0] == \"#\":\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tline_items = list(sline.split(\"\\t\"))\n",
    "\t\t\t\t\tword = line_items[1]\n",
    "\t\t\t\t\ttag = line_items[3]\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t# print(f'{lang}:{word.lower()}')\n",
    "\t\t\t\t\t\tidx = w2v_model.wv.key_to_index[f'{lang}:{word.lower()}']\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\texcept Exception as e: # some words from the gold data may not exist in bible. we just skip them\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif tag == '_':\n",
    "\t\t\t\t\t\t#print('tag is', tag)\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tpos_labels[idx, postag_map[tag]] += 1\n",
    "\t\n",
    "\tindex = (torch.sum(pos_labels, dim=1) > 0.1).nonzero()\n",
    "\t\n",
    "\tmaxes, tags = torch.max(pos_labels, dim=1)\n",
    "\tprint(torch.sum(pos_labels))\n",
    "\n",
    "\treturn tags[index], index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blinker_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_blinker_full_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf12 = \"eng-x-bible-mixed\"\n",
    "editf22 = 'fra-x-bible-louissegond'\n",
    "\n",
    "test_gold_eng_fra = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/eng_fra_pbc/eng-fra.gold\"\n",
    "\n",
    "pros_blinker, surs_blinker = autils.load_gold(test_gold_eng_fra)\n",
    "\n",
    "blinker_verse_alignments_inter = {}\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in blinker_test_dataset.nodes_map:\n",
    "    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "blinker_verses = [item[0] for item in sorted_verses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importlib.reload(afeatures)\n",
    "#grc_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_grc_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf_fin = \"fin-x-bible-helfi\"\n",
    "editf_grc = 'grc-x-bible-helfi'\n",
    "\n",
    "test_gold_grc = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_test.txt\"\n",
    "\n",
    "pros_grc, surs_grc = autils.load_gold(test_gold_grc)\n",
    "\n",
    "grc_test_verse_alignments_inter = {}\n",
    "grc_test_verse_alignments_gdfa = {}\n",
    "gc.collect()\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in grc_test_dataset.nodes_map:\n",
    "    for verse in grc_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in grc_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = grc_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "grc_test_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heb_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "\n",
    "test_gold_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_test.txt\"\n",
    "\n",
    "pros_heb, surs_heb = autils.load_gold(test_gold_heb)\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in heb_test_dataset.nodes_map:\n",
    "    for verse in heb_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in heb_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = heb_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "heb_test_verses = [item[0] for item in sorted_verses]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_map = {}\n",
    "\n",
    "for edit in train_dataset.nodes_map:\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in train_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = train_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "all_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "long_verses = set()\n",
    "\n",
    "for edit in train_dataset.nodes_map.keys():\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        to_print = False\n",
    "        for tok in train_dataset.nodes_map[edit][verse]:\n",
    "            if tok > 150:\n",
    "                to_print = True\n",
    "        if to_print == True:\n",
    "            long_verses.add(verse)\n",
    "\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "\n",
    "masked_verses = list(long_verses)\n",
    "#masked_verses.extend(blinker_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "class POSTAGGNNDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, verses, edit_files, alignments, node_cover, pos_labels, data_dir, create_data=False, group_size = 20):\n",
    "        self.node_cover = node_cover\n",
    "        self.pos_labels = pos_labels\n",
    "        self.data_dir = data_dir\n",
    "        self.items = self.calculate_size(verses, group_size, node_cover)\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if create_data:\n",
    "            self.calculate_verse_stats(verses, edit_files, alignments, dataset, data_dir)            \n",
    "        \n",
    "    def calculate_size(self, verses, group_size, node_cover):\n",
    "        res = []\n",
    "        for verse in verses:\n",
    "            covered_nodes = node_cover[verse]\n",
    "            random.shuffle(covered_nodes)\n",
    "            items = [covered_nodes[i:i + group_size] for i in range(0, len(covered_nodes), group_size)]\n",
    "            res.extend([(verse, i) for i in items])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calculate_verse_stats(self,verses, edition_files, alignments, dataset, data_dir):\n",
    "\n",
    "        min_edge = 0\n",
    "        for verse in tqdm(verses):\n",
    "            min_nodes = 99999999999999\n",
    "            max_nodes = 0\n",
    "            edges_tmp = [[],[]]\n",
    "            x_tmp = []\n",
    "            features = []\n",
    "            for i,editf1 in enumerate(edition_files):\n",
    "                for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "                    aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "                    if aligns != None:\n",
    "                        for align in aligns:\n",
    "                            try:\n",
    "                                n1,_ = gutils.node_nom(verse, editf1, align[0], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                n2,_ = gutils.node_nom(verse, editf2, align[1], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                edges_tmp[0].extend([n1, n2])\n",
    "\n",
    "                                max_nodes = max(n1, n2, max_nodes)\n",
    "                                min_nodes = min(n1, n2, min_nodes)\n",
    "                            except Exception as e:\n",
    "                                print(editf1, editf2, verse)\n",
    "                                raise(e)\n",
    "\n",
    "            self.verse_info = {}\n",
    "\n",
    "            self.verse_info['padding'] = min_nodes\n",
    "            \n",
    "            self.verse_info['x'] = torch.clone(dataset.x[min_nodes:max_nodes+1,:])\n",
    "            \n",
    "            self.verse_info['edge_index'] = torch.clone(dataset.edge_index[:, min_edge : min_edge + len(edges_tmp[0])] - min_nodes)\n",
    "\n",
    "            if torch.min(self.verse_info['edge_index']) != 0:\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info['edge_index']))\n",
    "            \n",
    "            if self.verse_info['x'].shape[0] != torch.max(self.verse_info['edge_index']) + 1 :\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info['edge_index']))\n",
    "            \n",
    "            min_edge = min_edge + len(edges_tmp[0])\n",
    "\n",
    "            torch.save(self.verse_info, f\"{data_dir}/verses/{verse}_info.torch.bin\")\n",
    "        \n",
    "        dataset.x = None\n",
    "        dataset.edge_index = None\n",
    "        torch.save(dataset, f\"{data_dir}/train_dataset_nox_noedge.torch.bin\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        verse, nodes = self.items[idx]\n",
    "        \n",
    "        self.verse_info = {verse: torch.load(f'{self.data_dir}/verses/{verse}_info.torch.bin')}\n",
    "\n",
    "\n",
    "        word_number = self.verse_info[verse]['x'][:, 9]\n",
    "        padding = self.verse_info[verse]['padding']\n",
    "        \n",
    "        language_based_nodes, transformer_indices = posutil.get_language_based_nodes(self.dataset.nodes_map, verse, nodes, padding)\n",
    "\n",
    "        ## # Add POSTAG to set of features\n",
    "        #postags = self.pos_labels[padding: self.verse_info[verse]['x'].size(0) + padding, : ]\n",
    "        #postags = postags.detach().clone()\n",
    "        #postags[torch.LongTensor(nodes) - padding, :] = 0\n",
    "        #self.verse_info[verse]['x'] = torch.cat((self.verse_info[verse]['x'], postags), dim=1)\n",
    "\n",
    "        # Add token id as a feature, used to extract token information (like token's tag distribution)\n",
    "        word_number = torch.unsqueeze(word_number, 1)\n",
    "        self.verse_info[verse]['x'] = torch.cat((self.verse_info[verse]['x'], word_number), dim=1)\n",
    "\n",
    "        return {'verse':verse, 'x':self.verse_info[verse]['x'], 'edge_index':self.verse_info[verse]['edge_index'], \n",
    "                'pos_classes': self.pos_labels[nodes, :], 'pos_index': torch.LongTensor(nodes) - padding, \n",
    "                'padding': padding, 'lang_based_nodes': language_based_nodes, 'transformer_indices': transformer_indices}\n",
    "\n",
    "def create_me_a_gnn_dataset_you_stupid(node_covers, labels, group_size=100, editions=current_editions):\n",
    "\n",
    "    train_ds = POSTAGGNNDataset(train_dataset, train_verses, editions, {}, node_covers[0], labels[0], data_dir_train, group_size=group_size)\n",
    "    grc_ds = POSTAGGNNDataset(grc_test_dataset, grc_test_verses, editions, {}, node_covers[1], labels[1], data_dir_grc, group_size=group_size)\n",
    "    heb_ds = POSTAGGNNDataset(heb_test_dataset, heb_test_verses, editions, {}, node_covers[2], labels[2], data_dir_heb, group_size=group_size)\n",
    "    blinker_ds = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, editions, {}, node_covers[3], labels[3], data_dir_blinker, group_size=group_size)\n",
    "\n",
    "    return train_ds, grc_ds, heb_ds, blinker_ds\n",
    "\n",
    "# if \"eng-x-bible-mixed\" in pos_lang_list:\n",
    "#     pos_lang_list.remove(\"eng-x-bible-mixed\")\n",
    "train_pos_labels, train_pos_node_cover = get_pos_tags(train_dataset, pos_lang_list)\n",
    "#torch.save({'pos_labels':train_pos_labels, 'pos_node_cover':train_pos_node_cover}, f'{data_dir_train}/pos_data.torch.bin')\n",
    "#pos_data = torch.load(f'{data_dir_train}/pos_data.torch.bin')\n",
    "#train_pos_labels, train_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "\n",
    "blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, pos_lang_list)\n",
    "##torch.save({'pos_labels':blinker_pos_labels, 'pos_node_cover': blinker_pos_node_cover}, f'{data_dir_blinker}/pos_data.torch.bin')\n",
    "#pos_data = torch.load(f'{data_dir_blinker}/pos_data.torch.bin')\n",
    "#blinker_pos_labels, blinker_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "\n",
    "grc_pos_labels, grc_pos_node_cover = get_pos_tags(grc_test_dataset, pos_lang_list)\n",
    "#torch.save({'pos_labels':grc_pos_labels, 'pos_node_cover': grc_pos_node_cover}, f'{data_dir_grc}/pos_data.torch.bin')\n",
    "#pos_data = torch.load(f'{data_dir_grc}/pos_data.torch.bin')\n",
    "#grc_pos_labels, grc_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "\n",
    "heb_pos_labels, heb_pos_node_cover = get_pos_tags(heb_test_dataset, pos_lang_list)\n",
    "#torch.save({'pos_labels':heb_pos_labels, 'pos_node_cover': heb_pos_node_cover}, f'{data_dir_heb}/pos_data.torch.bin')\n",
    "#pos_data = torch.load(f'{data_dir_heb}/pos_data.torch.bin')\n",
    "#heb_pos_labels, heb_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "\n",
    "gnn_dataset_train_pos, gnn_dataset_grc_pos, gnn_dataset_heb_pos, gnn_dataset_blinker_pos = create_me_a_gnn_dataset_you_stupid(\n",
    "    [train_pos_node_cover, grc_pos_node_cover, heb_pos_node_cover, blinker_pos_node_cover], [train_pos_labels, grc_pos_labels, heb_pos_labels, blinker_pos_labels], group_size=256)\n",
    "\n",
    "gnn_dataset_train_pos_bigbatch, gnn_dataset_grc_pos_bigbatch, gnn_dataset_heb_pos_bigbatch, gnn_dataset_blinker_pos_bigbatch = create_me_a_gnn_dataset_you_stupid(\n",
    "    [train_pos_node_cover, grc_pos_node_cover, heb_pos_node_cover, blinker_pos_node_cover], [train_pos_labels, grc_pos_labels, heb_pos_labels, blinker_pos_labels], group_size=10000)\n",
    "\n",
    "train_data_loader_bigbatch = DataLoader(gnn_dataset_train_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "grc_data_loader_bigbatch = DataLoader(gnn_dataset_grc_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "heb_data_loader_bigbatch = DataLoader(gnn_dataset_heb_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "blinker_data_loader_bigbatch = DataLoader(gnn_dataset_blinker_pos_bigbatch, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39817\n"
     ]
    }
   ],
   "source": [
    "print(len(gnn_dataset_train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44028011 156\n",
      "41012034 162\n",
      "45012008 158\n",
      "44014012 160\n",
      "58006008 156\n",
      "58010022 156\n",
      "66002018 156\n",
      "43013022 162\n",
      "44001020 160\n",
      "46014019 158\n",
      "50004005 158\n",
      "50001025 158\n",
      "42023030 162\n",
      "44007042 160\n",
      "40002018 160\n",
      "44016001 160\n",
      "48004008 158\n",
      "52002017 158\n",
      "59002010 158\n",
      "44028018 160\n",
      "44014020 160\n",
      "66021020 154\n",
      "66016009 152\n",
      "49005007 158\n",
      "66022013 156\n",
      "46007036 158\n",
      "44005036 160\n",
      "56001010 156\n",
      "46014009 158\n",
      "44008027 158\n",
      "58001003 156\n",
      "41009026 162\n",
      "40010024 162\n",
      "43011010 162\n",
      "44013001 160\n",
      "41001026 162\n",
      "51004002 156\n",
      "48002017 158\n",
      "49005032 158\n",
      "46004017 158\n",
      "40010025 162\n",
      "54003013 158\n",
      "55001011 152\n",
      "41003034 162\n",
      "44014006 158\n",
      "40027024 162\n",
      "44024003 150\n",
      "42005007 162\n",
      "44012019 160\n",
      "44024014 160\n",
      "51003015 156\n",
      "47003013 158\n",
      "42023014 160\n",
      "44017011 160\n",
      "58012028 156\n",
      "44004026 158\n",
      "40007016 162\n",
      "44014013 160\n",
      "54002007 154\n",
      "44027006 160\n",
      "44025024 158\n",
      "54004015 158\n",
      "44010004 160\n",
      "40002018 160\n",
      "40007016 162\n",
      "40010024 162\n",
      "40010025 162\n",
      "40027024 162\n",
      "41001026 162\n",
      "41003034 162\n",
      "41009026 162\n",
      "41012034 162\n",
      "42005007 162\n",
      "42006040 162\n",
      "42023014 160\n",
      "42023030 162\n",
      "43011010 162\n",
      "43013022 162\n",
      "44001020 160\n",
      "44002010 158\n",
      "44004026 158\n",
      "44005036 160\n",
      "44007042 160\n",
      "44008027 158\n",
      "44010004 160\n",
      "44012019 160\n",
      "44013001 160\n",
      "44014006 158\n",
      "44014012 160\n",
      "44014013 160\n",
      "44014020 160\n",
      "44016001 160\n",
      "44017011 160\n",
      "44020004 160\n",
      "44024003 150\n",
      "44024014 160\n",
      "44025024 158\n",
      "44026012 158\n",
      "44027006 160\n",
      "44028011 156\n",
      "44028018 160\n",
      "45012008 158\n",
      "46001014 158\n",
      "46004017 158\n",
      "46007036 158\n",
      "46014009 158\n",
      "46014019 158\n",
      "47003013 158\n",
      "48002017 158\n",
      "48004008 158\n",
      "49005007 158\n",
      "49005032 158\n",
      "50001025 158\n",
      "50004005 158\n",
      "51003015 156\n",
      "51004002 156\n",
      "52002017 158\n",
      "54002007 154\n",
      "54003013 158\n",
      "54004015 158\n",
      "55001011 152\n",
      "56001010 156\n",
      "58001003 156\n",
      "58006008 156\n",
      "58010022 156\n",
      "58012028 156\n",
      "59002010 158\n",
      "66002018 156\n",
      "66016009 152\n",
      "66021020 154\n",
      "66022013 156\n",
      "42006040 162\n",
      "44026012 158\n",
      "46001014 158\n",
      "44002010 158\n",
      "44020004 160\n",
      "72 19\n",
      "73 41\n",
      "74 29\n",
      "75 65\n",
      "76 157\n",
      "77 347\n",
      "78 991\n",
      "79 1605\n",
      "80 887\n",
      "81 2830\n",
      "12 39\n",
      "37 12\n",
      "38 35\n",
      "39 229\n",
      "40 1361\n",
      "41 8999\n",
      "42 7118\n",
      "72 13\n",
      "73 37\n",
      "74 26\n",
      "75 58\n",
      "76 132\n",
      "77 303\n",
      "78 877\n",
      "79 1429\n",
      "80 805\n",
      "81 2572\n",
      "40 24\n",
      "41 94\n",
      "42 54\n",
      "79 24\n",
      "80 17\n",
      "81 14\n",
      "76 25\n",
      "77 46\n",
      "78 124\n",
      "79 198\n",
      "80 99\n",
      "81 270\n",
      "39 24\n",
      "40 172\n",
      "41 1083\n",
      "42 920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_verse_size_distrib(nodes_map):\n",
    "    verse_sizes = {}\n",
    "    distrib = [0 for i in range(84)]\n",
    "    for edition in nodes_map:\n",
    "        for verse in nodes_map[edition]:\n",
    "            if verse in verse_sizes:\n",
    "                verse_sizes[verse] += 1\n",
    "            else:\n",
    "                verse_sizes[verse] = 1\n",
    "    \n",
    "    for verse in verse_sizes:\n",
    "        distrib[verse_sizes[verse]] += 1\n",
    "    \n",
    "    for i in range(len(distrib)):\n",
    "        if distrib[i] > 10:\n",
    "            print(i, distrib[i])\n",
    "\n",
    "def update_verse_sizes(nodes_map, verse_sizes):\n",
    "    for edition in nodes_map:\n",
    "        for verse in nodes_map[edition]:\n",
    "            if verse in verse_sizes:\n",
    "                verse_sizes[verse] += 1\n",
    "            else:\n",
    "                verse_sizes[verse] = 1\n",
    "\n",
    "def update_distribution(nodes_map, verse_sizes, distrib, edition):\n",
    "    if edition in nodes_map:\n",
    "        for verse in nodes_map[edition]:\n",
    "            if verse_sizes[verse] > 99:\n",
    "                print(verse, verse_sizes[verse])\n",
    "                continue\n",
    "            distrib[verse_sizes[verse]] += 1\n",
    "                \n",
    "def get_language_verse_distrib(nodes_map1, nodes_map2, nodes_map3, nodes_map4, edition):\n",
    "    global all_verse_sizes\n",
    "    all_verse_sizes = {}\n",
    "\n",
    "    update_verse_sizes(nodes_map1, all_verse_sizes)\n",
    "    update_verse_sizes(nodes_map2, all_verse_sizes)\n",
    "    update_verse_sizes(nodes_map3, all_verse_sizes)\n",
    "    update_verse_sizes(nodes_map4, all_verse_sizes)\n",
    "\n",
    "    \n",
    "    distrib = [0 for i in range(100)]\n",
    "    update_distribution(nodes_map1, all_verse_sizes, distrib, edition)\n",
    "    update_distribution(nodes_map2, all_verse_sizes, distrib, edition)\n",
    "    update_distribution(nodes_map3, all_verse_sizes, distrib, edition)\n",
    "    update_distribution(nodes_map4, all_verse_sizes, distrib, edition)\n",
    "    \n",
    "    for i in range(len(distrib)):\n",
    "        if distrib[i] > 10:\n",
    "            print(i, distrib[i])\n",
    "\n",
    "get_language_verse_distrib(train_dataset.nodes_map, blinker_test_dataset.nodes_map, grc_test_dataset.nodes_map, heb_test_dataset.nodes_map, 'tam-x-bible-newworld')\n",
    "get_verse_size_distrib(train_dataset.nodes_map)\n",
    "get_verse_size_distrib(blinker_test_dataset.nodes_map)\n",
    "get_verse_size_distrib(grc_test_dataset.nodes_map)\n",
    "get_verse_size_distrib(heb_test_dataset.nodes_map)\n",
    "# big_verses = []\n",
    "# for verse in train_verses:\n",
    "#     if all_verse_sizes[verse] > 50:\n",
    "#         big_verses.append(verse)\n",
    "# gnn_dataset_train_pos = POSTAGGNNDataset(train_dataset, big_verses, editions, {}, train_pos_node_cover, train_pos_labels, data_dir_train, group_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loadrs_for_target_editions(target_editions, dataset, pos_node_cover, verses, data_dir):\n",
    "    target_pos_labels, target_pos_node_cover = get_language_nodes(dataset, target_editions, pos_node_cover.keys())\n",
    "    gnn_dataset_target_pos = POSTAGGNNDataset(dataset, verses, None, {}, target_pos_node_cover, target_pos_labels, data_dir, group_size = 50000)\n",
    "    target_data_loader = DataLoader(gnn_dataset_target_pos, batch_size=1, shuffle=False)\n",
    "    \n",
    "    return target_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_eng_langs = pos_lang_list[:]\n",
    "#no_eng_langs.remove('eng-x-bible-mixed')\n",
    "\n",
    "## # train_pos_labels, train_pos_node_cover = get_pos_tags(train_dataset, no_eng_langs)\n",
    "## # gnn_dataset_train_pos = POSTAGGNNDataset(train_dataset, train_verses, current_editions, verse_alignments_inter,\n",
    "## #                        train_pos_node_cover, train_pos_labels, data_dir_train, group_size = 10)\n",
    "\n",
    "\n",
    "#_, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, ['eng-x-bible-mixed'])\n",
    "#blinker_pos_labels, _ = get_pos_tags(blinker_test_dataset, pos_lang_list)\n",
    "#gnn_dataset_blinker_pos_onlyeng = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                            blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    #model.encoder.feature_encoder.feature_types[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "    #model.encoder.feature_encoder.feature_types[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "    #model.encoder.feature_encoder.feature_types[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "    #model.encoder.feature_encoder.feature_types[8] = afeatures.OneHotFeature(32, 250, 'community_2')\n",
    "    #model.encoder.feature_encoder.feature_types[9] = afeatures.MappingFeature(100, 'word')\n",
    "    #model.encoder.feature_encoder.feature_types[10] = afeatures.MappingFeature(len(postag_map), 'tag_priors', freeze=True)\n",
    "    torch.save(model, f'/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_{name}_' + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_test_pos_labels, eng_test_pos_node_cover = get_pos_tags(blinker_test_dataset, ['eng-x-bible-mixed'])\n",
    "# gnn_dataset_engtest_pos = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, {},\n",
    "#                       eng_test_pos_node_cover, eng_test_pos_labels, data_dir_blinker, group_size = 500)\n",
    "# engtest_data_loader = DataLoader(gnn_dataset_engtest_pos, batch_size=1, shuffle=False)\n",
    "\n",
    "# #test(1, engtest_data_loader, mask_language=False) \n",
    "\n",
    "# finetune_pos_labels, finetune_pos_node_cover = get_pos_tags(train_dataset, ['eng-x-bible-mixed'])\n",
    "# gnn_dataset_finetune_pos = POSTAGGNNDataset(train_dataset, train_verses, current_editions, {},\n",
    "#                       finetune_pos_node_cover, finetune_pos_labels, data_dir_train, group_size = 100)\n",
    "# finetune_data_loader = DataLoader(gnn_dataset_finetune_pos, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "#train(1, finetune_data_loader, mask_language=False)\n",
    "#test(1, engtest_data_loader, mask_language=False) \n",
    "\n",
    "# blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, ['eng-x-bible-mixed'])\n",
    "# gnn_dataset_blinker_pos = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                              blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 10000)\n",
    "\n",
    "# blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, no_eng_langs)\n",
    "# gnn_dataset_blinker_pos_majvoting_test = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                              blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 10000)\n",
    "\n",
    "# test_data_loader = DataLoader(gnn_dataset_blinker_pos, batch_size=1, shuffle=False)\n",
    "# test_data_loader_majvoting = DataLoader(gnn_dataset_blinker_pos_majvoting_test, batch_size=1, shuffle=False)\n",
    "# majority_voting_test(test_data_loader, test_data_loader_majvoting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yor_bronze_labels, yor_bronze_node_cover = get_pos_tags_from_bronze_data(heb_test_dataset, '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_yor-x-bible-2010_posfeatFalse_transformerFalse_trainWEFalse_maskLangTrue.pickle', 'yor-x-bible-2010')\n",
    "yor_bronze_labels, yor_bronze_node_cover = get_pos_tags_from_bronze_data(heb_test_dataset, '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_yor-x-bible-2010_posfeatTrue_transformerFalse_trainWEFalse_maskLangTrue.pickle', 'yor-x-bible-2010')\n",
    "# yor_bronze_labels, yor_bronze_node_cover = get_pos_tags_from_bronze_data(heb_test_dataset, '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_yor-x-bible-2010_9lang_pos_tagging_posfeatTrue_transformerTrue6layresresidual_trainWETrue_maskLangFalse_epoch3_trainYoruba_20220219-052901.pickle_maskLangFalse.pickle', 'yor-x-bible-2010')\n",
    "#yor_bronze_labels, yor_bronze_node_cover = get_pos_tags_from_bronze_data(heb_test_dataset, '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_yor-x-bible-2010_11lang_posfeatTruealltargets_transformerTrue6layresresidual_trainWEFalse_epoch1_noEng_maskLangTrue.pickle', 'yor-x-bible-2010')\n",
    "\n",
    "tam_bronze_labels, tam_bronze_node_cover = get_pos_tags_from_bronze_data(grc_test_dataset, '/mounts/work/ayyoob/results/gnn_align/yoruba/pos_tags_tam-x-bible-newworld_12lang_posfeatFalsealltargets_transformerFalse6layresresidual_trainWEFalse_epoch1__maskLangTrue.pickle', 'tam-x-bible-newworld')\n",
    "\n",
    "\n",
    "gnn_dataset_yorbronz_pos = POSTAGGNNDataset(heb_test_dataset, heb_test_verses, current_editions, {},\n",
    "                      yor_bronze_node_cover, yor_bronze_labels, data_dir_heb, group_size = 500)\n",
    "yorbronz_data_loader = DataLoader(gnn_dataset_yorbronz_pos, batch_size=1, shuffle=False)\n",
    "\n",
    "gnn_dataset_tambronz_pos = POSTAGGNNDataset(grc_test_dataset, grc_test_verses, current_editions, {},\n",
    "                      tam_bronze_node_cover, tam_bronze_labels, data_dir_grc, group_size = 500)\n",
    "tambronz_data_loader = DataLoader(gnn_dataset_tambronz_pos, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mostfreq(tambronz_data_loader, True, tam_gold_mostfreq_tag, tam_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)), True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_existing_model(model_path, use_transformer):\n",
    "    global model\n",
    "    model = torch.load(f'/mounts/work/ayyoob/models/gnn/checkpoint/postagging/{model_path}', map_location=torch.device('cpu'))\n",
    "    model.to(dev)\n",
    "    print(model_path)\n",
    "    if use_transformer:\n",
    "        model.decoder.to(dev2)\n",
    "\n",
    "    test_mostfreq(yor_data_loader_heb, True, yor_gold_mostfreq_tag, yor_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "    #test(1, yorbronz_data_loader, True)\n",
    "    #test(1, engtest_data_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "por-x-bible-versaointernacional\n"
     ]
    }
   ],
   "source": [
    "for edit in current_editions:\n",
    "    if edit.startswith('por'):\n",
    "        print(edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5424.)\n",
      "tensor(1763.)\n",
      "tensor(10835.)\n",
      "tensor(17562.)\n"
     ]
    }
   ],
   "source": [
    "yor_gold_mostfreq_tag, yor_gold_mostfreq_index = read_ud_gold_file('/mounts/work/silvia/POS/yo_ytb-ud-test.conllu', w2v_model, 'yor')\n",
    "yor_data_loader_grc = get_data_loadrs_for_target_editions(['yor-x-bible-2010'], grc_test_dataset, grc_pos_node_cover, grc_test_verses, data_dir_grc)\n",
    "yor_data_loader_heb = get_data_loadrs_for_target_editions(['yor-x-bible-2010'], heb_test_dataset, heb_pos_node_cover, heb_test_verses, data_dir_heb)\n",
    "\n",
    "tam_gold_mostfreq_tag, tam_gold_mostfreq_index = read_ud_gold_file('/mounts/work/silvia/POS/ta_mwtt-ud-test.conllu', w2v_model, 'tam')\n",
    "tam_data_loader_grc = get_data_loadrs_for_target_editions(['tam-x-bible-newworld'], grc_test_dataset, grc_pos_node_cover, grc_test_verses, data_dir_grc)\n",
    "\n",
    "arb_gold_mostfreq_tag, arb_gold_mostfreq_index = read_ud_gold_file('/nfs/datx/UD/ar_pud-ud-test.conllu', w2v_model, 'arb')\n",
    "arb_data_loader_grc = get_data_loadrs_for_target_editions(['arb-x-bible'], grc_test_dataset, grc_pos_node_cover, grc_test_verses, data_dir_grc)\n",
    "\n",
    "por_gold_mostfreq_tag, por_gold_mostfreq_index = read_ud_gold_file('/nfs/datx/UD/pt_pud-ud-test.conllu', w2v_model, 'por')\n",
    "por_data_loader_grc = get_data_loadrs_for_target_editions(['por-x-bible-versaointernacional'], grc_test_dataset, grc_pos_node_cover, grc_test_verses, data_dir_grc)\n",
    "\n",
    "#test_existing_model('pos_tagging_posfeatFalse_transformerFalse_trainWEFalse_maskLangTrue_20220209-190017.pickle', False)\n",
    "\n",
    "#test_mostfreq(yorbronz_data_loader, True, yor_gold_mostfreq_tag, yor_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)), from_target=True)\n",
    "#test_mostfreq(tam_data_loader_grc, True, tam_gold_mostfreq_tag, tam_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mostfreq(yor_data_loader_grc, True, yor_gold_mostfreq_tag, yor_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "# test_mostfreq(arb_data_loader_grc, True, arb_gold_mostfreq_tag, arb_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "# test_mostfreq(por_data_loader_grc, True, por_gold_mostfreq_tag, por_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "# test_mostfreq(tam_data_loader_grc, True, tam_gold_mostfreq_tag, tam_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(116768.)\n",
      "tensor(82326.)\n",
      "tensor(34442.)\n",
      "tensor(0.1260)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(posutil)\n",
    "threshold = 0.95\n",
    "#target_editions = []\n",
    "\n",
    "#for edition in current_editions:\n",
    "#    if edition not in pos_lang_list:\n",
    "#        target_editions.append(edition)\n",
    "\n",
    "\n",
    "#model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_11langs-nopersian_posfeatFalsealltargets_transformerFalse6layresresidualFalse_trainWEFalse_maskLangTrue_epoch1__20220305-101058_earlystopping-GA_.pickle', map_location=torch.device('cpu'))\n",
    "#model.to(dev)\n",
    "##model.decoder.to(dev2)\n",
    "#test_mostfreq(yor_data_loader_heb, True, yor_gold_mostfreq_tag, yor_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "\n",
    "#target_data_loader_train = get_data_loadrs_for_target_editions(target_editions, train_dataset, train_pos_node_cover, train_verses, data_dir_train)\n",
    "#target_data_loader_grc = get_data_loadrs_for_target_editions(target_editions, grc_test_dataset, grc_pos_node_cover, grc_test_verses, data_dir_grc)\n",
    "#target_data_loader_heb = get_data_loadrs_for_target_editions(target_editions, heb_test_dataset, heb_pos_node_cover, heb_test_verses, data_dir_heb)\n",
    "#target_data_loader_blinker = get_data_loadrs_for_target_editions(target_editions, blinker_test_dataset, blinker_pos_node_cover, blinker_verses, data_dir_blinker)\n",
    "\n",
    "#res_ = torch.load(f'/mounts/work/ayyoob/results/gnn_postag/data/11lang-feature_vectors_posfeat{False}_transformer{True}_trainWE{False}_maskLang{True}_epoch{1}_Englishandallothertargets_typecheckFalse.torch.bin')\n",
    "#tag_frequencies, tag_frequencies_target, train_pos_node_cover_ext, train_pos_labels_ext, grc_pos_node_cover_ext, grc_pos_labels_ext, heb_pos_node_cover_ext, heb_pos_labels_ext, blinker_pos_node_cover_ext, blinker_pos_labels_ext = res_\n",
    "# res_ = posutil.get_tag_frequencies_node_tags(model, target_editions, train_pos_node_cover, train_pos_labels, grc_pos_node_cover, grc_pos_labels, heb_pos_node_cover, heb_pos_labels, blinker_pos_node_cover, blinker_pos_labels,\n",
    "#                                     len(postag_map), target_data_loader_train, target_data_loader_grc, target_data_loader_heb, target_data_loader_blinker,\n",
    "#                                     train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch, DataEncoder, target_train_treshold=threshold, type_check=False\n",
    "#                                     , source_tag_frequencies=tag_frequencies_source)\n",
    "\n",
    "# torch.save(res_, f'{len(pos_lang_list)}langs-nopersian_posfeatFalsealltargets_transformer{False}6layresresidual{False}_trainWE{False}_maskLang{True}_epoch{1}__{start_time}_earlystopping-GA_th{threshold}_typecheckFalse')\n",
    "res_ = torch.load('/mounts/work/ayyoob/results/gnn_postag/data/11langs-nopersian_posfeatFalsealltargets_transformerFalse6layresresidualFalse_trainWEFalse_maskLangTrue_epoch1__20220306-170230_earlystopping-GA_th95_typecheckFalse')\n",
    "tag_frequencies, tag_frequencies_target, train_pos_node_cover_ext, train_pos_labels_ext, grc_pos_node_cover_ext, grc_pos_labels_ext, heb_pos_node_cover_ext, heb_pos_labels_ext, blinker_pos_node_cover_ext, blinker_pos_labels_ext = res_\n",
    "\n",
    "print(torch.sum(blinker_pos_labels_ext))\n",
    "print(torch.sum(blinker_pos_labels))\n",
    "print(torch.sum(blinker_pos_labels_ext) - torch.sum(blinker_pos_labels))\n",
    "print((torch.sum(blinker_pos_labels_ext) - torch.sum(blinker_pos_labels))/273258) \n",
    "\n",
    "tag_frequencies_source = tag_frequencies - tag_frequencies_target\n",
    "# print(1, torch.sum(tag_frequencies_target))\n",
    "# posutil.keep_only_type_tags(tag_frequencies_target)\n",
    "# print(1, torch.sum(tag_frequencies_target))\n",
    "word_frequencies_target = torch.sum(tag_frequencies_target.to(torch.device('cpu')), dim=1)\n",
    "tag_frequencies = tag_frequencies_source + tag_frequencies_target\n",
    "tag_frequencies_copy = tag_frequencies.detach().clone()\n",
    "\n",
    "tag_frequencies_copy[torch.logical_and(word_frequencies_target>0.1, word_frequencies_target<3), :] = 0.0000001\n",
    "\n",
    "# We have to give uniform noise to some training examples to prevent the model from returning one of the most frequent tags always!!\n",
    "uniform_noise = torch.BoolTensor(tag_frequencies.size(0))\n",
    "uniform_noise[:] = True\n",
    "shuffle_tensor = torch.randperm(tag_frequencies.size(0))[:int(tag_frequencies.size(0)*0.7)]\n",
    "uniform_noise[shuffle_tensor] = False\n",
    "tag_frequencies_copy[torch.logical_and(uniform_noise, word_frequencies_target < 0.1), :] = 0.0000001\n",
    "\n",
    "sm = torch.sum(tag_frequencies_copy, dim=1)\n",
    "normalized_tag_frequencies = (tag_frequencies_copy.transpose(1,0) / sm).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "def create_model(train_gnn_dataset, grc_gnn_dataset, heb_gnn_dataset, blinker_gnn_dataset, test_gnn_dataset,\n",
    "                tag_frequencies=False, use_transformers=False, train_word_embedding=False, mask_language=True, residual_connection = False,\n",
    "                 params=''):\n",
    "    global model, criterion, optimizer, early_stopping, start_time\n",
    "\n",
    "    features = train_dataset.features[:]\n",
    "    #features.append(afeatures.PassFeature(name='posTAG', dim=len(postag_map)))\n",
    "    if tag_frequencies:\n",
    "        features.append(afeatures.MappingFeature(len(postag_map), 'tag_priors', freeze=True))\n",
    "    features[9].freeze = not train_word_embedding\n",
    "    \n",
    "    train_data_loader = DataLoader(train_gnn_dataset, batch_size=1, shuffle=True)\n",
    "    grc_data_loader = DataLoader(grc_gnn_dataset, batch_size=1, shuffle=True)\n",
    "    heb_data_loader = DataLoader(heb_gnn_dataset, batch_size=1, shuffle=True)\n",
    "    blinker_data_loader = DataLoader(blinker_gnn_dataset, batch_size=1, shuffle=True)\n",
    "    test_data_loader = DataLoader(test_gnn_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    clean_memory()\n",
    "    drop_out = 0\n",
    "    n_head = 1\n",
    "    in_dim = sum(t.out_dim for t in features)\n",
    "    start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    early_stopping = EarlyStopping(verbose=True, path=f'/mounts/work/ayyoob/models/gnn/checkpoint/postagging/check_point_{start_time}.pt', patience=5, delta=4)\n",
    "    channels = 512\n",
    "    decoder_in_dim = n_head * channels \n",
    "\n",
    "    print('len features', len(features), f'start time: {start_time}')\n",
    "\n",
    "    # model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer20210910-235352-.pickle')\n",
    "    if use_transformers:\n",
    "        decoder = POSDecoderTransformer(decoder_in_dim, decoder_in_dim*2, len(postag_map), residual_connection, drop_out=drop_out).to(dev2)\n",
    "    else:\n",
    "        decoder = POSDecoder(decoder_in_dim, decoder_in_dim*2, len(postag_map))\n",
    "        \n",
    "    model = pyg_nn.GAE(Encoder(in_dim, channels, features, n_head, has_tagfreq_feature=tag_frequencies), decoder).to(dev)\n",
    "\n",
    "\n",
    "    if use_transformers:\n",
    "        decoder.to(dev2)\n",
    "\n",
    "    # model.to(dev)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "    torch.set_printoptions(edgeitems=5)\n",
    "    print(\"model params - decoder params - conv1\", sum(p.numel() for p in model.parameters()), sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "    for epoch in range(1, 2):\n",
    "        print(f\"\\n----------------epoch {epoch} ---------------\")\n",
    "        \n",
    "        train(epoch, train_data_loader, mask_language, test_data_loader)\n",
    "        if not early_stopping.early_stop:\n",
    "            train(epoch, train_data_loader, mask_language, test_data_loader)\n",
    "        if not early_stopping.early_stop:\n",
    "            train(epoch, grc_data_loader, mask_language, test_data_loader)\n",
    "        if not early_stopping.early_stop:\n",
    "            train(epoch, heb_data_loader, mask_language, test_data_loader)\n",
    "        #train(epoch, blinker_data_loader, mask_language, test_data_loader)\n",
    "\n",
    "        if early_stopping:\n",
    "                model.load_state_dict(torch.load(f'/mounts/work/ayyoob/models/gnn/checkpoint/postagging/check_point_{start_time}.pt'))\n",
    "\n",
    "        save_model(model, f'{len(pos_lang_list)}langs-nopersian_posfeat{tag_frequencies}alltargets_transformer{use_transformers}6layresresidual{residual_connection}_trainWE{train_word_embedding}_maskLang{mask_language}_epoch{epoch}_{params}_{start_time}_earlystopping-GA')\n",
    "        test(epoch, test_data_loader, mask_language) \n",
    "        test_mostfreq(yor_data_loader_heb, True, yor_gold_mostfreq_tag, yor_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "        test_mostfreq(tam_data_loader_grc, True, tam_gold_mostfreq_tag, tam_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "        test_mostfreq(arb_data_loader_grc, True, arb_gold_mostfreq_tag, arb_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "        test_mostfreq(por_data_loader_grc, True, por_gold_mostfreq_tag, por_gold_mostfreq_index, (w2v_model.wv.vectors.shape[0], len(postag_map)))\n",
    "\n",
    "        clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len features 11 start time: 20220307-140939\n",
      "model params - decoder params - conv1 291580899 15262225\n",
      "\n",
      "----------------epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24143 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3047425916e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m    , [train_pos_labels_ext, grc_pos_labels_ext, heb_pos_labels_ext, blinker_pos_labels_ext], group_size=1024)\n\u001b[1;32m      4\u001b[0m create_model(gnn_dataset_train_pos_ext, gnn_dataset_grc_pos_ext, gnn_dataset_heb_pos_ext, gnn_dataset_blinker_pos_ext, gnn_dataset_blinker_pos_bigbatch,\n\u001b[0;32m----> 5\u001b[0;31m    train_word_embedding=False, mask_language=True, use_transformers=True, tag_frequencies=True, params=params)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#create_model(gnn_dataset_train_pos, gnn_dataset_grc_pos, gnn_dataset_heb_pos, gnn_dataset_blinker_pos, gnn_dataset_blinker_pos_bigbatch,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-aa6c17480e98>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(train_gnn_dataset, grc_gnn_dataset, heb_gnn_dataset, blinker_gnn_dataset, test_gnn_dataset, tag_frequencies, use_transformers, train_word_embedding, mask_language, residual_connection, params)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n----------------epoch {epoch} ---------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3e755be7d886>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, data_loader, mask_language, test_data_loader, max_batches)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_encoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3e755be7d886>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_language\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Z will be the output of the GNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoded'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/torch_geometric/nn/models/autoencoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34mr\"\"\"Runs the encoder and computes node-wise latent variables.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6bda5ce04029>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/code/POS-TAGGING/my_utils/alignment_features.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, dev)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mONE_HOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOSITION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAPPING\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFLOAT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/code/POS-TAGGING/my_utils/alignment_features.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, poses)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multalign_graph/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = 'traintarget{threshold}_typecheckFalse'\n",
    "gnn_dataset_train_pos_ext, gnn_dataset_grc_pos_ext, gnn_dataset_heb_pos_ext, gnn_dataset_blinker_pos_ext = create_me_a_gnn_dataset_you_stupid([ train_pos_node_cover_ext, grc_pos_node_cover_ext, heb_pos_node_cover_ext, blinker_pos_node_cover_ext]\n",
    "   , [train_pos_labels_ext, grc_pos_labels_ext, heb_pos_labels_ext, blinker_pos_labels_ext], group_size=1024)\n",
    "create_model(gnn_dataset_train_pos_ext, gnn_dataset_grc_pos_ext, gnn_dataset_heb_pos_ext, gnn_dataset_blinker_pos_ext, gnn_dataset_blinker_pos_bigbatch,\n",
    "   train_word_embedding=False, mask_language=True, use_transformers=True, tag_frequencies=True, params=params)\n",
    "\n",
    "#create_model(gnn_dataset_train_pos, gnn_dataset_grc_pos, gnn_dataset_heb_pos, gnn_dataset_blinker_pos, gnn_dataset_blinker_pos_bigbatch,\n",
    "#   train_word_embedding=False, mask_language=True, use_transformers=False, tag_frequencies=True)\n",
    "#model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerTrue6layresresidual_trainWEFalse_maskLangTrue_epoch1_trainYoruba_20220218-223347.pickle')\n",
    "#test(1, blinker_data_loader_bigbatch, True)\n",
    "#importlib.reload(posutil)\n",
    "\n",
    "\n",
    "posutil.generate_target_lang_tags(model, 'yor-x-bible-2010', f'{len(pos_lang_list)}langs-nopersian_posfeatTrueealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_{params}_{start_time}_earlystopping-GA_', True, \n",
    "        train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "        DataEncoder)\n",
    "\n",
    "posutil.generate_target_lang_tags(model, 'por-x-bible-versaointernacional',f'{len(pos_lang_list)}langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_{params}_{start_time}_earlystopping-GA', True, \n",
    "        train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "        DataEncoder)\n",
    " \n",
    "posutil.generate_target_lang_tags(model, 'tam-x-bible-newworld', f'{len(pos_lang_list)}langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_{params}_{start_time}_earlystopping-GA', True, \n",
    "        train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "        DataEncoder)\n",
    "\n",
    "rres = posutil.generate_target_lang_tags(model, 'arb-x-bible', f'{len(pos_lang_list)}langs-nopersian_posfeatTruealltargets_transformerTrue6layresresidualFalse_trainWEFalse_epoch1_{params}_{start_time}_earlystopping-GA', True, \n",
    "        train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "        DataEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'engtest_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-60800c175775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclean_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mengtest_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'engtest_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "decoder = None\n",
    "clean_memory()\n",
    "for item in engtest_data_loader:\n",
    "    if item['verse'][0] == verse:\n",
    "        print('found')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verse = list(eng_gen_data.keys())[0]\n",
    "number_to_tag_map = {postag_map[i]:i for i in postag_map}\n",
    "print(verse)\n",
    "print({item[0]:number_to_tag_map[item[1]] for item in sorted(eng_gen_data[verse], key=lambda x: x[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerTrue_trainWETrue_maskLangFalse_epoch3_20220218-101130.pickle')\n",
    "# test(1, engtest_data_loader, mask_language=False) \n",
    "\n",
    "importlib.reload(posutil)\n",
    "def get_language_node_cover(all_node_cover, target_edition, dataset):\n",
    "    nodes_map = dataset.nodes_map\n",
    "    res = collections.defaultdict(list)\n",
    "\n",
    "    if target_edition in nodes_map:\n",
    "        for verse in all_node_cover:\n",
    "            if verse in nodes_map[target_edition]:\n",
    "                lang_nodes = list(nodes_map[target_edition][verse].values())\n",
    "                for tok in all_node_cover[verse]:\n",
    "                    if tok in lang_nodes:\n",
    "                        res[verse].append(tok)\n",
    "    \n",
    "    return res\n",
    "    \n",
    "\n",
    "def finetune_and_generate_for_target_lang(model_path, target_edition):\n",
    "    global model, criterion, optimizer\n",
    "    \n",
    "    target_train_node_cover = get_language_node_cover(train_pos_node_cover_ext, target_edition, train_dataset)\n",
    "    target_grc_node_cover = get_language_node_cover(grc_pos_node_cover_ext, target_edition, grc_test_dataset)\n",
    "    target_heb_node_cover = get_language_node_cover(heb_pos_node_cover_ext, target_edition, heb_test_dataset)\n",
    "    target_blinker_node_cover = get_language_node_cover(blinker_pos_node_cover_ext, target_edition, blinker_test_dataset)\n",
    "    \n",
    "\n",
    "    train_ds, grc_ds, heb_ds, blinker_ds = create_me_a_gnn_dataset_you_stupid(\n",
    "            [target_train_node_cover, target_grc_node_cover, target_heb_node_cover, target_blinker_node_cover], \n",
    "            [train_pos_labels_ext, grc_pos_labels_ext,  heb_pos_labels_ext, blinker_pos_labels_ext], editions=[target_edition])\n",
    "    \n",
    "    train_data_loader = DataLoader(train_ds, shuffle=True)\n",
    "    grc_data_loader = DataLoader(grc_ds, shuffle=True)\n",
    "    heb_data_loader = DataLoader(heb_ds, shuffle=True)\n",
    "    blinker_data_loader = DataLoader(blinker_ds, shuffle=True)\n",
    "\n",
    "    # model = torch.load(model_path)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    # train(1, train_data_loader, mask_language=False, test_data_loader=blinker_data_loader, max_batches=100)\n",
    "    # posutil.generate_target_lang_tags(model, target_edition, 'posfeatTrue_transformerTrue6layerresidual_trainWEFalse_finetune100', False, \n",
    "    #         train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "    #         DataEncoder)\n",
    "    # model = None\n",
    "    # clean_memory\n",
    "\n",
    "    # model = torch.load(model_path)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    # train(1, train_data_loader, mask_language=False, test_data_loader=blinker_data_loader, max_batches=1000)\n",
    "    posutil.generate_target_lang_tags(model, target_edition, 'posfeatTrue_transformerTrue6layerresidual_trainWEFalse_finetune1000', False, \n",
    "            train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "            DataEncoder)\n",
    "    model = None\n",
    "    clean_memory\n",
    "\n",
    "    model = torch.load(model_path)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    train(1, train_data_loader, mask_language=False, test_data_loader=blinker_data_loader, max_batches=10000)\n",
    "    posutil.generate_target_lang_tags(model, target_edition, 'posfeatTrue_transformerTrue6layerresidual_trainWEFalse_finetune10000', False, \n",
    "            train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "            DataEncoder)\n",
    "    model = None\n",
    "    clean_memory\n",
    "\n",
    "    model = torch.load(model_path)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    train(1, train_data_loader, mask_language=False, test_data_loader=blinker_data_loader)\n",
    "    train(1, grc_data_loader, mask_language=False, test_data_loader=blinker_data_loader)\n",
    "    train(1, heb_data_loader, mask_language=False, test_data_loader=blinker_data_loader)\n",
    "    train(1, blinker_data_loader, mask_language=False, test_data_loader=blinker_data_loader)\n",
    "    posutil.generate_target_lang_tags(model, target_edition, 'posfeatTrue_transformerTrue6layerresidual_trainWEFalse_finetuneALL', False, \n",
    "            train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "            DataEncoder)\n",
    "\n",
    "\n",
    "def generate_target_lang_tags_all_models(target_langs):\n",
    "    global model\n",
    "    model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatFalse_transformerFalse_trainWEFalse_maskLangTrue_20220209-201345.pickle', map_location=torch.device('cpu')).to(dev)\n",
    "    test(0, blinker_data_loader_bigbatch, True)\n",
    "    for lang in target_langs:\n",
    "        posutil.generate_target_lang_tags(lang,  f\"posfeatFalse_transformerFalse_trainWEFalse\", True)\n",
    "\n",
    "# posutil.generate_target_lang_tags_all_models(['yor-x-bible-2010', 'tam-x-bible-newworld', 'fin-x-bible-helfi'])\n",
    "finetune_and_generate_for_target_lang('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerTru6Lresidual_trainWETrue_maskLangFalse_epoch3_20220218-101130.pickle', 'yor-x-bible-2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = sag\n",
    "batch = khar\n",
    "verse = gav\n",
    "print(i, verse)\n",
    "\n",
    "keys = list(gnn_dataset.verse_info.keys())\n",
    "\n",
    "gnn_dataset.verse_info[verse]\n",
    "print(keys)\n",
    "#save_model(model, 'freeze-embedding_noLang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_model = model.to('cpu')\n",
    "#model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerFalse_trainWEFalse_maskLangTrue_20220210-173912.pickle')\n",
    "#torch.cuda.set_device(0)\n",
    "#model.to(dev)\n",
    "#epoch = 0\n",
    "#test_data_loader = DataLoader(gnn_dataset_blinker_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "#test(epoch, test_data_loader, mask_language=True)\n",
    "##yoruba_postags = generate_target_lang_tags('yor-x-bible-2010', f\"posfeatFalse_transformerFalse_trainWEFalse\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importlib.reload(afeatures)\n",
    "#initial_model = model\n",
    "#torch.cuda.set_device(1)\n",
    "#create_model(gnn_dataset_train_pos, gnn_dataset_grc_pos, gnn_dataset_heb_pos, gnn_dataset_blinker_pos, gnn_dataset_blinker_pos_bigbatch, tag_frequencies=True)\n",
    "## yoruba_postags = generate_target_lang_tags('yor-x-bible-2010', f\"posfeatTrue_transformerFalse_trainWEFalse\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(4)\n",
    "gnn_ds_train_pos_ext, gnn_ds_grc_pos_ext, gnn_ds_heb_pos_ext, gnn_ds_blinker_pos_ext = create_me_a_gnn_dataset_you_stupid([train_pos_node_cover_ext, grc_pos_node_cover_ext, heb_pos_node_cover_ext, blinker_pos_node_cover_ext], \n",
    "    [train_pos_labels_ext, grc_pos_labels_ext, heb_pos_labels_ext, blinker_pos_labels_ext])\n",
    "\n",
    "create_model(gnn_ds_train_pos_ext, gnn_ds_grc_pos_ext, gnn_ds_heb_pos_ext, gnn_ds_blinker_pos_ext, gnn_dataset_blinker_pos_bigbatch, tag_frequencies=True\n",
    "    , use_transformers=False, train_word_embedding=True, mask_language=False)\n",
    "\n",
    "# yoruba_postags = generate_target_lang_tags('yor-x-bible-2010', f\"posfeatTrue_transformerTrue_trainWETrue\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(4)\n",
    "model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerTrue_trainWETrue_maskLangFalse_epoch1_20220214-190824.pickle')\n",
    "torch.cuda.set_device(0)\n",
    "model.to(dev)\n",
    "epoch = 0\n",
    "test_data_loader = DataLoader(gnn_dataset_blinker_pos_onlyeng, batch_size=1, shuffle=False)\n",
    "test(epoch, test_data_loader, mask_language=False)\n",
    "yoruba_postags = generate_target_lang_tags('fin-x-bible-helfi', f\"posfeatTrue_transformerTrue_trainWETrue\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#normalized_gold_frequencies, gold_frequencies_all = get_words_tag_frequence(model, 2354770, len(postag_map), english_data_loader, from_gold_data=True)\n",
    "\n",
    "#gold_frequencies_all = gold_frequencies\n",
    "word_frequencies = torch.sum(gold_frequencies_all, dim=1)\n",
    "\n",
    "subjectword_indices =  word_frequencies > 0.1\n",
    "print(word_frequencies.shape)\n",
    "gold_frequencies = gold_frequencies_all[subjectword_indices, :]\n",
    "predicted_frequencies = tag_frequencies_english[subjectword_indices, :]\n",
    "wordtype_frequencies = word_frequencies[subjectword_indices]\n",
    "print(gold_frequencies.shape)\n",
    "\n",
    "_, gold_tags = torch.max(gold_frequencies, dim=1)\n",
    "_, predicted_tags = torch.max(predicted_frequencies, dim=1)\n",
    "\n",
    "sorted_wordtype_frequencies, sort_pattern = torch.sort(wordtype_frequencies, descending=True)\n",
    "\n",
    "sorted_gold_tags = gold_tags[sort_pattern]\n",
    "sorted_predicted_tags = predicted_tags[sort_pattern]\n",
    "quarter_size = int(sorted_gold_tags.size(0)/2.0)\n",
    "\n",
    "print('quarter size', quarter_size)\n",
    "print(\"general accuracy\", torch.sum(gold_tags == predicted_tags)/predicted_tags.size(0))\n",
    "print('first quarter accuracy', torch.sum(sorted_gold_tags[:quarter_size] == sorted_predicted_tags[:quarter_size])/quarter_size)\n",
    "print('last part accuracy', torch.sum(sorted_gold_tags[1*quarter_size:] == sorted_predicted_tags[1*quarter_size:])/sorted_predicted_tags[1*quarter_size:].size(0))\n",
    "\n",
    "print('total token count', torch.sum(wordtype_frequencies))\n",
    "print('first quarter words token count', torch.sum(word_frequencies[:quarter_size]))\n",
    "\n",
    "print('1st frequency', sorted_wordtype_frequencies[0])\n",
    "print('10st frequency', sorted_wordtype_frequencies[10])\n",
    "print('100st frequency', sorted_wordtype_frequencies[100])\n",
    "print('100st frequency', sorted_wordtype_frequencies[736])\n",
    "print('1000st frequency', sorted_wordtype_frequencies[1000])\n",
    "print('10000st frequency', sorted_wordtype_frequencies[10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized_tag_frequencies = torch.softmax(tag_frequencies_copy, dim=1)\n",
    "\n",
    "sm = torch.sum(tag_frequencies_copy, dim=1)\n",
    "normalized_tag_frequencies = (tag_frequencies_copy.transpose(1,0) / sm).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global model, decoder\n",
    "#1/0\n",
    "\n",
    "decoder = None\n",
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = blinker_test_dataset.features[:]\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "from pprint import pprint\n",
    "#print('indim',in_dim)\n",
    "#features[-1].out_dim = 50\n",
    "for i in features:\n",
    "    #if i.type==3:\n",
    "    #    i.out_dim=4\n",
    "    print(vars(i))\n",
    "print(vars(model.encoder.feature_encoder.layers[10].emb))\n",
    "# print(model.encoder.feature_encoder.layers[10].emb)\n",
    "#sum(p.out_dim for p in features)\n",
    "#train_dataset.features.pop()\n",
    "#train_dataset.features[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "#train_dataset.features[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "#train_dataset.features[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "#train_dataset.features[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "#train_dataset.features[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "#train_dataset.features[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "#train_dataset.features[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "#train_dataset.features[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "##train_dataset.features.append(afeatures.MappingFeature(100, 'word'))\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\")\n",
    "#torch.save(train_dataset.features[-3], \"./features.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_map = train_dataset.nodes_map\n",
    "bad_edition_files = []\n",
    "for edit in nodes_map:\n",
    "    bad_count = 0\n",
    "    for verse in nodes_map[edit]:\n",
    "        if len(nodes_map[edit][verse].keys()) < 2:\n",
    "            bad_count += 1\n",
    "        if bad_count > 1:\n",
    "            bad_edition_files.append(edit)\n",
    "            break\n",
    "print(bad_edition_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_japanese_nodes = set()\n",
    "nodes_map = train_dataset.nodes_map\n",
    "\n",
    "for bad_editionf in bad_edition_files:\n",
    "    for verse in nodes_map[bad_editionf]:\n",
    "        for item in nodes_map[bad_editionf][verse].items():\n",
    "            all_japanese_nodes.add(item[1])\n",
    "\n",
    "print(\" all japansese nodes: \", len(all_japanese_nodes))\n",
    "edge_index = train_dataset.edge_index.to('cpu')\n",
    "remaining_edges_index = []\n",
    "for i in tqdm(range(0, edge_index.shape[1], 2)):\n",
    "    if edge_index[0, i].item() not in all_japanese_nodes and edge_index[0, i+1].item() not in all_japanese_nodes:\n",
    "        remaining_edges_index.extend([i, i+1])\n",
    "\n",
    "print('original total edges count', edge_index.shape)\n",
    "print('remaining edge count', len(remaining_edges_index))\n",
    "train_dataset.edge_index = edge_index[:, remaining_edges_index]\n",
    "train_dataset.edge_index.shape\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab640873abb67ad450730b814c8d7de015aa287a6fc3bae4b7154b533e57676"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
