{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, sys\n",
    "sys.path.insert(0, '../')\n",
    "from my_utils import gpu_utils\n",
    "import importlib, gc\n",
    "from my_utils.alignment_features import *\n",
    "import my_utils.alignment_features as afeatures\n",
    "importlib.reload(afeatures)\n",
    "import gnn_utils.graph_utils as gutils\n",
    "import postag_utils as posutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "dev2 = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_utils import align_utils as autils, utils\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "config_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc-ui-demo/config_pbc.ini\"\n",
    "utils.setup(config_file)\n",
    "\n",
    "params = argparse.Namespace()\n",
    "\n",
    "\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses = list(pros.keys())\n",
    "params.gold_file = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_train.txt\"\n",
    "pros, surs = autils.load_gold(params.gold_file)\n",
    "all_verses.extend(list(pros.keys()))\n",
    "all_verses = list(set(all_verses))\n",
    "print(len(all_verses))\n",
    "\n",
    "params.editions_file =  \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi_lang_list.txt\"\n",
    "editions, langs = autils.load_simalign_editions(params.editions_file)\n",
    "current_editions = [editions[lang] for lang in langs]\n",
    "\n",
    "def get_pruned_verse_alignments(args):\n",
    "    verse, current_editions = args\n",
    "    \n",
    "    verse_aligns_inter = autils.get_verse_alignments(verse)\n",
    "    verse_aligns_gdfa = autils.get_verse_alignments(verse, gdfa=True)\n",
    "\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_inter, current_editions)\n",
    "    autils.prune_non_necessary_alignments(verse_aligns_gdfa, current_editions)\n",
    "\n",
    "    gc.collect()\n",
    "    return verse_aligns_inter, verse_aligns_gdfa\n",
    "    \n",
    "\n",
    "args = []\n",
    "for i,verse in enumerate(all_verses):\n",
    "    args.append((verse, current_editions[:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(afeatures)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features, n_head = 2, has_tagfreq_feature=False,):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = pyg_nn.GATConv(in_channels, 2*out_channels, heads= n_head)\n",
    "        self.conv2 = pyg_nn.GATConv(2 * n_head *  out_channels , out_channels, heads= 1)\n",
    "        #self.fin_lin = nn.Linear(out_channels, out_channels)\n",
    "        \n",
    "        if has_tagfreq_feature:\n",
    "            self.feature_encoder = afeatures.FeatureEncoding(features, [normalized_tag_frequencies, word_vectors])\n",
    "            #self.feature_encoder = afeatures.FeatureEncoding(features, [normalized_tag_frequencies,train_pos_labels, word_vectors])\n",
    "        else:\n",
    "            self.feature_encoder = afeatures.FeatureEncoding(features, [word_vectors])\n",
    "            #self.feature_encoder = afeatures.FeatureEncoding(features, [train_pos_labels, word_vectors])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        encoded = self.feature_encoder(x, dev)\n",
    "        x = F.elu(self.conv1(encoded, edge_index, ))\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        #return F.relu(self.fin_lin(x)), encoded\n",
    "        return x, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_encoders_embedding(encoder):\n",
    "    for i,ft in enumerate(encoder.feature_types):\n",
    "        if ft.type == MAPPING:\n",
    "            print('doing it')\n",
    "            encoder.layers[i] = afeatures.MappingEncoding(encoder.layers[i].emb.weight, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class DataEncoder():\n",
    "\n",
    "    def __init__(self, data_loader, model, mask_language):\n",
    "        self.data_loader = data_loader\n",
    "        self.model = model\n",
    "        self.mask_language = mask_language\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i,batch in enumerate(tqdm(self.data_loader)):\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "            except NameError:\n",
    "                # it is ok since if we call test before train, optimizer is not defined\n",
    "                pass\n",
    "\n",
    "            x = batch['x'][0].to(dev)  # initial features (not encoded)\n",
    "            edge_index = batch['edge_index'][0].to(dev) \n",
    "            verse = batch['verse'][0]\n",
    "\n",
    "            if verse in masked_verses:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if self.mask_language:\n",
    "                    x[:, 0] = 0\n",
    "                z, encoded = self.model.encode(x, edge_index) # Z will be the output of the GNN\n",
    "                batch['encoded'] = encoded\n",
    "            except Exception as e:\n",
    "                global sag, khar, gav\n",
    "                sag, khar, gav =  (i, batch, verse)\n",
    "                print(e)\n",
    "                1/0\n",
    "            \n",
    "            yield z, verse, i, batch\n",
    "\n",
    "def train(epoch, data_loader, mask_language, test_data_loader, max_batches=999999999):\n",
    "    global optimizer\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    loss_multi_round = 0\n",
    "\n",
    "    data_encoder = DataEncoder(data_loader, model, mask_language)\n",
    "\n",
    "    for z, verse, i, batch in data_encoder:\n",
    "        \n",
    "        target = batch['pos_classes'][0].to(dev)\n",
    "        _, labels = torch.max(target, 1)\n",
    "        \n",
    "        index = batch['pos_index'][0].to(dev)\n",
    "\n",
    "        preds = model.decoder(z, index, batch)\n",
    "\n",
    "        # print(preds.shape, labels.shape)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss = loss * target.shape[0] # TODO check if this is necessary\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 1 == 0: # Gradient accumulation\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            print(f\"loss: {total_loss}\")\n",
    "            total_loss = 0\n",
    "            test(epoch, test_data_loader, mask_language)\n",
    "            model.train()\n",
    "            clean_memory()\n",
    "\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    \n",
    "    print(f\"total train loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class, drop_out=0):\n",
    "        super(POSDecoder, self).__init__()\n",
    "\n",
    "        self.transfer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                        nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out),\n",
    "                        nn.Linear(hidden_size, n_class))\n",
    "\n",
    "    def forward(self, z, index, batch=None):\n",
    "        h = z[index, :]\n",
    "\n",
    "        res = self.transfer(h)\n",
    "\n",
    "        return res\n",
    "\n",
    "class POSDecoderTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class, drop_out=0):\n",
    "        super(POSDecoderTransformer, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=8, dim_feedforward=hidden_size)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "\n",
    "        self.transfer = nn.Sequential( nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Dropout(drop_out), # TODO check what happens if I remove this.\n",
    "                        nn.Linear(hidden_size, n_class))\n",
    "\n",
    "    def forward(self, z_, index, batch_):\n",
    "        z = z_.to(dev2)\n",
    "\n",
    "        x = F.pad(batch_['encoded'], (0, z.size(1) - batch_['encoded'].size(1))).to(dev2)\n",
    "\n",
    "\n",
    "        language_based_nodes = batch_['lang_based_nodes'] # determines which node belongs to which language\n",
    "        transformer_indices = batch_['transformer_indices'] # the reverse of the prev structure\n",
    "\n",
    "        sentences = []\n",
    "        for lang_nodes in language_based_nodes: # we rearrange the nodes into sentences of each language\n",
    "            tensor = z[lang_nodes, :] + x[lang_nodes, :]\n",
    "            tensor = F.pad(tensor, (0, 0, 0, 150 - tensor.size(0)))\n",
    "            sentences.append(tensor)\n",
    "        \n",
    "        batch = torch.stack(sentences) # A batch contains all translations of one sentence in all training languages.\n",
    "        batch = torch.transpose(batch, 0, 1)\n",
    "\n",
    "        h = self.transformer(batch)\n",
    "        h = torch.transpose(h, 0, 1)\n",
    "        h = h[transformer_indices[0], transformer_indices[1], :] # rearrange the nodes back to the order in which we recieved (the order that represents the graph)\n",
    "\n",
    "        res = self.transfer(h)\n",
    "\n",
    "        return res.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, testloader, mask_language, filter_wordtypes=None):\n",
    "    print('testing',  epoch)\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    data_encoder = DataEncoder(testloader, model, mask_language)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for z, verse, i, batch in data_encoder:\n",
    "            \n",
    "            target = batch['pos_classes'][0].to(dev)\n",
    "            index = batch['pos_index'][0].to(dev)\n",
    "            \n",
    "            if filter_wordtypes != None:\n",
    "                non_filtered_words = filter_wordtypes[batch['x'][0][:, 9].long()] == 1\n",
    "                non_filtered_words = non_filtered_words[index]\n",
    "                index = index[non_filtered_words]\n",
    "\n",
    "                target = target[non_filtered_words, :]\n",
    "\n",
    "            preds = model.decoder(z, index, batch)\n",
    "            \n",
    "            if preds.size(0) > 0:\n",
    "                _, predicted = torch.max(preds, 1)\n",
    "                _, labels = torch.max(target, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'test, epoch: {epoch}, total:{total} ACC: {correct/total}')\n",
    "    clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting_test(data_loader1, data_loader2):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i,(batch, batch2) in enumerate(tqdm(zip(data_loader1, data_loader2))) :\n",
    "            \n",
    "        x = batch['x'][0]\n",
    "        edge_index = batch['edge_index'][0]\n",
    "        verse = batch['verse'][0]\n",
    "\n",
    "        if verse in masked_verses:\n",
    "            continue\n",
    "\n",
    "        target = batch['pos_classes'][0]\n",
    "        index = batch['pos_index'][0]\n",
    "\n",
    "        index2 = batch2['pos_index'][0]\n",
    "        \n",
    "\n",
    "        for node, label in zip(index,target):\n",
    "            other_side = edge_index[1, edge_index[0, :] == node]\n",
    "            other_side_withpos = other_side[[True if i in index2 else False for i in other_side]]\n",
    "            other_side_target_indices = [(i == index2).nonzero(as_tuple=True)[0].item() for i in other_side_withpos]\n",
    "            #print(other_side_target_indices)\n",
    "            proj_tags = batch2['pos_classes'][0][other_side_target_indices]\n",
    "\n",
    "            if proj_tags.size(0) > 0:\n",
    "                _, proj_tags = torch.max(proj_tags, 1)\n",
    "                #print(target.shape, node, index.shape, proj_tags, other_side)\n",
    "                \n",
    "                if torch.argmax(label) == torch.mode(proj_tags)[0]:\n",
    "                    correct += 1\n",
    "                \n",
    "                total += 1\n",
    "\n",
    "    print(f'test, , total:{total} ACC: {correct/total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wordtype_frequencies[736])\n",
    "#print(wordtype_frequencies[1473])\n",
    "#print(wordtype_frequencies[3683])\n",
    "#print(wordtype_frequencies[7367])\n",
    "#print(wordtype_frequencies[14733])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequent_words = torch.zeros(word_frequencies.size(0))\n",
    "#frequent_words[word_frequencies > -1] = 1\n",
    "\n",
    "#test(1, test_data_loader, filter_wordtypes=frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"/mounts/work/ayyoob/models/w2v/word2vec_helfi_langs_15e.model\")\n",
    "\n",
    "print(w2v_model.wv.vectors.shape)\n",
    "word_vectors = torch.from_numpy(w2v_model.wv.vectors).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "test_verses = all_verses[:] \n",
    "editf1 = 'fin-x-bible-helfi'\n",
    "editf2 = \"heb-x-bible-helfi\"\n",
    "\n",
    "\n",
    "if 'jpn-x-bible-newworld' in  current_editions[:]:\n",
    "     current_editions.remove('jpn-x-bible-newworld')\n",
    "if 'grc-x-bible-unaccented' in  current_editions[:]:\n",
    "     current_editions.remove('grc-x-bible-unaccented')\n",
    "\n",
    "\n",
    "\n",
    "data_dir_train = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_train_community_word\"\n",
    "data_dir_blinker = \"/mounts/data/proj/ayyoob/align_induction/dataset/pruned_alignments_blinker_inter/\"\n",
    "data_dir_grc = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_grc_community_word/\"\n",
    "data_dir_heb = \"/mounts/data/proj/ayyoob/align_induction/dataset/dataset_helfi_heb_community_word/\"\n",
    "\n",
    "train_dataset = torch.load(f\"{data_dir_train}/train_dataset_nox_noedge.torch.bin\")\n",
    "blinker_test_dataset = torch.load(f\"{data_dir_blinker}/train_dataset_nox_noedge.torch.bin\")\n",
    "grc_test_dataset = torch.load(f\"{data_dir_grc}/train_dataset_nox_noedge.torch.bin\")\n",
    "heb_test_dataset = torch.load(f\"{data_dir_heb}/train_dataset_nox_noedge.torch.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "\n",
    "postag_map = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "\n",
    "pos_lang_list = [\"eng-x-bible-mixed\", \"deu-x-bible-newworld\", \"ces-x-bible-newworld\", \n",
    "\t\t#\"prs-x-bible-goodnews\", \"hin-x-bible-newworld\", \"ron-x-bible-2006\",\n",
    "\t\t'dan-x-bible-newworld', 'fin-x-bible-helfi', 'nld-x-bible-newworld', 'pol-x-bible-newworld', 'swe-x-bible-newworld',\n",
    "\t\t\"ita-x-bible-2009\", \"fra-x-bible-louissegond\", \"spa-x-bible-newworld\"]\n",
    "\n",
    "def get_db_nodecount(dataset):\n",
    "\tres = 0\n",
    "\tfor lang in dataset.nodes_map.values():\n",
    "\t\tfor verse in lang.values():\n",
    "\t\t\tres += len(verse)\n",
    "\t\n",
    "\treturn res\n",
    "\n",
    "def get_language_nodes(dataset, lang_list, sentences):\n",
    "\tnode_count = get_db_nodecount(dataset)\n",
    "\tpos_labels = torch.zeros(node_count, len(postag_map))\n",
    "\n",
    "\tpos_node_cover = collections.defaultdict(list)\n",
    "\tfor lang in lang_list:\n",
    "\t\tif lang in dataset.nodes_map:\n",
    "\t\t\tfor sentence in sentences:\n",
    "\t\t\t\tif sentence in dataset.nodes_map[lang]:\n",
    "\t\t\t\t\tfor tok in dataset.nodes_map[lang][sentence]:\n",
    "\t\t\t\t\t\tpos_node_cover[sentence].append(dataset.nodes_map[lang][sentence][tok])\n",
    "\t\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\n",
    "\n",
    "def get_pos_tags(dataset, pos_lang_list):\n",
    "\tall_tags = {}\n",
    "\tfor lang in pos_lang_list:\n",
    "\t\tif lang not in dataset.nodes_map:\n",
    "\t\t\tcontinue\n",
    "\t\tall_tags[lang] = {}\n",
    "\t\tbase_path = ''\n",
    "\t\tif os.path.exists(F\"/mounts/work/mjalili/projects/gnn-align/data/pbc_pos_tags/{lang}.conllu\"):\n",
    "\t\t\tbase_path = F\"/mounts/work/mjalili/projects/gnn-align/data/pbc_pos_tags/\"\n",
    "\t\telse:\n",
    "\t\t\tbase_path = F\"/mounts/work/silvia/POS/TAGGED_LANGS/\"\n",
    "\t\twith codecs.open(F\"{base_path}{lang}.conllu\", \"r\", \"utf-8\") as lang_pos:\n",
    "\t\t\ttag_sent = []\n",
    "\t\t\tsent_id = \"\"\n",
    "\t\t\tfor sline in lang_pos:\n",
    "\t\t\t\tsline = sline.strip()\n",
    "\t\t\t\tif sline == \"\":\n",
    "\t\t\t\t\tif sent_id not in dataset.nodes_map[lang]:\n",
    "\t\t\t\t\t\ttag_sent = []\n",
    "\t\t\t\t\t\tsent_id = \"\"\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tall_tags[lang][sent_id] = [p[3] for p in tag_sent]\n",
    "\t\t\t\t\ttag_sent = []\n",
    "\t\t\t\t\tsent_id = \"\"\n",
    "\t\t\t\telif \"# verse_id\" in sline:\n",
    "\t\t\t\t\tsent_id = sline.split()[-1]\n",
    "\t\t\t\telif sline[0] == \"#\":\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttag_sent.append(sline.split(\"\\t\"))\n",
    "\n",
    "\tnode_count = get_db_nodecount(dataset)\n",
    "\tpos_labels = torch.zeros(node_count, len(postag_map))\n",
    "\tpos_node_cover = collections.defaultdict(list)\n",
    "\n",
    "\tfor lang in all_tags:\n",
    "\t\tfor sent_id in all_tags[lang]:\n",
    "\t\t\tsent_tags = all_tags[lang][sent_id]\n",
    "\t\t\tfor w_i in range(len(sent_tags)):\n",
    "\t\t\t\tif w_i not in dataset.nodes_map[lang][sent_id]:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tpos_labels[dataset.nodes_map[lang][sent_id][w_i], postag_map[sent_tags[w_i]]] = 1\n",
    "\t\t\t\tpos_node_cover[sent_id].append(dataset.nodes_map[lang][sent_id][w_i])\n",
    "\n",
    "\treturn pos_labels, pos_node_cover\n",
    "\t#pos_pickle = {\"pos_labels\": pos_labels, \"node_ids_train\": pos_ids_train, \"node_ids_dev\": pos_ids_dev}\n",
    "\t#torch.save(pos_pickle, '/mounts/work/ayyoob/models/gnn/postag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blinker_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_blinker_full_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf12 = \"eng-x-bible-mixed\"\n",
    "editf22 = 'fra-x-bible-louissegond'\n",
    "\n",
    "test_gold_eng_fra = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/eng_fra_pbc/eng-fra.gold\"\n",
    "\n",
    "pros_blinker, surs_blinker = autils.load_gold(test_gold_eng_fra)\n",
    "\n",
    "blinker_verse_alignments_inter = {}\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in blinker_test_dataset.nodes_map:\n",
    "    for verse in blinker_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in blinker_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = blinker_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "blinker_verses = [item[0] for item in sorted_verses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(afeatures)\n",
    "#grc_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_grc_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "editf_fin = \"fin-x-bible-helfi\"\n",
    "editf_grc = 'grc-x-bible-helfi'\n",
    "\n",
    "test_gold_grc = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-grc-gold-alignments_test.txt\"\n",
    "\n",
    "pros_grc, surs_grc = autils.load_gold(test_gold_grc)\n",
    "\n",
    "grc_test_verse_alignments_inter = {}\n",
    "grc_test_verse_alignments_gdfa = {}\n",
    "gc.collect()\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in grc_test_dataset.nodes_map:\n",
    "    for verse in grc_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in grc_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = grc_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "grc_test_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heb_test_dataset = torch.load(\"/mounts/work/ayyoob/models/gnn/dataset_helfi_heb_test_community_word.pickle\", map_location=torch.device('cpu'))\n",
    "\n",
    "test_gold_heb = \"/mounts/Users/student/ayyoob/Dokumente/code/pbc_utils/data/helfi/splits/helfi-fin-heb-gold-alignments_test.txt\"\n",
    "\n",
    "pros_heb, surs_heb = autils.load_gold(test_gold_heb)\n",
    "\n",
    "verses_map = {}\n",
    "\n",
    "for edit in heb_test_dataset.nodes_map:\n",
    "    for verse in heb_test_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in heb_test_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = heb_test_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "heb_test_verses = [item[0] for item in sorted_verses]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_map = {}\n",
    "\n",
    "for edit in train_dataset.nodes_map:\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        if verse not in verses_map:\n",
    "            for tok in train_dataset.nodes_map[edit][verse]:\n",
    "                verses_map[verse] = train_dataset.nodes_map[edit][verse][tok]\n",
    "                break\n",
    "\n",
    "sorted_verses = sorted(verses_map.items(), key = lambda x: x[1])\n",
    "all_verses = [item[0] for item in sorted_verses]\n",
    "\n",
    "long_verses = set()\n",
    "\n",
    "for edit in train_dataset.nodes_map.keys():\n",
    "    for verse in train_dataset.nodes_map[edit]:\n",
    "        to_print = False\n",
    "        for tok in train_dataset.nodes_map[edit][verse]:\n",
    "            if tok > 150:\n",
    "                to_print = True\n",
    "        if to_print == True:\n",
    "            long_verses.add(verse)\n",
    "\n",
    "\n",
    "train_verses = all_verses[:]\n",
    "\n",
    "masked_verses = list(long_verses)\n",
    "#masked_verses.extend(blinker_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "class POSTAGGNNDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, verses, edit_files, alignments, node_cover, pos_labels, data_dir, create_data=False, group_size = 20):\n",
    "        self.node_cover = node_cover\n",
    "        self.pos_labels = pos_labels\n",
    "        self.data_dir = data_dir\n",
    "        self.items = self.calculate_size(verses, group_size, node_cover)\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if create_data:\n",
    "            self.calculate_verse_stats(verses, edit_files, alignments, dataset, data_dir)            \n",
    "        \n",
    "    def calculate_size(self, verses, group_size, node_cover):\n",
    "        res = []\n",
    "        for verse in verses:\n",
    "            covered_nodes = node_cover[verse]\n",
    "            random.shuffle(covered_nodes)\n",
    "            items = [covered_nodes[i:i + group_size] for i in range(0, len(covered_nodes), group_size)]\n",
    "            res.extend([(verse, i) for i in items])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def calculate_verse_stats(self,verses, edition_files, alignments, dataset, data_dir):\n",
    "\n",
    "        min_edge = 0\n",
    "        for verse in tqdm(verses):\n",
    "            min_nodes = 99999999999999\n",
    "            max_nodes = 0\n",
    "            edges_tmp = [[],[]]\n",
    "            x_tmp = []\n",
    "            features = []\n",
    "            for i,editf1 in enumerate(edition_files):\n",
    "                for j,editf2 in enumerate(edition_files[i+1:]):\n",
    "                    aligns = autils.get_aligns(editf1, editf2, alignments[verse])\n",
    "                    if aligns != None:\n",
    "                        for align in aligns:\n",
    "                            try:\n",
    "                                n1,_ = gutils.node_nom(verse, editf1, align[0], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                n2,_ = gutils.node_nom(verse, editf2, align[1], None, dataset.nodes_map, x_tmp, edition_files, features)\n",
    "                                edges_tmp[0].extend([n1, n2])\n",
    "\n",
    "                                max_nodes = max(n1, n2, max_nodes)\n",
    "                                min_nodes = min(n1, n2, min_nodes)\n",
    "                            except Exception as e:\n",
    "                                print(editf1, editf2, verse)\n",
    "                                raise(e)\n",
    "\n",
    "            self.verse_info = {}\n",
    "\n",
    "            self.verse_info['padding'] = min_nodes\n",
    "            \n",
    "            self.verse_info['x'] = torch.clone(dataset.x[min_nodes:max_nodes+1,:])\n",
    "            \n",
    "            self.verse_info['edge_index'] = torch.clone(dataset.edge_index[:, min_edge : min_edge + len(edges_tmp[0])] - min_nodes)\n",
    "\n",
    "            if torch.min(self.verse_info['edge_index']) != 0:\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info['edge_index']))\n",
    "            \n",
    "            if self.verse_info['x'].shape[0] != torch.max(self.verse_info['edge_index']) + 1 :\n",
    "                print(verse, min_nodes, max_nodes, min_edge, len(edges_tmp[0]))\n",
    "                print(torch.min(self.verse_info['edge_index']))\n",
    "            \n",
    "            min_edge = min_edge + len(edges_tmp[0])\n",
    "\n",
    "            torch.save(self.verse_info, f\"{data_dir}/verses/{verse}_info.torch.bin\")\n",
    "        \n",
    "        dataset.x = None\n",
    "        dataset.edge_index = None\n",
    "        torch.save(dataset, f\"{data_dir}/train_dataset_nox_noedge.torch.bin\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        verse, nodes = self.items[idx]\n",
    "        \n",
    "        self.verse_info = {verse: torch.load(f'{self.data_dir}/verses/{verse}_info.torch.bin')}\n",
    "\n",
    "\n",
    "        word_number = self.verse_info[verse]['x'][:, 9]\n",
    "        padding = self.verse_info[verse]['padding']\n",
    "        \n",
    "        language_based_nodes, transformer_indices = posutil.get_language_based_nodes(self.dataset.nodes_map, verse, nodes, padding)\n",
    "\n",
    "        ## # Add POSTAG to set of features\n",
    "        #postags = self.pos_labels[padding: self.verse_info[verse]['x'].size(0) + padding, : ]\n",
    "        #postags = postags.detach().clone()\n",
    "        #postags[torch.LongTensor(nodes) - padding, :] = 0\n",
    "        #self.verse_info[verse]['x'] = torch.cat((self.verse_info[verse]['x'], postags), dim=1)\n",
    "\n",
    "        # Add token id as a feature, used to extract token information (like token's tag distribution)\n",
    "        word_number = torch.unsqueeze(word_number, 1)\n",
    "        self.verse_info[verse]['x'] = torch.cat((self.verse_info[verse]['x'], word_number), dim=1)\n",
    "\n",
    "        return {'verse':verse, 'x':self.verse_info[verse]['x'], 'edge_index':self.verse_info[verse]['edge_index'], \n",
    "                'pos_classes': self.pos_labels[nodes, :], 'pos_index': torch.LongTensor(nodes) - padding, \n",
    "                'padding': padding, 'lang_based_nodes': language_based_nodes, 'transformer_indices': transformer_indices}\n",
    "\n",
    "def create_me_a_gnn_dataset_you_stupid(node_covers, labels, group_size=100, editions=current_editions):\n",
    "\n",
    "    train_ds = POSTAGGNNDataset(train_dataset, train_verses, editions, {}, node_covers[0], labels[0], data_dir_train, group_size=group_size)\n",
    "    grc_ds = POSTAGGNNDataset(grc_test_dataset, grc_test_verses, editions, {}, node_covers[1], labels[1], data_dir_grc, group_size=group_size)\n",
    "    heb_ds = POSTAGGNNDataset(heb_test_dataset, heb_test_verses, editions, {}, node_covers[2], labels[2], data_dir_heb, group_size=group_size)\n",
    "    blinker_ds = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, editions, {}, node_covers[3], labels[3], data_dir_blinker, group_size=group_size)\n",
    "\n",
    "    return train_ds, grc_ds, heb_ds, blinker_ds\n",
    "\n",
    "if \"eng-x-bible-mixed\" in pos_lang_list:\n",
    "    pos_lang_list.remove(\"eng-x-bible-mixed\")\n",
    "train_pos_labels, train_pos_node_cover = get_pos_tags(train_dataset, pos_lang_list)\n",
    "#torch.save({'pos_labels':train_pos_labels, 'pos_node_cover':train_pos_node_cover}, f'{data_dir_train}/pos_data.torch.bin')\n",
    "#pos_data = torch.load(f'{data_dir_train}/pos_data.torch.bin')\n",
    "#train_pos_labels, train_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "\n",
    "blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, pos_lang_list)\n",
    "##torch.save({'pos_labels':blinker_pos_labels, 'pos_node_cover': blinker_pos_node_cover}, f'{data_dir_blinker}/pos_data.torch.bin')\n",
    "#pos_data = torch.load(f'{data_dir_blinker}/pos_data.torch.bin')\n",
    "#blinker_pos_labels, blinker_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "\n",
    "grc_pos_labels, grc_pos_node_cover = get_pos_tags(grc_test_dataset, pos_lang_list)\n",
    "#torch.save({'pos_labels':grc_pos_labels, 'pos_node_cover': grc_pos_node_cover}, f'{data_dir_grc}/pos_data.torch.bin')\n",
    "#pos_data = torch.load(f'{data_dir_grc}/pos_data.torch.bin')\n",
    "#grc_pos_labels, grc_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "\n",
    "heb_pos_labels, heb_pos_node_cover = get_pos_tags(heb_test_dataset, pos_lang_list)\n",
    "#torch.save({'pos_labels':heb_pos_labels, 'pos_node_cover': heb_pos_node_cover}, f'{data_dir_heb}/pos_data.torch.bin')\n",
    "#pos_data = torch.load(f'{data_dir_heb}/pos_data.torch.bin')\n",
    "#heb_pos_labels, heb_pos_node_cover = pos_data['pos_labels'], pos_data['pos_node_cover']\n",
    "\n",
    "gnn_dataset_train_pos, gnn_dataset_grc_pos, gnn_dataset_heb_pos, gnn_dataset_blinker_pos = create_me_a_gnn_dataset_you_stupid(\n",
    "    [train_pos_node_cover, grc_pos_node_cover, heb_pos_node_cover, blinker_pos_node_cover], [train_pos_labels, grc_pos_labels, heb_pos_labels, blinker_pos_labels], group_size=128)\n",
    "\n",
    "gnn_dataset_train_pos_bigbatch, gnn_dataset_grc_pos_bigbatch, gnn_dataset_heb_pos_bigbatch, gnn_dataset_blinker_pos_bigbatch = create_me_a_gnn_dataset_you_stupid(\n",
    "    [train_pos_node_cover, grc_pos_node_cover, heb_pos_node_cover, blinker_pos_node_cover], [train_pos_labels, grc_pos_labels, heb_pos_labels, blinker_pos_labels], group_size=10000)\n",
    "\n",
    "train_data_loader_bigbatch = DataLoader(gnn_dataset_train_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "grc_data_loader_bigbatch = DataLoader(gnn_dataset_grc_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "heb_data_loader_bigbatch = DataLoader(gnn_dataset_heb_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "blinker_data_loader_bigbatch = DataLoader(gnn_dataset_blinker_pos_bigbatch, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loadrs_for_target_editions(target_editions, dataset, pos_node_cover, verses, data_dir):\n",
    "    target_pos_labels, target_pos_node_cover = get_language_nodes(dataset, target_editions, pos_node_cover.keys())\n",
    "    gnn_dataset_target_pos = POSTAGGNNDataset(dataset, verses, None, {}, target_pos_node_cover, target_pos_labels, data_dir, group_size = 50000)\n",
    "    target_data_loader = DataLoader(gnn_dataset_target_pos, batch_size=1, shuffle=False)\n",
    "    \n",
    "    return target_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_eng_langs = pos_lang_list[:]\n",
    "#no_eng_langs.remove('eng-x-bible-mixed')\n",
    "\n",
    "## # train_pos_labels, train_pos_node_cover = get_pos_tags(train_dataset, no_eng_langs)\n",
    "## # gnn_dataset_train_pos = POSTAGGNNDataset(train_dataset, train_verses, current_editions, verse_alignments_inter,\n",
    "## #                        train_pos_node_cover, train_pos_labels, data_dir_train, group_size = 10)\n",
    "\n",
    "\n",
    "#_, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, ['eng-x-bible-mixed'])\n",
    "#blinker_pos_labels, _ = get_pos_tags(blinker_test_dataset, pos_lang_list)\n",
    "#gnn_dataset_blinker_pos_onlyeng = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                            blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    #model.encoder.feature_encoder.feature_types[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "    #model.encoder.feature_encoder.feature_types[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "    #model.encoder.feature_encoder.feature_types[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "    #model.encoder.feature_encoder.feature_types[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "    #model.encoder.feature_encoder.feature_types[8] = afeatures.OneHotFeature(32, 250, 'community_2')\n",
    "    #model.encoder.feature_encoder.feature_types[9] = afeatures.MappingFeature(100, 'word')\n",
    "    #model.encoder.feature_encoder.feature_types[10] = afeatures.MappingFeature(len(postag_map), 'tag_priors', freeze=True)\n",
    "    torch.save(model, f'/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_{name}_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_test_pos_labels, eng_test_pos_node_cover = get_pos_tags(blinker_test_dataset, ['eng-x-bible-mixed'])\n",
    "gnn_dataset_engtest_pos = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, {},\n",
    "                      eng_test_pos_node_cover, eng_test_pos_labels, data_dir_blinker, group_size = 500)\n",
    "engtest_data_loader = DataLoader(gnn_dataset_engtest_pos, batch_size=1, shuffle=False)\n",
    "\n",
    "#test(1, engtest_data_loader, mask_language=False) \n",
    "\n",
    "finetune_pos_labels, finetune_pos_node_cover = get_pos_tags(train_dataset, ['eng-x-bible-mixed'])\n",
    "gnn_dataset_finetune_pos = POSTAGGNNDataset(train_dataset, train_verses, current_editions, {},\n",
    "                      finetune_pos_node_cover, finetune_pos_labels, data_dir_train, group_size = 100)\n",
    "finetune_data_loader = DataLoader(gnn_dataset_finetune_pos, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "#train(1, finetune_data_loader, mask_language=False)\n",
    "#test(1, engtest_data_loader, mask_language=False) \n",
    "\n",
    "# blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, ['eng-x-bible-mixed'])\n",
    "# gnn_dataset_blinker_pos = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                              blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 10000)\n",
    "\n",
    "# blinker_pos_labels, blinker_pos_node_cover = get_pos_tags(blinker_test_dataset, no_eng_langs)\n",
    "# gnn_dataset_blinker_pos_majvoting_test = POSTAGGNNDataset(blinker_test_dataset, blinker_verses, current_editions, blinker_verse_alignments_inter,\n",
    "#                              blinker_pos_node_cover, blinker_pos_labels, data_dir_blinker, group_size = 10000)\n",
    "\n",
    "# test_data_loader = DataLoader(gnn_dataset_blinker_pos, batch_size=1, shuffle=False)\n",
    "# test_data_loader_majvoting = DataLoader(gnn_dataset_blinker_pos_majvoting_test, batch_size=1, shuffle=False)\n",
    "# majority_voting_test(test_data_loader, test_data_loader_majvoting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(posutil)\n",
    "#target_editions = ['yor-x-bible-2010']\n",
    "target_editions = [\"eng-x-bible-mixed\"]\n",
    "\n",
    "for edition in current_editions:\n",
    "    if edition not in pos_lang_list:\n",
    "        target_editions.append(edition)\n",
    "\n",
    "\n",
    "model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_11lang_posfeatTruealltargets_transformerTrue6layresresidual_trainWEFalse_maskLangTrue_epoch1_noEng_20220227-015430.pickle', map_location=torch.device('cpu'))\n",
    "model.to(dev)\n",
    "model.decoder.to(dev2)\n",
    "test(1, engtest_data_loader, True) \n",
    "\n",
    "target_data_loader_train = get_data_loadrs_for_target_editions(target_editions, train_dataset, train_pos_node_cover, train_verses, data_dir_train)\n",
    "target_data_loader_grc = get_data_loadrs_for_target_editions(target_editions, grc_test_dataset, grc_pos_node_cover, grc_test_verses, data_dir_grc)\n",
    "target_data_loader_heb = get_data_loadrs_for_target_editions(target_editions, heb_test_dataset, heb_pos_node_cover, heb_test_verses, data_dir_heb)\n",
    "target_data_loader_blinker = get_data_loadrs_for_target_editions(target_editions, blinker_test_dataset, blinker_pos_node_cover, blinker_verses, data_dir_blinker)\n",
    "\n",
    "res_ = torch.load(f'/mounts/work/ayyoob/results/gnn_postag/data/11lang-feature_vectors_posfeat{False}_transformer{True}_trainWE{False}_maskLang{True}_epoch{1}_Englishandallothertargets_typecheckFalse.torch.bin')\n",
    "tag_frequencies, tag_frequencies_target, train_pos_node_cover_ext, train_pos_labels_ext, grc_pos_node_cover_ext, grc_pos_labels_ext, heb_pos_node_cover_ext, heb_pos_labels_ext, blinker_pos_node_cover_ext, blinker_pos_labels_ext = res_\n",
    "res_ = posutil.get_tag_frequencies_node_tags(model, target_editions, train_pos_node_cover, train_pos_labels, grc_pos_node_cover, grc_pos_labels, heb_pos_node_cover, heb_pos_labels, blinker_pos_node_cover, blinker_pos_labels,\n",
    "                                    len(postag_map), target_data_loader_train, target_data_loader_grc, target_data_loader_heb, target_data_loader_blinker,\n",
    "                                    train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch, DataEncoder, target_train_treshold=0.99, type_check=True\n",
    "                                    , source_tag_frequencies=tag_frequencies - tag_frequencies_target, tag_frequencies_target=tag_frequencies_target.to('cpu'))\n",
    "\n",
    "#torch.save(res_, f'/mounts/work/ayyoob/results/gnn_postag/data/11lang-feature_vectors_posfeat{False}_transformer{True}_trainWE{False}_maskLang{True}_epoch{1}_Englishandallothertargets_typecheckFalse.torch.bin')\n",
    "tag_frequencies, tag_frequencies_target, train_pos_node_cover_ext, train_pos_labels_ext, grc_pos_node_cover_ext, grc_pos_labels_ext, heb_pos_node_cover_ext, heb_pos_labels_ext, blinker_pos_node_cover_ext, blinker_pos_labels_ext = res_\n",
    "\n",
    "print(torch.sum(blinker_pos_labels_ext))\n",
    "print(torch.sum(blinker_pos_labels))\n",
    "print(torch.sum(blinker_pos_labels_ext) - torch.sum(blinker_pos_labels))\n",
    "print((torch.sum(blinker_pos_labels_ext) - torch.sum(blinker_pos_labels))/7477)\n",
    "\n",
    "tag_frequencies_source = tag_frequencies - tag_frequencies_target\n",
    "# print(1, torch.sum(tag_frequencies_target))\n",
    "# posutil.keep_only_type_tags(tag_frequencies_target)\n",
    "# print(1, torch.sum(tag_frequencies_target))\n",
    "word_frequencies_target = torch.sum(tag_frequencies_target.to(torch.device('cpu')), dim=1)\n",
    "tag_frequencies = tag_frequencies_source + tag_frequencies_target\n",
    "tag_frequencies_copy = tag_frequencies.detach().clone()\n",
    "\n",
    "tag_frequencies_copy[torch.logical_and(word_frequencies_target>0.1, word_frequencies_target<3), :] = 0.0000001\n",
    "\n",
    "# We have to give uniform noise to some training examples to prevent the model from returning one of the most frequent tags always!!\n",
    "uniform_noise = torch.BoolTensor(tag_frequencies.size(0))\n",
    "uniform_noise[:] = True\n",
    "shuffle_tensor = torch.randperm(tag_frequencies.size(0))[:int(tag_frequencies.size(0)*0.7)]\n",
    "uniform_noise[shuffle_tensor] = False\n",
    "tag_frequencies_copy[torch.logical_and(uniform_noise, word_frequencies_target < 0.1), :] = 0.0000001\n",
    "\n",
    "sm = torch.sum(tag_frequencies_copy, dim=1)\n",
    "normalized_tag_frequencies = (tag_frequencies_copy.transpose(1,0) / sm).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "def create_model(train_gnn_dataset, grc_gnn_dataset, heb_gnn_dataset, blinker_gnn_dataset, test_gnn_dataset,\n",
    "                tag_frequencies=False, use_transformers=False, train_word_embedding=False, mask_language=True):\n",
    "    global model, criterion, optimizer\n",
    "\n",
    "    features = train_dataset.features[:]\n",
    "    #features.append(afeatures.PassFeature(name='posTAG', dim=len(postag_map)))\n",
    "    if tag_frequencies:\n",
    "        features.append(afeatures.MappingFeature(len(postag_map), 'tag_priors', freeze=True))\n",
    "    features[9].freeze = not train_word_embedding\n",
    "    print('len features', len(features))\n",
    "    \n",
    "    train_data_loader = DataLoader(train_gnn_dataset, batch_size=1, shuffle=True)\n",
    "    grc_data_loader = DataLoader(grc_gnn_dataset, batch_size=1, shuffle=True)\n",
    "    heb_data_loader = DataLoader(heb_gnn_dataset, batch_size=1, shuffle=True)\n",
    "    blinker_data_loader = DataLoader(blinker_gnn_dataset, batch_size=1, shuffle=True)\n",
    "    test_data_loader = DataLoader(test_gnn_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    clean_memory()\n",
    "    drop_out = 0.1\n",
    "    n_head = 1\n",
    "    in_dim = sum(t.out_dim for t in features)\n",
    "\n",
    "\n",
    "    channels = 512\n",
    "\n",
    "    decoder_in_dim = n_head * channels \n",
    "\n",
    "    # model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/gnn_512_flggll_word_halfTrain_nofeatlinear_encoderlineear_decoderonelayer20210910-235352-.pickle')\n",
    "    if use_transformers:\n",
    "        decoder = POSDecoderTransformer(decoder_in_dim, decoder_in_dim*2, len(postag_map), drop_out=drop_out).to(dev2)\n",
    "    else:\n",
    "        decoder = POSDecoder(decoder_in_dim, decoder_in_dim*2, len(postag_map))\n",
    "        \n",
    "    model = pyg_nn.GAE(Encoder(in_dim, channels, features, n_head, has_tagfreq_feature=tag_frequencies), decoder).to(dev)\n",
    "\n",
    "\n",
    "    #vgg = models.vgg16()\n",
    "    #summary(vgg, (3, 224, 224))\n",
    "    if use_transformers:\n",
    "        decoder.to(dev2)\n",
    "\n",
    "    # model.to(dev)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "    torch.set_printoptions(edgeitems=5)\n",
    "    print(\"model params - decoder params - conv1\", sum(p.numel() for p in model.parameters()), sum(p.numel() for p in decoder.parameters()))\n",
    "\n",
    "    for epoch in range(1, 2):\n",
    "        print(f\"\\n----------------epoch {epoch} ---------------\")\n",
    "        \n",
    "        train(epoch, train_data_loader, mask_language, engtest_data_loader)\n",
    "        train(epoch, grc_data_loader, mask_language, engtest_data_loader)\n",
    "        train(epoch, heb_data_loader, mask_language, engtest_data_loader)\n",
    "        train(epoch, blinker_data_loader, mask_language, engtest_data_loader)\n",
    "        save_model(model, f'11lang_posfeat{tag_frequencies}alltargets_transformer{use_transformers}6layresresidual_trainWE{train_word_embedding}_maskLang{mask_language}_epoch{epoch}_noEng_reversetypecheck')\n",
    "        test(epoch, test_data_loader, mask_language) \n",
    "        clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, '/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatFalse_transformerFalse6layresresidual_trainWEFalse_maskLangTrue_epoch1_noEng_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatFalse_transformerTrue6layresresidual_trainWEFalse_maskLangTrue_epoch1_noEng_20220222-141130.pickle', map_location=torch.device('cpu'))\n",
    "# model.to(dev)\n",
    "# model.decoder.to(dev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loader = DataLoader(gnn_dataset_blinker_pos_ext, batch_size=1, shuffle=True)\n",
    "#train(1, data_loader, False, engtest_data_loader)\n",
    "# test(1, engtest_data_loader, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn_dataset_train_pos_ext, gnn_dataset_grc_pos_ext, gnn_dataset_heb_pos_ext, gnn_dataset_blinker_pos_ext = create_me_a_gnn_dataset_you_stupid([ train_pos_node_cover_ext, grc_pos_node_cover_ext, heb_pos_node_cover_ext, blinker_pos_node_cover_ext]\n",
    "#   , [train_pos_labels_ext, grc_pos_labels_ext, heb_pos_labels_ext, blinker_pos_labels_ext], group_size=512)\n",
    "#create_model(gnn_dataset_train_pos_ext, gnn_dataset_grc_pos_ext, gnn_dataset_heb_pos_ext, gnn_dataset_blinker_pos_ext, gnn_dataset_engtest_pos,\n",
    "#   train_word_embedding=False, mask_language=True, use_transformers=True, tag_frequencies=True)\n",
    "\n",
    "#create_model(gnn_dataset_train_pos, gnn_dataset_grc_pos, gnn_dataset_heb_pos, gnn_dataset_blinker_pos, gnn_dataset_engtest_pos,\n",
    "#   train_word_embedding=False, mask_language=False, use_transformers=True, tag_frequencies=True)\n",
    "#model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerTrue6layresresidual_trainWEFalse_maskLangTrue_epoch1_trainYoruba_20220218-223347.pickle')\n",
    "#test(1, blinker_data_loader_bigbatch, True)\n",
    "importlib.reload(posutil)\n",
    "\n",
    "posutil.generate_target_lang_tags(model, 'yor-x-bible-2010', '11lang_posfeatTruealltargets_transformerTrue6layresresidual_trainWEFalse_epoch1_noEng', True, \n",
    "        train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "        DataEncoder)\n",
    "\n",
    "posutil.generate_target_lang_tags(model, 'fin-x-bible-helfi', '11lang_posfeatTruealltargets_transformerTrue6layresresidual_trainWEFalse_epoch1_noEng', True, \n",
    "        train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "        DataEncoder)\n",
    " \n",
    "posutil.generate_target_lang_tags(model, 'tam-x-bible-newworld', '11lang_posfeatTruealltargets_transformerTrue6layresresidual_trainWEFalse_epoch1_noEng', True, \n",
    "        train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "        DataEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in engtest_data_loader:\n",
    "    if item['verse'][0] == verse:\n",
    "        print('found')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verse = list(eng_gen_data.keys())[0]\n",
    "number_to_tag_map = {postag_map[i]:i for i in postag_map}\n",
    "print(verse)\n",
    "print({item[0]:number_to_tag_map[item[1]] for item in sorted(eng_gen_data[verse], key=lambda x: x[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerTrue_trainWETrue_maskLangFalse_epoch3_20220218-101130.pickle')\n",
    "# test(1, engtest_data_loader, mask_language=False) \n",
    "\n",
    "importlib.reload(posutil)\n",
    "def get_language_node_cover(all_node_cover, target_edition, dataset):\n",
    "    nodes_map = dataset.nodes_map\n",
    "    res = collections.defaultdict(list)\n",
    "\n",
    "    if target_edition in nodes_map:\n",
    "        for verse in all_node_cover:\n",
    "            if verse in nodes_map[target_edition]:\n",
    "                lang_nodes = list(nodes_map[target_edition][verse].values())\n",
    "                for tok in all_node_cover[verse]:\n",
    "                    if tok in lang_nodes:\n",
    "                        res[verse].append(tok)\n",
    "    \n",
    "    return res\n",
    "    \n",
    "\n",
    "def finetune_and_generate_for_target_lang(model_path, target_edition):\n",
    "    global model, criterion, optimizer\n",
    "    \n",
    "    target_train_node_cover = get_language_node_cover(train_pos_node_cover_ext, target_edition, train_dataset)\n",
    "    target_grc_node_cover = get_language_node_cover(grc_pos_node_cover_ext, target_edition, grc_test_dataset)\n",
    "    target_heb_node_cover = get_language_node_cover(heb_pos_node_cover_ext, target_edition, heb_test_dataset)\n",
    "    target_blinker_node_cover = get_language_node_cover(blinker_pos_node_cover_ext, target_edition, blinker_test_dataset)\n",
    "    \n",
    "\n",
    "    train_ds, grc_ds, heb_ds, blinker_ds = create_me_a_gnn_dataset_you_stupid(\n",
    "            [target_train_node_cover, target_grc_node_cover, target_heb_node_cover, target_blinker_node_cover], \n",
    "            [train_pos_labels_ext, grc_pos_labels_ext,  heb_pos_labels_ext, blinker_pos_labels_ext], editions=[target_edition])\n",
    "    \n",
    "    train_data_loader = DataLoader(train_ds, shuffle=True)\n",
    "    grc_data_loader = DataLoader(grc_ds, shuffle=True)\n",
    "    heb_data_loader = DataLoader(heb_ds, shuffle=True)\n",
    "    blinker_data_loader = DataLoader(blinker_ds, shuffle=True)\n",
    "\n",
    "    # model = torch.load(model_path)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    # train(1, train_data_loader, mask_language=False, test_data_loader=blinker_data_loader, max_batches=100)\n",
    "    # posutil.generate_target_lang_tags(model, target_edition, 'posfeatTrue_transformerTrue6layerresidual_trainWEFalse_finetune100', False, \n",
    "    #         train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "    #         DataEncoder)\n",
    "    # model = None\n",
    "    # clean_memory\n",
    "\n",
    "    # model = torch.load(model_path)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    # train(1, train_data_loader, mask_language=False, test_data_loader=blinker_data_loader, max_batches=1000)\n",
    "    posutil.generate_target_lang_tags(model, target_edition, 'posfeatTrue_transformerTrue6layerresidual_trainWEFalse_finetune1000', False, \n",
    "            train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "            DataEncoder)\n",
    "    model = None\n",
    "    clean_memory\n",
    "\n",
    "    model = torch.load(model_path)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    train(1, train_data_loader, mask_language=False, test_data_loader=blinker_data_loader, max_batches=10000)\n",
    "    posutil.generate_target_lang_tags(model, target_edition, 'posfeatTrue_transformerTrue6layerresidual_trainWEFalse_finetune10000', False, \n",
    "            train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "            DataEncoder)\n",
    "    model = None\n",
    "    clean_memory\n",
    "\n",
    "    model = torch.load(model_path)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "    train(1, train_data_loader, mask_language=False, test_data_loader=blinker_data_loader)\n",
    "    train(1, grc_data_loader, mask_language=False, test_data_loader=blinker_data_loader)\n",
    "    train(1, heb_data_loader, mask_language=False, test_data_loader=blinker_data_loader)\n",
    "    train(1, blinker_data_loader, mask_language=False, test_data_loader=blinker_data_loader)\n",
    "    posutil.generate_target_lang_tags(model, target_edition, 'posfeatTrue_transformerTrue6layerresidual_trainWEFalse_finetuneALL', False, \n",
    "            train_dataset, grc_test_dataset, heb_test_dataset, blinker_test_dataset, train_data_loader_bigbatch, grc_data_loader_bigbatch, heb_data_loader_bigbatch, blinker_data_loader_bigbatch,\n",
    "            DataEncoder)\n",
    "\n",
    "\n",
    "def generate_target_lang_tags_all_models(target_langs):\n",
    "    global model\n",
    "    model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatFalse_transformerFalse_trainWEFalse_maskLangTrue_20220209-201345.pickle', map_location=torch.device('cpu')).to(dev)\n",
    "    test(0, blinker_data_loader_bigbatch, True)\n",
    "    for lang in target_langs:\n",
    "        posutil.generate_target_lang_tags(lang,  f\"posfeatFalse_transformerFalse_trainWEFalse\", True)\n",
    "\n",
    "# posutil.generate_target_lang_tags_all_models(['yor-x-bible-2010', 'tam-x-bible-newworld', 'fin-x-bible-helfi'])\n",
    "finetune_and_generate_for_target_lang('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerTru6Lresidual_trainWETrue_maskLangFalse_epoch3_20220218-101130.pickle', 'yor-x-bible-2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = sag\n",
    "batch = khar\n",
    "verse = gav\n",
    "print(i, verse)\n",
    "\n",
    "keys = list(gnn_dataset.verse_info.keys())\n",
    "\n",
    "gnn_dataset.verse_info[verse]\n",
    "print(keys)\n",
    "#save_model(model, 'freeze-embedding_noLang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_model = model.to('cpu')\n",
    "#model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerFalse_trainWEFalse_maskLangTrue_20220210-173912.pickle')\n",
    "#torch.cuda.set_device(0)\n",
    "#model.to(dev)\n",
    "#epoch = 0\n",
    "#test_data_loader = DataLoader(gnn_dataset_blinker_pos_bigbatch, batch_size=1, shuffle=False)\n",
    "#test(epoch, test_data_loader, mask_language=True)\n",
    "##yoruba_postags = generate_target_lang_tags('yor-x-bible-2010', f\"posfeatFalse_transformerFalse_trainWEFalse\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importlib.reload(afeatures)\n",
    "#initial_model = model\n",
    "#torch.cuda.set_device(1)\n",
    "#create_model(gnn_dataset_train_pos, gnn_dataset_grc_pos, gnn_dataset_heb_pos, gnn_dataset_blinker_pos, gnn_dataset_blinker_pos_bigbatch, tag_frequencies=True)\n",
    "## yoruba_postags = generate_target_lang_tags('yor-x-bible-2010', f\"posfeatTrue_transformerFalse_trainWEFalse\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(4)\n",
    "gnn_ds_train_pos_ext, gnn_ds_grc_pos_ext, gnn_ds_heb_pos_ext, gnn_ds_blinker_pos_ext = create_me_a_gnn_dataset_you_stupid([train_pos_node_cover_ext, grc_pos_node_cover_ext, heb_pos_node_cover_ext, blinker_pos_node_cover_ext], \n",
    "    [train_pos_labels_ext, grc_pos_labels_ext, heb_pos_labels_ext, blinker_pos_labels_ext])\n",
    "\n",
    "create_model(gnn_ds_train_pos_ext, gnn_ds_grc_pos_ext, gnn_ds_heb_pos_ext, gnn_ds_blinker_pos_ext, gnn_dataset_blinker_pos_bigbatch, tag_frequencies=True\n",
    "    , use_transformers=False, train_word_embedding=True, mask_language=False)\n",
    "\n",
    "# yoruba_postags = generate_target_lang_tags('yor-x-bible-2010', f\"posfeatTrue_transformerTrue_trainWETrue\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(4)\n",
    "model = torch.load('/mounts/work/ayyoob/models/gnn/checkpoint/postagging/pos_tagging_posfeatTrue_transformerTrue_trainWETrue_maskLangFalse_epoch1_20220214-190824.pickle')\n",
    "torch.cuda.set_device(0)\n",
    "model.to(dev)\n",
    "epoch = 0\n",
    "test_data_loader = DataLoader(gnn_dataset_blinker_pos_onlyeng, batch_size=1, shuffle=False)\n",
    "test(epoch, test_data_loader, mask_language=False)\n",
    "yoruba_postags = generate_target_lang_tags('fin-x-bible-helfi', f\"posfeatTrue_transformerTrue_trainWETrue\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#normalized_gold_frequencies, gold_frequencies_all = get_words_tag_frequence(model, 2354770, len(postag_map), english_data_loader, from_gold_data=True)\n",
    "\n",
    "#gold_frequencies_all = gold_frequencies\n",
    "word_frequencies = torch.sum(gold_frequencies_all, dim=1)\n",
    "\n",
    "subjectword_indices =  word_frequencies > 0.1\n",
    "print(word_frequencies.shape)\n",
    "gold_frequencies = gold_frequencies_all[subjectword_indices, :]\n",
    "predicted_frequencies = tag_frequencies_english[subjectword_indices, :]\n",
    "wordtype_frequencies = word_frequencies[subjectword_indices]\n",
    "print(gold_frequencies.shape)\n",
    "\n",
    "_, gold_tags = torch.max(gold_frequencies, dim=1)\n",
    "_, predicted_tags = torch.max(predicted_frequencies, dim=1)\n",
    "\n",
    "sorted_wordtype_frequencies, sort_pattern = torch.sort(wordtype_frequencies, descending=True)\n",
    "\n",
    "sorted_gold_tags = gold_tags[sort_pattern]\n",
    "sorted_predicted_tags = predicted_tags[sort_pattern]\n",
    "quarter_size = int(sorted_gold_tags.size(0)/2.0)\n",
    "\n",
    "print('quarter size', quarter_size)\n",
    "print(\"general accuracy\", torch.sum(gold_tags == predicted_tags)/predicted_tags.size(0))\n",
    "print('first quarter accuracy', torch.sum(sorted_gold_tags[:quarter_size] == sorted_predicted_tags[:quarter_size])/quarter_size)\n",
    "print('last part accuracy', torch.sum(sorted_gold_tags[1*quarter_size:] == sorted_predicted_tags[1*quarter_size:])/sorted_predicted_tags[1*quarter_size:].size(0))\n",
    "\n",
    "print('total token count', torch.sum(wordtype_frequencies))\n",
    "print('first quarter words token count', torch.sum(word_frequencies[:quarter_size]))\n",
    "\n",
    "print('1st frequency', sorted_wordtype_frequencies[0])\n",
    "print('10st frequency', sorted_wordtype_frequencies[10])\n",
    "print('100st frequency', sorted_wordtype_frequencies[100])\n",
    "print('100st frequency', sorted_wordtype_frequencies[736])\n",
    "print('1000st frequency', sorted_wordtype_frequencies[1000])\n",
    "print('10000st frequency', sorted_wordtype_frequencies[10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized_tag_frequencies = torch.softmax(tag_frequencies_copy, dim=1)\n",
    "\n",
    "sm = torch.sum(tag_frequencies_copy, dim=1)\n",
    "normalized_tag_frequencies = (tag_frequencies_copy.transpose(1,0) / sm).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global model, decoder\n",
    "#1/0\n",
    "\n",
    "decoder = None\n",
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = blinker_test_dataset.features[:]\n",
    "#features_edge = train_dataset.features_edge[:]\n",
    "from pprint import pprint\n",
    "#print('indim',in_dim)\n",
    "#features[-1].out_dim = 50\n",
    "for i in features:\n",
    "    #if i.type==3:\n",
    "    #    i.out_dim=4\n",
    "    print(vars(i))\n",
    "print(vars(model.encoder.feature_encoder.layers[10].emb))\n",
    "# print(model.encoder.feature_encoder.layers[10].emb)\n",
    "#sum(p.out_dim for p in features)\n",
    "#train_dataset.features.pop()\n",
    "#train_dataset.features[0] = afeatures.OneHotFeature(20, 83, 'editf')\n",
    "#train_dataset.features[1] = afeatures.OneHotFeature(32, 150, 'position')\n",
    "#train_dataset.features[2] = afeatures.FloatFeature(4, 'degree_centrality')\n",
    "#train_dataset.features[3] = afeatures.FloatFeature(4, 'closeness_centrality')\n",
    "#train_dataset.features[4] = afeatures.FloatFeature(4, 'betweenness_centrality')\n",
    "#train_dataset.features[5] = afeatures.FloatFeature(4, 'load_centrality')\n",
    "#train_dataset.features[6] = afeatures.FloatFeature(4, 'harmonic_centrality')\n",
    "#train_dataset.features[7] = afeatures.OneHotFeature(32, 250, 'greedy_modularity_community')\n",
    "##train_dataset.features.append(afeatures.MappingFeature(100, 'word'))\n",
    "#torch.save(train_dataset, \"/mounts/work/ayyoob/models/gnn/dataset_helfi_train_community_word.pickle\")\n",
    "#torch.save(train_dataset.features[-3], \"./features.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_map = train_dataset.nodes_map\n",
    "bad_edition_files = []\n",
    "for edit in nodes_map:\n",
    "    bad_count = 0\n",
    "    for verse in nodes_map[edit]:\n",
    "        if len(nodes_map[edit][verse].keys()) < 2:\n",
    "            bad_count += 1\n",
    "        if bad_count > 1:\n",
    "            bad_edition_files.append(edit)\n",
    "            break\n",
    "print(bad_edition_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_japanese_nodes = set()\n",
    "nodes_map = train_dataset.nodes_map\n",
    "\n",
    "for bad_editionf in bad_edition_files:\n",
    "    for verse in nodes_map[bad_editionf]:\n",
    "        for item in nodes_map[bad_editionf][verse].items():\n",
    "            all_japanese_nodes.add(item[1])\n",
    "\n",
    "print(\" all japansese nodes: \", len(all_japanese_nodes))\n",
    "edge_index = train_dataset.edge_index.to('cpu')\n",
    "remaining_edges_index = []\n",
    "for i in tqdm(range(0, edge_index.shape[1], 2)):\n",
    "    if edge_index[0, i].item() not in all_japanese_nodes and edge_index[0, i+1].item() not in all_japanese_nodes:\n",
    "        remaining_edges_index.extend([i, i+1])\n",
    "\n",
    "print('original total edges count', edge_index.shape)\n",
    "print('remaining edge count', len(remaining_edges_index))\n",
    "train_dataset.edge_index = edge_index[:, remaining_edges_index]\n",
    "train_dataset.edge_index.shape\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab640873abb67ad450730b814c8d7de015aa287a6fc3bae4b7154b533e57676"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('multalign_graph': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
